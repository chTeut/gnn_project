{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4f513849-46ca-4516-98ca-97eda2f31358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: optuna in /home/jonas/.local/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /home/jonas/.local/lib/python3.10/site-packages (from optuna) (2.0.29)\n",
      "Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from optuna) (5.4.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /home/jonas/.local/lib/python3.10/site-packages (from optuna) (1.13.1)\n",
      "Requirement already satisfied: numpy in /home/jonas/.local/lib/python3.10/site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jonas/.local/lib/python3.10/site-packages (from optuna) (23.2)\n",
      "Requirement already satisfied: colorlog in /home/jonas/.local/lib/python3.10/site-packages (from optuna) (6.8.2)\n",
      "Requirement already satisfied: tqdm in /home/jonas/.local/lib/python3.10/site-packages (from optuna) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/jonas/.local/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.9.0)\n",
      "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/lib/python3/dist-packages (from sqlalchemy>=1.3.0->optuna) (1.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wandb in /home/jonas/.local/lib/python3.10/site-packages (0.16.4)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/lib/python3/dist-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from wandb) (59.6.0)\n",
      "Requirement already satisfied: setproctitle in /home/jonas/.local/lib/python3.10/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /home/jonas/.local/lib/python3.10/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/jonas/.local/lib/python3.10/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/lib/python3/dist-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from wandb) (5.4.1)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/jonas/.local/lib/python3.10/site-packages (from wandb) (1.43.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/jonas/.local/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /home/jonas/.local/lib/python3.10/site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/jonas/.local/lib/python3.10/site-packages (from wandb) (3.1.42)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/jonas/.local/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jonas/.local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jonas/.local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.2.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/jonas/.local/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: plotly in /home/jonas/.local/lib/python3.10/site-packages (5.20.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/jonas/.local/lib/python3.10/site-packages (from plotly) (8.2.3)\n",
      "Requirement already satisfied: packaging in /home/jonas/.local/lib/python3.10/site-packages (from plotly) (23.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install optuna\n",
    "%pip install wandb\n",
    "%pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "feb5dea7-6090-40c1-9d55-5a1735b9b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import pickle\n",
    "import wandb\n",
    "import os\n",
    "import optuna\n",
    "import plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dae5eb94-e248-472c-a0fc-4eb75a7210e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b74833a3-dbb8-4358-99f2-62a800d2845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "block_size = 256\n",
    "linScale = 5\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5946fbec-548e-40f8-b5b0-aff3abe0534a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c469c8-8871-4970-99fe-c9bd8734574a",
   "metadata": {},
   "source": [
    "# Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "815733ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stoi\", \"rb\") as f:\n",
    "    stoi = pickle.load(f)\n",
    "\n",
    "with open(\"itos\", \"rb\") as f:\n",
    "    itos = pickle.load(f)\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])\n",
    "\n",
    "vocab_size = len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "51201e00-88f0-4410-81ca-6ac09dcd6b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"/datasets/bash-history/dataset.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#    text = f.read()\n",
    "#\n",
    "#chars = sorted(list(set(text)))\n",
    "#\n",
    "#data = torch.tensor(encode(text), dtype=torch.long)\n",
    "#n = int(0.9 * len(data))\n",
    "#train_data = data[:n]\n",
    "#val_data = data[n:]\n",
    "#\n",
    "#def get_batch(split, batch_size):\n",
    "#    data = train_data if split == \"train\" else val_data\n",
    "#    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "#    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "#    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "#    x, y = x.to(device), y.to(device)\n",
    "#    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0e08d3-47a8-4593-861a-fd223b84f3ce",
   "metadata": {},
   "source": [
    "# Transformer Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f341c693-f2d9-4800-9202-2fbbdf312055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, headSize, block_size=block_size):\n",
    "        super().__init__()\n",
    "        self.dim = np.sqrt(headSize)\n",
    "\n",
    "        self.linQ = nn.Linear(headSize, headSize, bias=False)\n",
    "        self.linK = nn.Linear(headSize, headSize, bias=False)\n",
    "        self.linV = nn.Linear(headSize, headSize, bias=False)\n",
    "\n",
    "        self.triu = (\n",
    "            torch.triu(torch.ones((block_size, block_size)), diagonal=1).to(device) == 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.linQ(x)\n",
    "        V = self.linV(x)\n",
    "        K = self.linK(x)\n",
    "\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        K = torch.transpose(K, 1, 2)\n",
    "        weights = (Q @ K) / self.dim\n",
    "        weights = weights.masked_fill(self.triu[:T, :T], -torch.inf)\n",
    "        weights = nn.functional.softmax(weights, -1)\n",
    "        return weights @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f43ceec8-1f35-43c8-98af-8628a3cf9660",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, numHeads, headSize):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [SelfAttentionBlock(headSize) for _ in range(numHeads)]\n",
    "        )\n",
    "        self.lin = nn.Linear(headSize * numHeads, headSize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        return self.lin(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2088b510-7fe6-4c2c-b27c-a3dcd4873696",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddingSize):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embeddingSize, embeddingSize * linScale),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embeddingSize * linScale, embeddingSize),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3f55c990-717f-4e14-ae6c-37dab2a6b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, numLayers, embeddingSize, vocabSize, headSize):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.LayerNorm(embeddingSize),\n",
    "                MultiHeadSelfAttention(headSize, embeddingSize),\n",
    "                nn.LayerNorm(embeddingSize),\n",
    "                Linear(embeddingSize),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for i in range(numLayers - 1):\n",
    "            self.layers.extend(\n",
    "                [\n",
    "                    nn.LayerNorm(embeddingSize),\n",
    "                    MultiHeadSelfAttention(headSize, embeddingSize),\n",
    "                    nn.LayerNorm(embeddingSize),\n",
    "                    Linear(embeddingSize),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.ln = nn.LayerNorm(embeddingSize)\n",
    "        self.final_linear = nn.Linear(embeddingSize, vocabSize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, MultiHeadSelfAttention):\n",
    "                x = x + layer(x)\n",
    "            if isinstance(layer, Linear):\n",
    "                x = x + layer(x)\n",
    "            if isinstance(layer, nn.LayerNorm):\n",
    "                x = layer(x)\n",
    "\n",
    "        return self.final_linear(self.ln(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "99c203f7-c1e8-4915-9ffc-b4c700115eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        numLayersDecoder,\n",
    "        embeddingSize,\n",
    "        headSize,\n",
    "        vocabSize=vocab_size,\n",
    "        maxBlockSize=block_size,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.decoder = TransformerDecoder(\n",
    "            numLayersDecoder, embeddingSize, vocabSize, headSize\n",
    "        )\n",
    "        self.embed = nn.Embedding(vocabSize, embeddingSize)\n",
    "        self.positional_encoding = nn.Embedding(maxBlockSize, embeddingSize)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "    ):\n",
    "        B, T = x.shape\n",
    "        x = self.embed(x)\n",
    "        pos_embed = self.positional_encoding(\n",
    "            torch.arange(T, device=torch.device(device))\n",
    "        )\n",
    "        x = x + pos_embed\n",
    "        pred = self.decoder(x)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f615ca99-b33c-44fc-9ba9-43be625ebd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(pred, target):\n",
    "    B, T, C = pred.shape\n",
    "    pred = pred.view(B * T, C)\n",
    "    target = target.view(B * T)\n",
    "    return F.cross_entropy(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0cc1bbd7-29df-47ce-8cc1-b19202067bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters, batch_size):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size)\n",
    "            pred = model(X)\n",
    "            loss = get_loss(pred, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e1b43ad1-240a-4210-91bf-085fc3b51153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    batch_size,\n",
    "    learning_rate,\n",
    "    n_embd,\n",
    "    n_head,\n",
    "    n_layer,\n",
    "    max_iters,\n",
    "    save_path=False,\n",
    "    log=True,\n",
    "    model=False,\n",
    "):\n",
    "\n",
    "    run = wandb.init(\n",
    "        project=\"shell-transformer-study\",\n",
    "        config={\n",
    "            \"batch_size\": batch_size,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"n_embd\": n_embd,\n",
    "            \"n_head\": n_head,\n",
    "            \"n_layer\": n_layer,\n",
    "            \"block_size\": block_size,\n",
    "            \"lin_scale\": linScale,\n",
    "            \"dropout\": dropout,\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    if not model:\n",
    "        model = Transformer(n_layer, n_embd, n_head).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss(model, eval_iters, batch_size)\n",
    "            if log:\n",
    "                print(\n",
    "                    f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\n",
    "                )\n",
    "            wandb.log({\"val loss\": losses[\"val\"], \"train loss\": losses[\"train\"]})\n",
    "            \n",
    "            if save_path:\n",
    "                model_path = f\"{save_path}/{run.name}\"\n",
    "                os.makedirs(model_path, exist_ok=True)\n",
    "                torch.save(model.state_dict(), f\"{model_path}/shell_transformer_{iter}\")\n",
    "            \n",
    "        xb, yb = get_batch(\"train\", batch_size)\n",
    "\n",
    "        pred = model(xb)\n",
    "\n",
    "        loss = get_loss(pred, yb)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return losses[\"val\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faa04a8-17b0-4a5d-88f6-445b7823cb90",
   "metadata": {},
   "source": [
    "# Hyperparameter Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0e610356-e3b6-45d1-aa4e-0c0aaad9f8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 32, 100)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.0003, 0.001)\n",
    "    n_embd = trial.suggest_int(\"n_embd\", 50, 320)\n",
    "    n_layer = trial.suggest_int(\"n_layer\", 2, 10)\n",
    "    n_head = trial.suggest_int(\"n_head\", 2, 10)\n",
    "\n",
    "    return train_model(batch_size, learning_rate, n_embd, n_head, n_layer, 5000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "466190d1-9647-44b8-9268-d4ee83eccb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#study = optuna.load_study(\n",
    "#    study_name=\"distributed-shell-transformer\", storage=\"sqlite:///optuna.db\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7f1ad069-f1fd-49a2-b999-c6613b2aef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5c0bc054-bb0e-4d83-b85a-ad3eb9feb84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7ced75ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'batch_size': 71,\n",
    "    'learning_rate': 0.00048767835960680843,\n",
    "    'n_embd': 234,\n",
    "    'n_layer': 6,\n",
    "    'n_head': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a1ec86-6922-4397-ba9f-e6ec6db8bc00",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "049e6e13-a878-4b74-bc7e-4295087dbee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train_model(best_params[\"batch_size\"], best_params[\"learning_rate\"], best_params[\"n_embd\"], best_params[\"n_head\"], best_params[\"n_layer\"], 50000, \"final_with_preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2801791-3248-4bb6-8615-eb1e5d4ed12d",
   "metadata": {},
   "source": [
    "# Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9d0cc9cd-ae10-4690-8094-56ae324e101b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(best_params[\"n_layer\"], best_params[\"n_embd\"], best_params[\"n_head\"]).to(device)\n",
    "model.load_state_dict(torch.load(f\"./final_with_preprocessing/jumping-river-27/shell_transformer_23000\", map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "be7ea016-058b-456b-ad5d-112e0d9dd81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -block_size:]\n",
    "        logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        res = torch.argmax(probs)\n",
    "        idx_next = torch.tensor([[res]]).to(device)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "        if res == torch.tensor(encode([\"\\n\"])).to(device):\n",
    "            break\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "647a591c-46a6-453a-91d9-ab76ca134b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nano encoder.py\n",
      "python3 encoder.py\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "context = \"\"\"\n",
    "nano encoder.py\n",
    "\"\"\"\n",
    "context = torch.tensor([encode(context)]).to(device)\n",
    "out = decode(generate(model, context, max_new_tokens=500)[0].tolist())\n",
    "autocomplete = out.split(\"\\n\")[-2]\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
