{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_forecasting in /usr/local/lib/python3.9/dist-packages (1.0.0)\n",
      "Requirement already satisfied: optuna<4.0.0,>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_forecasting) (3.6.0)\n",
      "Requirement already satisfied: fastapi>=0.80 in /usr/local/lib/python3.9/dist-packages (from pytorch_forecasting) (0.110.0)\n",
      "Requirement already satisfied: scikit-learn<2.0,>=1.2 in /usr/local/lib/python3.9/dist-packages (from pytorch_forecasting) (1.4.1.post1)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_forecasting) (2.2.1)\n",
      "Requirement already satisfied: pandas<=3.0.0,>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_forecasting) (1.5.0)\n",
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.9/dist-packages (from pytorch_forecasting) (0.14.1)\n",
      "Requirement already satisfied: lightning<3.0.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_forecasting) (2.2.1)\n",
      "Requirement already satisfied: scipy<2.0,>=1.8 in /usr/local/lib/python3.9/dist-packages (from pytorch_forecasting) (1.12.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from pytorch_forecasting) (3.6.1)\n",
      "Requirement already satisfied: pytorch-optimizer<3.0.0,>=2.5.1 in /usr/local/lib/python3.9/dist-packages (from pytorch_forecasting) (2.12.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from fastapi>=0.80->pytorch_forecasting) (1.9.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from fastapi>=0.80->pytorch_forecasting) (4.10.0)\n",
      "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /usr/local/lib/python3.9/dist-packages (from fastapi>=0.80->pytorch_forecasting) (0.36.3)\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.9/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch_forecasting) (2.2.1)\n",
      "Requirement already satisfied: fsspec[http]<2025.0,>=2022.5.0 in /usr/local/lib/python3.9/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch_forecasting) (2023.1.0)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.9/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch_forecasting) (4.64.1)\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch_forecasting) (1.3.2)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch_forecasting) (0.11.0)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.9/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch_forecasting) (23.0)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.9/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch_forecasting) (5.4.1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch_forecasting) (1.23.4)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.9/dist-packages (from optuna<4.0.0,>=3.1.0->pytorch_forecasting) (1.13.1)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.9/dist-packages (from optuna<4.0.0,>=3.1.0->pytorch_forecasting) (6.8.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from optuna<4.0.0,>=3.1.0->pytorch_forecasting) (1.4.41)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas<=3.0.0,>=1.3.0->pytorch_forecasting) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas<=3.0.0,>=1.3.0->pytorch_forecasting) (2022.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn<2.0,>=1.2->pytorch_forecasting) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn<2.0,>=1.2->pytorch_forecasting) (1.2.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.9/dist-packages (from torch<3.0.0,>=2.0.0->pytorch_forecasting) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.9/dist-packages (from torch<3.0.0,>=2.0.0->pytorch_forecasting) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.9/dist-packages (from torch<3.0.0,>=2.0.0->pytorch_forecasting) (2.19.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch<3.0.0,>=2.0.0->pytorch_forecasting) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.9/dist-packages (from torch<3.0.0,>=2.0.0->pytorch_forecasting) (8.9.2.26)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch<3.0.0,>=2.0.0->pytorch_forecasting) (1.12)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch<3.0.0,>=2.0.0->pytorch_forecasting) (3.9.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch<3.0.0,>=2.0.0->pytorch_forecasting) (3.1.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch<3.0.0,>=2.0.0->pytorch_forecasting) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.9/dist-packages (from torch<3.0.0,>=2.0.0->pytorch_forecasting) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch<3.0.0,>=2.0.0->pytorch_forecasting) (12.1.105)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch<3.0.0,>=2.0.0->pytorch_forecasting) (3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch<3.0.0,>=2.0.0->pytorch_forecasting) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.9/dist-packages (from torch<3.0.0,>=2.0.0->pytorch_forecasting) (2.2.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.9/dist-packages (from torch<3.0.0,>=2.0.0->pytorch_forecasting) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.9/dist-packages (from torch<3.0.0,>=2.0.0->pytorch_forecasting) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.9/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.0.0->pytorch_forecasting) (12.4.99)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->pytorch_forecasting) (1.0.7)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->pytorch_forecasting) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->pytorch_forecasting) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->pytorch_forecasting) (9.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->pytorch_forecasting) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->pytorch_forecasting) (1.4.4)\n",
      "Requirement already satisfied: patsy>=0.5.4 in /usr/local/lib/python3.9/dist-packages (from statsmodels->pytorch_forecasting) (0.5.6)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.9/dist-packages (from alembic>=1.5.0->optuna<4.0.0,>=3.1.0->pytorch_forecasting) (1.3.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch_forecasting) (2.28.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.9/dist-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch_forecasting) (3.8.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from lightning-utilities<2.0,>=0.8.0->lightning<3.0.0,>=2.0.0->pytorch_forecasting) (66.1.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from patsy>=0.5.4->statsmodels->pytorch_forecasting) (1.14.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from sqlalchemy>=1.3.0->optuna<4.0.0,>=3.1.0->pytorch_forecasting) (2.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.9/dist-packages (from starlette<0.37.0,>=0.36.3->fastapi>=0.80->pytorch_forecasting) (3.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch<3.0.0,>=2.0.0->pytorch_forecasting) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch<3.0.0,>=2.0.0->pytorch_forecasting) (1.3.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch_forecasting) (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch_forecasting) (2.1.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch_forecasting) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch_forecasting) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch_forecasting) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch_forecasting) (18.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch_forecasting) (6.0.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.9/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi>=0.80->pytorch_forecasting) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/lib/python3/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi>=0.80->pytorch_forecasting) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch_forecasting) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch_forecasting) (2019.11.28)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting bashlex\n",
      "  Downloading bashlex-0.18-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.5/69.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bashlex\n",
      "Successfully installed bashlex-0.18\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pytorch_forecasting\n",
    "%pip install bashlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gitlab.ics.muni.cz/muni-kypo-trainings/datasets/commands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_1 = \"./data/commands\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = []\n",
    "for root, dirs, files in os.walk(data_dir_1):\n",
    "    for name in files:\n",
    "        if name.endswith((\".json\")):\n",
    "            full_path = os.path.join(root, name)\n",
    "            file_paths.append(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in file_paths:\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "        data = f.read()\n",
    "        for line in data.split(\"\\n\"):\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            json_data = json.loads(line)\n",
    "            if \"cmd\" in json_data:\n",
    "                dataset.append(json_data[\"cmd\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21089"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/datasets/spignelon/bash_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_2 = \"./data/bash.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir_2, \"r\") as f:\n",
    "    data = f.read()\n",
    "    lines = data.split(\"\\n\")\n",
    "    dataset.extend(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202059"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/datasets/gayathrimanoj/dataset_shell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_3 = \"./data/train.jsonl\"\n",
    "data_dir_4 = \"./data/test.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir_3, \"r\") as f:\n",
    "    data = f.read()\n",
    "    lines = data.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        json_data = json.loads(line)\n",
    "        text = json_data[\"text\"]\n",
    "        text_split = text.split(\"[/INST]\")[1]\n",
    "        res = text_split.rstrip(\" </s>\")\n",
    "        if res == \"Invalid command\":\n",
    "            continue\n",
    "        if res == \"Invalid command.\":\n",
    "            continue\n",
    "        if res == \"Invalid Command\":\n",
    "            continue\n",
    "        if res == \"Invalid Command.\":\n",
    "            continue\n",
    "        dataset.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir_4, \"r\") as f:\n",
    "    data = f.read()\n",
    "    lines = data.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        json_data = json.loads(line)\n",
    "        text = json_data[\"text\"]\n",
    "        text_split = text.split(\"[/INST]\")[1]\n",
    "        res = text_split.rstrip(\" </s>\")\n",
    "        if res == \"Invalid command\":\n",
    "            continue\n",
    "        if res == \"Invalid command.\":\n",
    "            continue\n",
    "        if res == \"Invalid Command\":\n",
    "            continue\n",
    "        if res == \"Invalid Command.\":\n",
    "            continue\n",
    "        dataset.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202601\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/datasets/byroneverson/shell-cmd-instruct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_5 = \"./data/train (1).jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir_5, \"r\") as f:\n",
    "    data = f.read()\n",
    "    for line in data.split(\"\\n\"):\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        json_data = json.loads(line)\n",
    "        if \"cmd\" in json_data:\n",
    "            dataset.append(json_data[\"cmd\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203101\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/thuyngandao/bashlogs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_6 = \"./data/bash_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = []\n",
    "for root, dirs, files in os.walk(data_dir_6):\n",
    "    for name in files:\n",
    "        full_path = os.path.join(root, name)\n",
    "        file_paths.append(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in file_paths:\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "        data = f.read()\n",
    "        for line in data.split(\"\\n\"):\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            dataset.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407365\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Own bash history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_6 = \"./data/own_bash_history.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "own_commands = []\n",
    "with open(data_dir_6, \"r\") as f:\n",
    "    data = f.read().split(\"\\n\")\n",
    "    for line in data:\n",
    "        if \";\" in line:\n",
    "            dataset.append(line.split(\";\", 1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411492\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, cmd in enumerate(dataset):\n",
    "    dataset[i] += \"\\n\"\n",
    "    dataset[i] = dataset[i].lstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatinated = \"\".join(dataset)\n",
    "with open(\"dataset.txt\", \"w\") as fp:\n",
    "    fp.write(concatinated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(concatinated)))\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "with open(\"stoi\", \"wb\") as fp:\n",
    "    pickle.dump(stoi, fp)\n",
    "\n",
    "with open(\"itos\", \"wb\") as fp:\n",
    "    pickle.dump(itos, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bashlex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "validated = []\n",
    "for cmd in dataset:\n",
    "    try:\n",
    "        parsed = bashlex.parse(cmd)\n",
    "        validated.append(cmd)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395723"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "commands = []\n",
    "for cmd in validated:\n",
    "    command = cmd.split(\" \")[0].rstrip(\"\\n\")\n",
    "    commands.append(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "counted = Counter(commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict = dict(sorted(counted.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"commands.txt\", \"w\") as fp:\n",
    "#    fp.write(\"\\n\".join(list(sorted_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually select most relevant commands => filter out trash commands and random data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"commands.txt\", \"r\") as fp:\n",
    "    sorted_dict = fp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict = sorted_dict.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_commands = []\n",
    "for cmd in validated:\n",
    "    if cmd.split(\" \")[0] in sorted_dict:\n",
    "        selected_commands.append(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251874"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_commands = []\n",
    "for cmd in selected_commands:\n",
    "    if len(cmd) <= 60:\n",
    "        filtered_commands.append(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239267"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup = []\n",
    "for idx, line in enumerate(filtered_commands):\n",
    "    if line == filtered_commands[idx - 1]:\n",
    "        pass\n",
    "    else:\n",
    "        dedup.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def could_be_path(s):\n",
    "    # This regex matches strings that start with either \"./\", \"../\", \"/\", or a word followed by \"/\"\n",
    "    # It's a very basic check and might not cover all possible valid paths\n",
    "    return bool(re.match(r\"^(\\.\\/|\\.\\.\\/|\\/|\\w+\\/)\", s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_replaced = []\n",
    "for line in dedup:\n",
    "    line = line.replace(\"\\n\", \"\")\n",
    "    words = line.split(\" \")\n",
    "    new_line = \"\"\n",
    "    for word in words:\n",
    "        isdir = could_be_path(word)\n",
    "        isfile = \".\" in word\n",
    "        edited = False\n",
    "\n",
    "        if word == \"..\":\n",
    "            new_line += \" \" + word + \" \"\n",
    "        elif word == \".\":\n",
    "            new_line += \" \" + word + \" \"\n",
    "        elif word == \"../\":\n",
    "            new_line += \" \" + word + \" \"\n",
    "        elif word == \"../../\":\n",
    "            new_line += \" \" + word + \" \"\n",
    "        elif isfile:\n",
    "            edited = True\n",
    "\n",
    "            new_line += \" file\"\n",
    "            split = word.split(\".\")\n",
    "            if len(split) > 1:\n",
    "                ending = split[-1] + \" \"\n",
    "                new_line += \".\" + ending\n",
    "        elif isdir:\n",
    "            edited = True\n",
    "            new_line += \" example/path \"\n",
    "\n",
    "        else:\n",
    "            new_line += \" \" + word + \" \"\n",
    "\n",
    "    new_line = new_line.replace(\"  \", \" \").lstrip(\" \")\n",
    "\n",
    "    texts = new_line.split('\"')[1::2]\n",
    "\n",
    "    for text in texts:\n",
    "        new_line = new_line.replace('\"' + text + '\"', '\"text\"')\n",
    "\n",
    "    # if edited:\n",
    "    #    print(line)\n",
    "    #    print(new_line)\n",
    "    #    print(\"###########\")\n",
    "\n",
    "    new_line = new_line.rstrip(\" \")\n",
    "    new_line = new_line + \"\\n\"\n",
    "\n",
    "    path_replaced.append(new_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle data to get meaningful validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/46447949/is-there-a-function-in-python-that-shuffle-data-by-data-blocks\n",
    "import random\n",
    "\n",
    "blocksize = 3500  # keep some bash history context\n",
    "\n",
    "blocks = [\n",
    "    path_replaced[i : i + blocksize] for i in range(0, len(path_replaced), blocksize)\n",
    "]\n",
    "\n",
    "random.shuffle(blocks)\n",
    "\n",
    "data = [b for bs in blocks for b in bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219247"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatinated = \"\".join(data)\n",
    "with open(\"dataset.txt\", \"w\") as fp:\n",
    "    fp.write(concatinated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(concatinated)))\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "with open(\"stoi\", \"wb\") as fp:\n",
    "    pickle.dump(stoi, fp)\n",
    "\n",
    "with open(\"itos\", \"wb\") as fp:\n",
    "    pickle.dump(itos, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
