{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pickle\n",
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = \"cuda:0\"\n",
    "else:\n",
    "  device = \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    dim_k = query.size(-1)\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / np.sqrt(dim_k)\n",
    "    if mask is not None:\n",
    "        if scores.shape[1] == mask.shape[1]:\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        else:\n",
    "            mask = torch.tril(torch.ones(scores.shape[1], scores.shape[1])).unsqueeze(0).to(device)\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return weights.bmm(value)\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "        self.mask = torch.tril(torch.ones(vocab_size, vocab_size)).unsqueeze(0).to(device)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        attn_outputs = scaled_dot_product_attention(\n",
    "            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state), self.mask)\n",
    "        return attn_outputs\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(embed_dim, head_dim, 72) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n",
    "        x = self.output_linear(x)\n",
    "        return x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply layer normalization and then copy input into query, key, value\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        # Apply attention with a skip connection\n",
    "        x = x + self.attention(hidden_state)\n",
    "        # Apply feed-forward layer with a skip connection\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        return x\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size, \n",
    "                                             config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n",
    "                                                config.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # Create position IDs for input sequence\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        # Create token and position embeddings\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        \n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        # Combine token and position embeddings\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(config) \n",
    "                                     for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class ShellTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x)\n",
    "        x = self.encoder(x)#[:, 0, :] # select hidden state of [CLS] token\n",
    "        #print(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        #print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/chris/University/gnn_project/dataset', 'rb') as fp:\n",
    "    _ = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nmap\\n',\n",
       " 'nmap -v 10.1.26.4\\n',\n",
       " 'nmap -v 10.1.26.9\\n',\n",
       " 'ssh --help\\n',\n",
       " 'ssh 10.1.26.9\\n',\n",
       " 'ssh 10.1.26.9 admin/123456\\n',\n",
       " 'ssh --help\\n',\n",
       " 'ssh 10.1.26.9\\n',\n",
       " 'ssh -l admin 10.1.26.9\\n',\n",
       " 'ssh admin@admin 10.1.26.9\\n']"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given array of text elements\n",
    "texts = _\n",
    "\n",
    "# Create a folder to store the text files\n",
    "import os\n",
    "folder_path = '/home/chris/University/gnn_project/data/'\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# Write each text element to a separate file\n",
    "for i, text in enumerate(texts):\n",
    "    file_path = os.path.join(folder_path, f'text_{i}.txt')\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = L(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Path('/home/chris/University/gnn_project/data/text_173923.txt'), Path('/home/chris/University/gnn_project/data/text_97621.txt'), Path('/home/chris/University/gnn_project/data/text_51016.txt'), Path('/home/chris/University/gnn_project/data/text_198692.txt'), Path('/home/chris/University/gnn_project/data/text_117671.txt')]\n",
      "[(TensorText([2]),), (TensorText([ 2, 79]),), (TensorText([ 2, 10]),), (TensorText([   2,  379,   75, 4433,  138,   12,  210,  491, 7255]),), (TensorText([   2,   17, 3725]),)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos vi out xxbos echo 100 &gt; / sys / class / backlight / intel_backlight / brightness xxbos nano xxmaj controllers / xxunk xxbos ls xxbos sudo nano / etc / network / interfaces xxbos help xxbos sudo service collectd start xxbos [ goalador@gatanda hhneuauf.de]$ bash : [ goalador@gatanda : command not found xxbos clear xxbos # xxunk xxbos tree xxbos vim .xresources xxbos vim palind.rb xxbos chmod xxunk xxmaj public /</td>\n",
       "      <td>vi out xxbos echo 100 &gt; / sys / class / backlight / intel_backlight / brightness xxbos nano xxmaj controllers / xxunk xxbos ls xxbos sudo nano / etc / network / interfaces xxbos help xxbos sudo service collectd start xxbos [ goalador@gatanda hhneuauf.de]$ bash : [ goalador@gatanda : command not found xxbos clear xxbos # xxunk xxbos tree xxbos vim .xresources xxbos vim palind.rb xxbos chmod xxunk xxmaj public / xxbos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-n xxunk xxbos ll xxbos # xxunk xxbos python xxbos cm --pkg auv_map xxbos 0mt+olwjfsja072upuhe7gwdlexqh0l59q4ohog+zjke1u74br9wo6kiv54iqgvs xxbos j=0;for i in * .jpg ; do printf -v new \" % 02d \" \" $ ( ( + + j ) ) \" ; mv -v -- \" $ i \" 2 - hh - xxunk \" ; done xxbos ls xxbos ls -al xxbos bzr push lp:~williams - jayson / + junk / bashprefs</td>\n",
       "      <td>xxunk xxbos ll xxbos # xxunk xxbos python xxbos cm --pkg auv_map xxbos 0mt+olwjfsja072upuhe7gwdlexqh0l59q4ohog+zjke1u74br9wo6kiv54iqgvs xxbos j=0;for i in * .jpg ; do printf -v new \" % 02d \" \" $ ( ( + + j ) ) \" ; mv -v -- \" $ i \" 2 - hh - xxunk \" ; done xxbos ls xxbos ls -al xxbos bzr push lp:~williams - jayson / + junk / bashprefs xxbos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0 xxbos pintos -j 1 -v -k --bochs --filesys - size=2 -p tests / vm / page - parallel -a page - parallel -p tests / vm / child - linear -a child - linear --swap - size=4 -- -q -f -ul=10 run page - parallel xxbos ls xxbos vim manage.py xxbos . / relacao 2 2 xxbos ls xxbos du . / download_img / -d 1 xxbos show options xxbos i</td>\n",
       "      <td>xxbos pintos -j 1 -v -k --bochs --filesys - size=2 -p tests / vm / page - parallel -a page - parallel -p tests / vm / child - linear -a child - linear --swap - size=4 -- -q -f -ul=10 run page - parallel xxbos ls xxbos vim manage.py xxbos . / relacao 2 2 xxbos ls xxbos du . / download_img / -d 1 xxbos show options xxbos i d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>temp / kodi.log xxbos [ goalador@gatanda hhneuauf.de]$ [ goalador@gatanda hhneuauf.de]$ [ goalador@gatanda hhneuauf.de]$ [ goalador@gatanda hhneuauf.de]$ [ goalador@gatanda hhneuauf.de]$ bash : [ goalador[goalador@gatanda hhneuauf.de]$ [ goalador@gatanda hhneuauf.de]$ bash : [ goalador@gatanda : command not found xxbos make all xxbos rm trabalho xxbos trabalho xxbos top xxbos pintos -v -k -t 60 --qemu --filesys - size=2 -p tests / vm / page - parallel -a page - parallel -p tests / vm /</td>\n",
       "      <td>/ kodi.log xxbos [ goalador@gatanda hhneuauf.de]$ [ goalador@gatanda hhneuauf.de]$ [ goalador@gatanda hhneuauf.de]$ [ goalador@gatanda hhneuauf.de]$ [ goalador@gatanda hhneuauf.de]$ bash : [ goalador[goalador@gatanda hhneuauf.de]$ [ goalador@gatanda hhneuauf.de]$ bash : [ goalador@gatanda : command not found xxbos make all xxbos rm trabalho xxbos trabalho xxbos top xxbos pintos -v -k -t 60 --qemu --filesys - size=2 -p tests / vm / page - parallel -a page - parallel -p tests / vm / child</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MyTokenizer(Transform):\n",
    "    def setups(self, items):\n",
    "        path = untar_data(URLs.IMDB)\n",
    "        self.tok =  Tokenizer.from_folder(path)\n",
    "        self.tok.setup(items)\n",
    "        \n",
    "    def encodes(self, txts):\n",
    "        with open(txts, 'r') as file:\n",
    "            content = file.read()\n",
    "        return self.tok(content)\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        return self.tok.decode(encoded)\n",
    "            \n",
    "class MyNumerizer(Transform):\n",
    "    def setups(self, items):\n",
    "        self.num = Numericalize()\n",
    "        self.num.setup(items)\n",
    "        self.vocab = self.num.vocab\n",
    "\n",
    "    def encodes(self, toks):\n",
    "        return self.num(toks)\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        return self.num.decode(encoded)\n",
    "    \n",
    "    \n",
    "    \n",
    "#path_test = untar_data(URLs.IMDB)\n",
    "path_test = '/home/chris/University/gnn_project/'\n",
    "tfms = [[MyTokenizer(),MyNumerizer()]]\n",
    "files = get_text_files(path_test, folders = ['data'])\n",
    "#files = get_text_files(path_test, folders = ['train'])\n",
    "print(files[0:5])\n",
    "#files = get_text_files(path, folders = ['train'])\n",
    "dsets = Datasets(files[:], tfms)\n",
    "print(dsets[0:5])\n",
    "dls = dsets.dataloaders(dl_type=LMDataLoader, bs=4)\n",
    "\n",
    "dls.show_batch(max_n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained('bert-base-uncased')\n",
    "\n",
    "config.vocab_size = len(dls.vocab)\n",
    "config.num_labels = len(dls.vocab)\n",
    "config.hidden_size = 132\n",
    "transformer = ShellTransformer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.600421</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>43:39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastprogress/fastprogress.py:73: UserWarning: Your generator is empty.\n",
      "  warn(\"Your generator is empty.\")\n"
     ]
    }
   ],
   "source": [
    "model = transformer\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "dls.to(device)\n",
    "\n",
    "learn = Learner(\n",
    "    dls, \n",
    "    model, \n",
    "    loss_func=CrossEntropyLossFlat(), \n",
    "    metrics=[accuracy]\n",
    ")\n",
    "\n",
    "learn.fit_one_cycle(1, 1e-3)\n",
    "\n",
    "#learn.export('mymodel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB)\n",
    "tok = Tokenizer.from_folder(path)\n",
    "tok.setup(txts)\n",
    "toks = txts.map(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = Numericalize()\n",
    "num.setup(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4491/600091294.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(token_ids).unsqueeze(0).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxbos ssh --help xxbos ls xxbos ls xxbos ls xxbos ls xxbos ls\n"
     ]
    }
   ],
   "source": [
    "mytokenizer = Pipeline([tok,num])\n",
    "\n",
    "# Define a function for text generation\n",
    "def generate_text(model, starting_text, max_length=10):\n",
    "    token_ids = mytokenizer(starting_text).to(device)\n",
    "    input_ids = torch.tensor(token_ids).unsqueeze(0).to(device)  \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs[:, -1, :]  \n",
    "            next_token_id = torch.argmax(logits, dim=-1)\n",
    "            token_ids = torch.cat((token_ids, next_token_id),dim=0)\n",
    "            input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0).to(device)], dim=-1)\n",
    "            if mytokenizer.decode(next_token_id) == 'xxboss':\n",
    "                break\n",
    "    return mytokenizer.decode(token_ids)\n",
    "\n",
    "# Generate text\n",
    "generated_text = generate_text(learn.model, \"ssh --help\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.3905, -6.9732,  2.0654,  ..., -6.2189, -6.3091, -6.3464],\n",
       "         [ 3.3691, -6.6982,  2.0184,  ..., -6.1612, -6.1415, -6.1101],\n",
       "         [ 3.9643, -6.8780,  3.5139,  ..., -6.0036, -5.1728, -5.2569]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[2, 2, 150]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
