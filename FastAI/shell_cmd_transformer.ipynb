{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pickle\n",
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = \"cuda:0\"\n",
    "else:\n",
    "  device = \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    dim_k = query.size(-1)\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / np.sqrt(dim_k)\n",
    "    if mask is not None:\n",
    "        if scores.shape[1] == mask.shape[1]:\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        else:\n",
    "            mask = torch.tril(torch.ones(scores.shape[1], scores.shape[1], dtype=torch.float)).unsqueeze(0).to(device)\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return weights.bmm(value)\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "        self.mask = torch.tril(torch.ones(head_dim, head_dim, dtype=torch.float)).unsqueeze(0).to(device)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        attn_outputs = scaled_dot_product_attention(\n",
    "            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state), self.mask)\n",
    "        return attn_outputs\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(embed_dim, head_dim, config.vocab_size) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n",
    "        x = self.output_linear(x)\n",
    "        return x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply layer normalization and then copy input into query, key, value\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        # Apply attention with a skip connection\n",
    "        x = x + self.attention(hidden_state)\n",
    "        # Apply feed-forward layer with a skip connection\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        return x\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size, \n",
    "                                             config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n",
    "                                                config.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        if input_ids.dtype != torch.int:\n",
    "            input_ids = input_ids.int()\n",
    "        # Create position IDs for input sequence\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.int).unsqueeze(0).to(device)\n",
    "        # Create token and position embeddings\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        # Combine token and position embeddings\n",
    "        embeddings = token_embeddings #+ position_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(config) \n",
    "                                     for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class ShellTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.decoder = TransformerDecoder(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)#[:, 0, :] # select hidden state of [CLS] token\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/chris/University/gnn_project/dataset', 'rb') as fp:\n",
    "    _ = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nmap\\n',\n",
       " 'nmap -v 10.1.26.4\\n',\n",
       " 'nmap -v 10.1.26.9\\n',\n",
       " 'ssh --help\\n',\n",
       " 'ssh 10.1.26.9\\n',\n",
       " 'ssh 10.1.26.9 admin/123456\\n',\n",
       " 'ssh --help\\n',\n",
       " 'ssh 10.1.26.9\\n',\n",
       " 'ssh -l admin 10.1.26.9\\n',\n",
       " 'ssh admin@admin 10.1.26.9\\n']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Given array of text elements\n",
    "texts = _\n",
    "\n",
    "# Create a folder to store the text files\n",
    "folder_path = '/home/chris/University/gnn_project/data/'\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# Write each non-empty text element to a separate file\n",
    "for i, text in enumerate(texts):\n",
    "    # Remove trailing newline character\n",
    "    text = text.rstrip('\\n')\n",
    "    \n",
    "    # Check if text is not empty after stripping newline\n",
    "    if text.strip():\n",
    "        file_path = os.path.join(folder_path, f'text_{i}.txt')\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = L(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203101"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Encode from Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m tfms \u001b[38;5;241m=\u001b[39m [[MyTokenizer(),MyNumerizer()]]\n\u001b[1;32m     30\u001b[0m files \u001b[38;5;241m=\u001b[39m get_text_files(path_test, folders \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 31\u001b[0m dsets \u001b[38;5;241m=\u001b[39m \u001b[43mDatasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m dls \u001b[38;5;241m=\u001b[39m dsets\u001b[38;5;241m.\u001b[39mdataloaders(dl_type\u001b[38;5;241m=\u001b[39mLMDataLoader, bs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     34\u001b[0m dls\u001b[38;5;241m.\u001b[39mshow_batch(max_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m~/gnn_project/.venv/lib/python3.10/site-packages/fastai/data/core.py:443\u001b[0m, in \u001b[0;36mDatasets.__init__\u001b[0;34m(self, items, tfms, tls, n_inp, dl_type, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    435\u001b[0m     items:\u001b[38;5;28mlist\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# List of items to create `Datasets`\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     tfms:MutableSequence\u001b[38;5;241m|\u001b[39mPipeline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# List of `Transform`(s) or `Pipeline` to apply\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    441\u001b[0m ):\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(dl_type\u001b[38;5;241m=\u001b[39mdl_type)\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtls \u001b[38;5;241m=\u001b[39m L(tls \u001b[38;5;28;01mif\u001b[39;00m tls \u001b[38;5;28;01melse\u001b[39;00m [TfmdLists(items, t, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m L(ifnone(tfms,[\u001b[38;5;28;01mNone\u001b[39;00m]))])\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_inp \u001b[38;5;241m=\u001b[39m ifnone(n_inp, \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtls)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/gnn_project/.venv/lib/python3.10/site-packages/fastai/data/core.py:443\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    435\u001b[0m     items:\u001b[38;5;28mlist\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# List of items to create `Datasets`\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     tfms:MutableSequence\u001b[38;5;241m|\u001b[39mPipeline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# List of `Transform`(s) or `Pipeline` to apply\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    441\u001b[0m ):\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(dl_type\u001b[38;5;241m=\u001b[39mdl_type)\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtls \u001b[38;5;241m=\u001b[39m L(tls \u001b[38;5;28;01mif\u001b[39;00m tls \u001b[38;5;28;01melse\u001b[39;00m [\u001b[43mTfmdLists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m L(ifnone(tfms,[\u001b[38;5;28;01mNone\u001b[39;00m]))])\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_inp \u001b[38;5;241m=\u001b[39m ifnone(n_inp, \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtls)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/gnn_project/.venv/lib/python3.10/site-packages/fastcore/foundation.py:98\u001b[0m, in \u001b[0;36m_L_Meta.__call__\u001b[0;34m(cls, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x,\u001b[38;5;28mcls\u001b[39m): \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gnn_project/.venv/lib/python3.10/site-packages/fastai/data/core.py:357\u001b[0m, in \u001b[0;36mTfmdLists.__init__\u001b[0;34m(self, items, tfms, use_list, do_setup, split_idx, train_setup, splits, types, verbose, dl_type)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_setup:\n\u001b[1;32m    356\u001b[0m     pv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSetting up \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtfms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, verbose)\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_setup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_setup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gnn_project/.venv/lib/python3.10/site-packages/fastai/data/core.py:386\u001b[0m, in \u001b[0;36mTfmdLists.setup\u001b[0;34m(self, train_setup)\u001b[0m\n\u001b[1;32m    384\u001b[0m         x \u001b[38;5;241m=\u001b[39m f(x)\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mtype\u001b[39m(x))\n\u001b[0;32m--> 386\u001b[0m types \u001b[38;5;241m=\u001b[39m L(t \u001b[38;5;28;01mif\u001b[39;00m is_listy(t) \u001b[38;5;28;01melse\u001b[39;00m [t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtypes)\u001b[38;5;241m.\u001b[39mconcat()\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretty_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m types])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "class MyTokenizer(Transform):\n",
    "    def setups(self, items):\n",
    "        path = untar_data(URLs.IMDB)\n",
    "        self.tok =  Tokenizer.from_folder(path)\n",
    "        self.tok.setup(items)\n",
    "        \n",
    "    def encodes(self, txts):\n",
    "        with open(txts, 'r') as file:\n",
    "            content = file.read()\n",
    "        return self.tok(content)\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        return self.tok.decode(encoded)\n",
    "            \n",
    "class MyNumerizer(Transform):\n",
    "    def setups(self, items):\n",
    "        self.num = Numericalize()\n",
    "        self.num.setup(items)\n",
    "        self.vocab = self.num.vocab\n",
    "\n",
    "    def encodes(self, toks):\n",
    "        return self.num(toks)\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        return self.num.decode(encoded)\n",
    "    \n",
    "limit = 10000\n",
    "path_test = '/home/chris/University/gnn_project/'\n",
    "tfms = [[MyTokenizer(),MyNumerizer()]]\n",
    "files = get_text_files(path_test, folders = ['data'])\n",
    "dsets = Datasets(files[:limit], tfms)\n",
    "dls = dsets.dataloaders(dl_type=LMDataLoader, bs=4)\n",
    "\n",
    "dls.show_batch(max_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 1400 ['xxunk', 'xxpad', 'xxbos', 'xxeos', 'xxfld', 'xxrep', 'xxwrep', 'xxup', 'xxmaj', '/', 'ls', '-', ':', 'sudo', '\"', 'git', '.', 'vim', '[', 'bash', 'goalador@gatanda', \"'\", '\\\\', 'hhneuauf.de]$', '#', 'command', 'not', 'vi', 'found', 'cat', 'make', 'nano', 'rm', 'clear', 'python', 'install', '|', 'll', 'apt', '3', '*', '>', 'ssh', 'etc', '-a', '0', 'add', '$', 'exit', '1', ';', '-p', 'bin', 'cd', 'grep', 'home', '=', 'status', '-l', '..', '-i', 'echo', '~', '&', '4', 'get', '-f', 'docker', 'pwd', '-u', 'push', 'run', 'nmap', '-v', 'mv', ',', '-r', 'commit', '172.18.1.5', '-h', 'set', 'master', '-m', ')', '{', 'ps', 'usr', '-d', 'origin', 'find', '}', 'mkdir', 'config', 'app', '(', 'ifconfig', 'node', 'cp', 'su', 'chmod', '-ltr', '`', 'less', '-t', 'log', '-s', '2', '-o', 'server', '-la', ']', 'dev', 'man', 'lib', 'python3', 'scp', 'a', 'test', 'php', 'w', 'root', 'update', 'service', 'tar', 'x', 'systemctl', '-rf', 'sh', '.invoices2019.zip', 'remote', 'pull', 'gcc', 'exploit', 'restart', 'clean', '-c', '--help', '-n', 'fcrackzip', 'the', '-k', 'wget', 'var', '-name', '+', 'tests', '%', 'java', 'src', 'page', 'ping', 'ansible', 'args', 'mysql', '.bashrc', 'all', 'trabalho0304', 'tail', 'search', 'share', 'parallel', 'top', 'manage.py', 'curl', 'checkout', 'ruby', '--', '.ssh', 'id_rsa', '5', 'opt', 'eth0', 'pip', 'pi', 'primeiro', 'http', 'pintos', 'l', '7', '-q', 'nohup.out', 'start', 'list', 'pacman', '_', 'local', 'init', 'reboot', 'linux', '.config', 'single', 'teste', '--global', 'show', 'g', 'stop', 'tmp', 'v', 'source', 'iptables', 'new', 'd', 's', 'personal', 'la', '--filesys', 'size=2', 'vm', '|grep', 'dir', 'admin@10.1.26.9', '-e', 'proc', 'use', '-al', 'hacking', 'edx', 'edxapp', 'passwd', 'build', 'print', 'history', 'diff', 'bundle', 'admin', 'locate', 'child', 'linear', 'lhost', 'templates', 'for', 'file', '-lrt', 'user', '-sv', 'which', 'options', 'in', 'db', 'awk', 'f', 'xzvf', 'javac', '-y', 'rails', '172.18.1.5:1', 'touch', 'tg', 'data', '!', 'c', 'downloads', 'configure', 'john', 'clone', 'theme', 'perl', 'sed', '<', 'auv_map', 'localhost', 'mdk', 'scan.tgz', 'corpwebsite', 'makefile', 'to', 'main', 'ip', '-b', 'invoices2019.zip', '-j', 'playbook', 'exec', 'emacs', 'bzr', '10', 'sessions', 'network', 'gulp', 'cut', 'documents', 'lo', 'cve_2019_15107.py', 'uname', 'kill', 'printf', 'pkill', 'starterkit]$', 'help', 'branch', 'hosts', 'qemu', 'nox', 'storage', 'sort', 'spec', 'i', 'and', 'more', \"'s\", 'up', 'isitools', 'msfconsole', 'play.sh', 'b', 'npm', 'syscoind', 'key', 'gwsh', 'oc', '--version', 'catalog', 'rhosts', 'yum', '10.1.135.83', 'jenkins', 'main.java', 'df', 'name', 'type', 'f.tgz', '60', 'trabalho', 'mount', 'netstat', 'tasks', '-x', '24', 'ifup', 'ssh2john.py', '.xresources', 'cache', 'head', 'gavana.uv.ro', '--qemu', 'if', 'scripts', 'env', '-type', 'shutdown', '58.34.244.67', 'nc', 'host', 'view', 'sxhkd', 'path', '6', 'gunicorn_start', '-ss', 'user.name', 'nohup', 'i3', '127.0.0.1', '8', 'mongod', 'do', 'qasnap', 'center_src', 'xargs', 'git@github.com', 'hello', 'watch', 'migrate', '-9', 'g++', 'rockyou.txt', 'ifdown', 'cpuinfo', 'ppgsigc', 'zsh', 'apache2', '--swap', 'size=4', 'lms', 'wal', 'index.php', 'user.email', 'done', 'wordlist.txt', '--cpu', '22', 'userprog', 'stranger', 'as', 'out', 'interfaces', 'public', 'cm', 'images', 'wordlists', '-jar', 'mplayer', 'readme.md', 'webmin_backdoor', 'cli', 'concataxofastqfiles.sh', 'udp.pl', 'html', '-w', 'du', 'newbie', 'printinfo.c', 'centos-5', 'controller', 'brew', 'desktop', 'default', 'scan', 'rept.rb', 'rake', 'runserver', 'users', 'image', 'collectd', 'eve@10.1.17.4', 'wlan0', '--pkg', 'nslookup', '--all', 'fi', 'upgrade', 'fastqc', 'so', 'modification', 'copy', 'redis', 'openstack', 'shm.c', 'rport', 'telegram', 'chg', 'a.out', 'trizen', '--bochs', 'q', 'server_unit_test.py', 'compose', 'nginx', 'temp', 'script', 'addview.c', 'index.html', 'check', 'common', 'fg', 'application', 'ubuntu', 'tools', 'things', 'date', 'init.d', 'true', 'cv', 'media', 'artisan', 'composer', '-1', 'workspace', 'mnt', 'ffmpeg', 'rvm', 'getnotice.php', 'dnf', 'comando', 'reload', '53', 'rosrun', '10.1.26.9', 'rubocop', 'views.py', 'server.c', '-lpthread', 'tessel', 'rspec', 'genetic', 'learn', '-fr', '0057home', '--clean', 'htop', 'tf', 'wiload.sh', '…', 'manager', 'you', 'filter', '.vimrc', '20', 'setup', 'rxeflash.sh', 'study.py', 'arg', 'axomiketest', 'ss', 'create', '--name', 'flow', 'sys', 'stack', 'h', 'chown', '3490', 'seq', 'tcp', 'first', '-lh', 'machine', 'firefox', '23', 'terraform', 'valgrind', 'unicorn', 'l.', '||', '.git', 'on', 'trabalho_03_04_12.sh', 'map_builder', 'views', 'activate', 'tg-server.pub', 'authorized_keys', 'csapp.c', 'matching', 'ad_placement_spec.rb', '-g', 'unzip', 'pictures', 'post', 'export', '135', 'result', 'import', 'doc', 'sbin', '.bash_profile', 'run.sh', 'accept', 'project', 'web', 'aux', '--set', 'gdb', 'info', 'admin@10.1.26.9:~', 'crontab', 'polybar', 'be', 'module', 'template', 'megastore', 'trab1', 'directory', 'epadmin', 'null', 'release', 'mongodb', 'conf', 'rhost', 'open', 'themes', 'raccoongang_theme', 'marvel', 'ginkgo', 'r', 'ln', 'fetch', 'dist', 'multiple', 'read', 'terror', 'who', 'logout', '--save', 'gem', 'namp', 'repository', 'cdrom', 'from', 'thunar', '-ef', 'eclipse', 'merge', 'iwconfig', 'profile', 'scriptrabalho', 'adb', 'jar', 'genetic.jar', 'samba', 'of', 'sqainfra', 'lex.yy.c', 'y.tab.c', 'wc', 'proc.c', 'css', 'addview', 'getnoticetype1.php', 'clonestoread', 'stdout', 'yaourt', 'connect', 'babymarvel', 'trabalho.sh', 'chrome', 'no', 'bjohnston', 'uniq', 'pdommeti', 'gnome', 'xvf', 'm', 'alas', 'ml12087', 'killall', 'adding', 'net', 'build_package.sh', 'firstpython.py', 'nofile', 'telnet', 'remove', '-ul=11', 'n', 'dig', 'led.js', 'rev', 'down', 'marathon', 'cmd', 'ant', 'reset', 'sleep', 'arquivotrabalho', 'daemon', 'generate', 'in01', 'system', 'json', '139', '16', 'hoje', 'root@attacker', 'whoami', 'where', 'httpd', 'trab', 'recipes', 'cyg', '-lt', '-iname', 'services', 'archives', 'pianobar', 'apache', 'server-rhel.yml', 'image.jpg', 'error', 'nagios', 'gedit', 'pgrep', 'webmin', 'serve', '^', 'index.js', 'id_rsa.pub', 'whereis', 'dns', 'debian', 'some', 'now', 'readme', 'zxvf', 'sonarqube', 'main.yml', 'tag', 'matlab', 'with', 'idade', '--list', 'docker-compose.yml', 'iwlist', 'proj4.asm', 'error.log', 'dpkg', 'conky', '.vim', 'ryo.tar.gz', 'main.c', 'rmi', 'platform', 'pulsar', 'extension', '04', '--refresh', 'login', '11', 'vilker3_pipe02', 'vilker3_pipe02.c', '-am', 'makemigrations', '.sh', 'back', 'kernel', 'sc_main.py', 'packages', 'test1.yml', 'map', 'navigation', 'navigation.html', 'include', '09', 'api', '?', 'integer', 'client', 'asm.c', 'content', 'register', 'mosquitto_pub', '.bash_history', '-version', 'while', 'time', '755', 'tomcat', 'send.py', 'main.py', '100', 'file.txt', '@', 'da.py', 'shell', 'gtk', '-ii', 'auxiliary', 'scanner', 'rc.local', '-exec', 'file1.py', 'root@10.1.26.23', 'sqlmap', 'intel', 'compilers_and_libraries', 'stulogin.dcs.shef.ac.uk', 'psql', '-ls', 'google.com', 'examples', 'test1', 'deurbel2.py', 'puppet', 'agent', 'object', 'led', 'java.yml', 'rofi', '-show', 'overcloud', 'stable', 'variavel', 'mpirun', 'bg', 'line', '-std', 'c++11', 'bitbucket', 'sftp', 'target', '--wordlist', 'startx', 'urxvt', 'xft', 'mobile', 'go', 'yacc', 'parse.y', 'sysctl', 'refresh_hb', 'apachectl', 'az', '01', 'p', 'mpeg1video', 'johnpi', 'test.c', 'isiseqruns', 'ronaxotrinity', 'account', 'mpd', '-path', 'devel', 'hash', 'test.pl', 'password', 'palind.rb', 'libattr.so', 'nmbd', 'is', 'virtualenv', '100311_axo4_evita', 'tcpdump', 'backup', 'mode', 'ipacct.sh', 'restore', 'test.py', 'trinity', 'feature', 'convert', 'screen', 'work', 'getinfo', 'sql', 'keygen', 'postgresql', '0.0.0.0:20568', 'ipython', 'con2.java', 'dc', '12', '600', 'bash.c', 'sl', 'jenkinsfile', 'combined.py', 'toto.sh', '--env', '--add', 'mpd.conf', 'none', 'session', 'uberwriter', 'example', 'syntax', 'my', 'vendor', 'msf', 'test2', '--wordlist=', '19', '.yahoo.com', 'testing.sh', '.xinitrc', 'teste.sh', 'r-2.15.2', 'edx.log', 'userinfo.sh', 'fortune', 'mongod.conf', 'borrow_lend.rs', 'unix', '0057damon', '25', 'what-time-script.sh', 'scaffold', 'chef', '-z', 'default.rb', 'nid', 'tx', 'reference', 'psybnc', 'sources.list.d', 'server.php', 'index', 'chrysalis', 'trinity.timing', 'a.txt', 'yii', 'socket', '.cpp', 'username', 'slider', 'by', '.mythtv', '.w++', '-ef|grep', 'grading', 'code', 'ovs', 'xfce4', 'avahi', 'your', 'have', 'install.sh', 'check_mk', '-ul=25', 'lxml', 'inventory', 'echo_selectserv.c', 'mydata.xlsx', 'cfg', 'resolv.conf', 'bus', 'br', 'notify', 'mod', '3_axo2pe_eva', '10.1.26.0', 'rebase', 'openssl', '.i3', '0057sean', '0057', 'led.py', '-sn', 'gzip', 'eve', 'both.fa', 'unexpected', 'ssl', 'bybys.c', '-aux', 'addnode', 'c8', 'f7:33:33', 'e2', 'f7', 'uuids', '-8', '-00805f9b34fb', 'then', 'gdpr', 'es', 'extractor', 'httpd.conf', 'essid', 'route', 'rostopic', 'rep1.rb', 'secrets', 'metasploit', 'mainapp.py', 'maxspan', 'l1', '-np', 'gpaw', '-zxvf', 'models.py', 'repair', 'myfirstplaybook.yml', 'ppa', 'svg', '2018', 'pz.lua', 'sources.list', 'id', '-all', '320x240', 'video4linux2', 'video0', 'rmdir', '0.0.0.0', 'engine', '.local', '.ssh]$', '22.py', 'utils', 'urls.py', 'cmakelists.txt', '-rn', 'deurbel', '1234', 'broadcom', 'execute.py', 'https', 'netctl', 'port', '80', 'libinput-gestures.conf', '64', 'sdb1', 'thackbarth', 'umount', 'purge', 'rules.v4', 'out.txt', 'added', '8080', 'jobs', '--track', 'origins', 'yes', 'clnt', '-altr', '537.36', 'rw', 'fc', 'smbd', 'set.sh', '4096', '15107', 'eth1', 'networking', 'ranger', 'conky.conf', 'snap', '.dotfiles', 'gpu.job', 'webapp', 'ctf', 'ro', 'sshd_config', 'nr', '66.34', 'in02', 'so_listing_tabs', 'default_items.twig', 'models', 'terminal', 'packages.yml', 'baremetal', 'ws', 'heat', 'active', 'spotify', 'kafka', 'repo', 'syscall', 'xrandr', 'frame.c', 'rlg327', '-lah', 'oh', 'yay', 'alice@172.18.1.5', 'diretorio', 'xml', 'group', 'exist', 'exit(0', 'stat', 'test2.py', 'ubuntu@ip-172', '31', '102:~', 'myproject$', 'mvim', 'readinfo.py', 'aravind.rb', 'reseed', 'nevens.sh', 'hash.txt', 'check_mk_agent', 'develop', 'stopwatchui.py', 'runs', 'like', 'convertfastqtofastaright.sh', 'chong@10.1.17.4', 'y29yb25h', 'vagrant', 'clmb', 'cmk', '5phi1jf+wojvarkqlzffamry0rhu6tsqtydm2qycssze1fhldxgnkpathyz1suhe', 'j', '-crf', '512k', 'aac', '-strict', '-2', '.mp4', 'software', 'test.lua', 'string', 'filesearch.c', '123,\"type', 'saveuserroot', '-f2', 'tee', 'djynp3xqcknflivtbwvsshqnhsn6zcl8btefdrsf6kaj6qm1', 'cafl73otlgzyr7', 'grafana', 'akamake', '-quit', 'blue', 'hostname', 'sd', 'boot', 'ep', 'kwset.c', 'fn', 'unset', '-empty', '-printf', 'forstalkapp.com', 'webappcreations', 'attach', 'mpicc', 'gemset', 'rrdtool', 'geanclm.sh', 'dump', 'br_ovs', 'howodd.sh', 'wlan_normal.sh', '--trace', 'mysum.sh', 'alias', 'boost', 'deluge', 'modules', '-help', '-sch', 'seed', 'acpi', 'ibm', 'fan', 'jq', 'chong', '-uroot', 'chsh', 'ks', 'objdump', 'router', 'library', 'edquota', 'air1', 'cfg.d', 'swift', 'machineip', '--upgrade', '-syu', '--topic', 'readlink', 'color', 'structure', '120706_ssprimaryhepmicrorna', '92_evita', '--nummon', 'php5', 'tab', '--force', '--continue', 't', 'relacaonumeros', 'pg', 'test3', 'github.com', 'helloworld', 'wallpapers', 'class', 'clones.git', 'groups', 'ap', '.htaccess', 'root@sqainfra-docker01.sqaextranet.akamai.com', 'forceexploit', 'chapter', '--no', 'unicorn.log', 'youtube', 'sensorcheck.py', 'arquivotrabalho2', 'certs', 'devices', 'popel.py', 'smb.conf', 'substitution', 'mesos-migrate-dc.py', '--colddc', '--hotdc', 'hello.c', 'wireshark', '-testnet', 'tony', 'build_binary.sh', 'room602', 'liu567890', 'auwx', 'publico_ins', 'usrbin', 'initialadminpassword', 'expose', 'dunst', 'johnkey', 'lms.env.json', 'courseware', 'python2', 'x86_64', 'nbgrader', 'logs', 'named', '0.0.0.0:8', 'hadoop', 'fs', 'info_pc1_cesar', 'sample', '2015', 'latest', 'addon', 'does', \"n't\", 'auxww', 'post_handler.c', '--node', 'webappcreations.diessbacherbablde', '172.18.1.1', 'aluno', 'flag.txt', 'bzimage', 'lightsensor.py', 'can', 'loops', 'axoprojecttest', 'bower', '-st', '-p-', 'zerador.sh', 'saiyuuki@', '.31yuuki.com', 'mac_rsa', 'cool-tree.java', 'tree', 'freqtrade', '0.05', 'running', 'output', '320', '240', 'axo_left.fq', 'state', 'ffserver.conf', 'goalad[goalador@gatanda', 'h>[goalador@gatanda', 'makefile.unix', 'rbenv', 's_1_1.fq.gz', 'vault', 'maior', 'z', 'quit', '.google.com', 'full_assembly', 'tissue_types', 'alan.zip', 'vsctl', '-stop', '.m2', 'express', 'cloud', 'projects', 'voyager', 'main.tf', 'public_html', 'neigh', 'star', 'digital', 'videos', 'mcopy', 'qsub', 'qsub_phase1a.sh', 'resource', 'serv', 'goalador@gatandabash', '--forms', 'pass', 'join', 'i386', 'delete', 'pro', 'orobardet', 'lsof', '--output', 'upstream', 'sshd', 'waitpid', 'sort.rb', 'dl', 'e]710;%s', '007', 'pixelsize=12', 'php_playbook.yml', 'dungeon.c', 'bc', 'y2o0rvcjsin3d2h9f0fp0xvc3ehzd7ct2hvzev6x3kulfdwehkuf+7wuetjfsmvq', 'getnoticetype6.php', '13', '9', 'browser', 'collectd.conf', 'sdb', 'average', 'ledcontrol.py', '-update', 'enable', 'ds', 'google', 'package', 'main.rb', '.h', 'dns.py', '-p5353', '-ax', 'sda', 'pylast', 'redshift', 'auto', 'files', '05', 'admin@server', 'grub', 'index.html.slim', 'postgres', 'lab01.txt', 'l2', 'sqlite3', 'namd-tutorial-160715.tgz', 'language', 'windows', 'connection', 'keep', '--data', 'binary', 'mac', 'dmc', 'playbooks', 'playbook1.yml', '.c', 'debug.log', 'mongo', 'usertests.c', 'rsa', 'login.php', '-j2', '-c1', 'bavail', 'wlan_ap.sh', 'zerador', 'write.c', 'root@sqainfra-docker05.sqaextranet.akamai.com', 'when', 'cve:2019', 'key.txt', 'stash', 'smbcredentials', 'launch.sh', '775', 'window', 'container', 'st', 'stage', 'libssh_auth_bypass', 'patch', '--exe', '--np', '--fi', '--fo', 'test_query_string_9', 'version', 'ref01', 'wifi', 'if2', '.themes', 'run_tests.sh', '196', 'pgcc', '-acc', '-minfo', 'accel', 'laplace_data_manag_acc.c', 'sbatch', 'cc', '.cure.data', 'xrdb', '2017', 'confirmed', 'sendmany', 'saph8g6s7efresnqfumruxn6eap1pw2ztu', 'acer', '-pn', 'mvn', 'cve-2019', 'sda1', 'sqa2', '2019', 'systemd', 'seeds.rb', 'dependencies', 'pip3', '0xc00221c5', 'cmake', 'msfdb', 'arch', 'components', 'config.php', 'rj6xi5cibddo83vdqhlyauawtlv', 'wupqsrc9xsk', 'y2dljzqhad+qkwbm98jrcagx', 'gradlew', 'setupdecompworkspace', 'colors-rofi-dark.rasi', 'product', '9][0', 'pt', 'grow', 'element', 'xxfake']\n"
     ]
    }
   ],
   "source": [
    "print(\"Length:\",len(dls.vocab),dls.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>▁n▁o▁d▁e▁b▁r▁e▁w▁a▁l▁i▁a▁s▁d▁e▁f▁a▁u▁l▁t▁v▁0▁.▁8▁.▁1▁5▁l▁s▁s▁h▁o▁w▁o▁p▁t</td>\n",
       "      <td>n▁o▁d▁e▁b▁r▁e▁w▁a▁l▁i▁a▁s▁d▁e▁f▁a▁u▁l▁t▁v▁0▁.▁8▁.▁1▁5▁l▁s▁s▁h▁o▁w▁o▁p▁t▁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>▁3▁2▁5▁3▁0▁4▁2▁m▁a▁k▁e▁s▁u▁d▁o▁s▁e▁r▁v▁i▁c▁e▁d▁o▁c▁k▁e▁r▁h▁e▁l▁p▁R▁U▁N▁A</td>\n",
       "      <td>3▁2▁5▁3▁0▁4▁2▁m▁a▁k▁e▁s▁u▁d▁o▁s▁e▁r▁v▁i▁c▁e▁d▁o▁c▁k▁e▁r▁h▁e▁l▁p▁R▁U▁N▁A▁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>▁a▁r▁e▁m▁e▁t▁a▁l▁n▁o▁d▁e▁l▁i▁s▁t▁l▁e▁s▁s▁c▁u▁t▁o▁f▁f▁.▁p▁y▁m▁a▁n▁t▁p▁u▁t</td>\n",
       "      <td>a▁r▁e▁m▁e▁t▁a▁l▁n▁o▁d▁e▁l▁i▁s▁t▁l▁e▁s▁s▁c▁u▁t▁o▁f▁f▁.▁p▁y▁m▁a▁n▁t▁p▁u▁t▁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>▁e▁x▁i▁t▁m▁k▁d▁i▁r▁r▁u▁n▁l▁s▁t▁r▁i▁n▁i▁t▁y▁_▁t▁e▁s▁t▁/▁v▁i▁c▁o▁n▁c▁a▁t▁A</td>\n",
       "      <td>e▁x▁i▁t▁m▁k▁d▁i▁r▁r▁u▁n▁l▁s▁t▁r▁i▁n▁i▁t▁y▁_▁t▁e▁s▁t▁/▁v▁i▁c▁o▁n▁c▁a▁t▁A▁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>▁a▁l▁l▁r▁m▁-▁r▁f▁e▁x▁p▁.▁t▁g▁z▁g▁i▁t▁c▁o▁m▁m▁i▁t▁-▁m▁\"▁u▁p▁d▁a▁t▁e▁s▁\"▁e</td>\n",
       "      <td>a▁l▁l▁r▁m▁-▁r▁f▁e▁x▁p▁.▁t▁g▁z▁g▁i▁t▁c▁o▁m▁m▁i▁t▁-▁m▁\"▁u▁p▁d▁a▁t▁e▁s▁\"▁e▁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>▁i▁t▁]▁$▁&gt;▁&gt;▁[▁g▁o▁a▁l▁a▁d▁o▁r▁@▁g▁a▁t▁a▁n▁d▁a▁c▁o▁r▁p▁w▁e▁b▁s▁i▁t▁e▁-▁s</td>\n",
       "      <td>i▁t▁]▁$▁&gt;▁&gt;▁[▁g▁o▁a▁l▁a▁d▁o▁r▁@▁g▁a▁t▁a▁n▁d▁a▁c▁o▁r▁p▁w▁e▁b▁s▁i▁t▁e▁-▁s▁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>▁.▁/▁s▁c▁a▁n▁2▁1▁6▁.▁8▁9▁;▁l▁a▁d▁o▁r▁@▁g▁a▁t▁a▁n▁d▁a▁h▁h▁n▁e▁u▁a▁u▁f▁.▁b</td>\n",
       "      <td>.▁/▁s▁c▁a▁n▁2▁1▁6▁.▁8▁9▁;▁l▁a▁d▁o▁r▁@▁g▁a▁t▁a▁n▁d▁a▁h▁h▁n▁e▁u▁a▁u▁f▁.▁b▁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>▁m▁e▁s▁o▁s▁p▁h▁e▁r▁e▁/▁m▁a▁r▁a▁t▁h▁o▁n▁/▁a▁p▁i▁/▁S▁y▁s▁t▁e▁m▁R▁e▁s▁o▁u▁r</td>\n",
       "      <td>m▁e▁s▁o▁s▁p▁h▁e▁r▁e▁/▁m▁a▁r▁a▁t▁h▁o▁n▁/▁a▁p▁i▁/▁S▁y▁s▁t▁e▁m▁R▁e▁s▁o▁u▁r▁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>▁4▁-▁0▁6▁-▁4▁4▁_▁e▁d▁i▁t▁.▁m▁k▁v▁.▁m▁k▁v▁s▁u▁d▁o▁n▁e▁t▁s▁t▁a▁t▁-▁p▁l▁a▁n</td>\n",
       "      <td>4▁-▁0▁6▁-▁4▁4▁_▁e▁d▁i▁t▁.▁m▁k▁v▁.▁m▁k▁v▁s▁u▁d▁o▁n▁e▁t▁s▁t▁a▁t▁-▁p▁l▁a▁n▁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>▁r▁t▁i▁e▁s▁-▁c▁o▁m▁m▁o▁n▁:▁q▁a▁d▁d▁u▁s▁e▁r▁e▁c▁o▁l▁l▁e▁c▁t▁l▁s▁v▁i▁m▁/▁u</td>\n",
       "      <td>r▁t▁i▁e▁s▁-▁c▁o▁m▁m▁o▁n▁:▁q▁a▁d▁d▁u▁s▁e▁r▁e▁c▁o▁l▁l▁e▁c▁t▁l▁s▁v▁i▁m▁/▁u▁</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MyTokenizer(Transform):\n",
    "    def setups(self, items):\n",
    "        self.tok = SubwordTokenizer(vocab_sz=200)\n",
    "        self.tok.setup(items)\n",
    "        \n",
    "    def encodes(self, txts):\n",
    "        with open(txts, 'r') as file:\n",
    "            content = file.read()\n",
    "        flattened_list = [item for sublist in list(self.tok(content)) for item in sublist]\n",
    "        return flattened_list\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        decoded_values = TitledStr(''.join(encoded))\n",
    "        return  decoded_values\n",
    "            \n",
    "class MyNumerizer(Transform):\n",
    "    def setups(self, items):\n",
    "        self.num = Numericalize()\n",
    "        self.num.setup(items)\n",
    "        self.vocab = self.num.vocab\n",
    "        \n",
    "\n",
    "    def encodes(self, toks):\n",
    "        return self.num(toks)\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        return self.num.decode(encoded)  \n",
    "\n",
    "limit = 1000\n",
    "path_test = '/home/chris/University/gnn_project/'\n",
    "tfms = [[MyTokenizer(),MyNumerizer()]]\n",
    "files = get_text_files(path_test, folders = ['data'])\n",
    "dsets = Datasets(files[:limit], tfms)\n",
    "dls = dsets.dataloaders(dl_type=LMDataLoader, bs=64)\n",
    "\n",
    "dls.show_batch(max_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 80 ['xxunk', 'xxpad', 'xxbos', 'xxeos', 'xxfld', 'xxrep', 'xxwrep', 'xxup', 'xxmaj', '▁', 'a', 't', 'o', 's', 'e', 'i', 'n', 'r', 'l', 'c', '-', 'p', 'd', 'm', '.', 'g', '1', 'h', '2', 'f', 'u', '0', 'b', 'v', '/', '8', '_', '3', 'k', ':', 'y', '4', '7', '5', 'w', 'R', 'D', '9', '6', 'V', 'T', 'S', 'B', 'H', 'E', '#', '\"', 'A', 'I', 'x', 'N', 'z', '@', 'P', ';', 'O', 'q', '`', 'X', 'G', 'Z', '|', 'xxfake', 'xxfake', 'xxfake', 'xxfake', 'xxfake', 'xxfake', 'xxfake', 'xxfake']\n"
     ]
    }
   ],
   "source": [
    "print(\"Length:\",len(dls.vocab),dls.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gitconfig--global--add</td>\n",
       "      <td>itconfig--global--add</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>web.browser/opt/firefox/fire</td>\n",
       "      <td>web.browser/opt/firefox/firef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>foxclearwgetpilotu.110mb.com/</td>\n",
       "      <td>oxclearwgetpilotu.110mb.com/R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RaZvaNBv.tgz;tarxvfRaZvaNBv</td>\n",
       "      <td>aZvaNBv.tgz;tarxvfRaZvaNBv.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.tgz;rm-rfRaZvaNBv.tgz;cd</td>\n",
       "      <td>tgz;rm-rfRaZvaNBv.tgz;cd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>.tmp;./startprintubutignome-t</td>\n",
       "      <td>tmp;./startprintubutignome-te</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>erminal--role\"gnome\"sudop</td>\n",
       "      <td>rminal--role\"gnome\"sudopy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ythongitcommit-m\"firstc</td>\n",
       "      <td>thongitcommit-m\"firstco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ommit\"ifconfig#1357789494ls</td>\n",
       "      <td>mmit\"ifconfig#1357789494ls-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-lpython3xxunkombined.pycdmysql</td>\n",
       "      <td>lpython3xxunkombined.pycdmysql</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MyTokenizer(Transform):\n",
    "    def setups(self, items):\n",
    "        self.tok = BaseTokenizer()\n",
    "        #self.tok.setup(items)\n",
    "        \n",
    "    def encodes(self, txts):\n",
    "        with open(txts, 'r') as file:\n",
    "            content = file.read()\n",
    "        flattened_list = [item for sublist in list(self.tok(content)) for item in sublist]\n",
    "        return flattened_list\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        decoded_values = TitledStr(''.join(encoded))\n",
    "        return  decoded_values\n",
    "            \n",
    "class MyNumerizer(Transform):\n",
    "    def setups(self, items):\n",
    "        self.num = Numericalize()\n",
    "        self.num.setup(items)\n",
    "        self.vocab = self.num.vocab\n",
    "        \n",
    "\n",
    "    def encodes(self, toks):\n",
    "        return self.num(toks)\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        return self.num.decode(encoded)  \n",
    "\n",
    "limit = 100\n",
    "path_test = '/home/chris/University/gnn_project/'\n",
    "tfms = [[MyTokenizer(),MyNumerizer()]]\n",
    "files = get_text_files(path_test, folders = ['data'])\n",
    "dsets = Datasets(files[:limit], tfms)\n",
    "dls = dsets.dataloaders(dl_type=LMDataLoader, bs=64)\n",
    "\n",
    "dls.show_batch(max_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 80 ['xxunk', 'xxpad', 'xxbos', 'xxeos', 'xxfld', 'xxrep', 'xxwrep', 'xxup', 'xxmaj', '', 'a', 't', 'o', 's', 'e', 'i', 'n', 'r', 'l', 'c', '-', 'p', 'd', 'm', '.', 'g', '1', 'h', '2', 'f', 'u', '0', 'b', 'v', '/', '8', '_', '3', 'k', ':', 'y', '4', '7', '5', 'w', 'R', 'D', '9', '6', 'V', 'T', 'S', 'B', 'H', 'E', '#', '\"', 'A', 'I', 'x', 'N', 'z', '@', 'P', ';', 'O', 'q', '`', 'X', 'G', 'Z', '|', 'xxfake', 'xxfake', 'xxfake', 'xxfake', 'xxfake', 'xxfake', 'xxfake', 'xxfake']\n"
     ]
    }
   ],
   "source": [
    "print(\"Length:\",len(dls.vocab),dls.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sudo raspi-configfind ./ -nam</td>\n",
       "      <td>udo raspi-configfind ./ -name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e \"xxunk.fq.gz\" |zcat | grep xxunkHxxunkI</td>\n",
       "      <td>\"xxunk.fq.gz\" |zcat | grep xxunkHxxunkIxxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxunkSIxxunklador@gatanda hhneuauf.ba</td>\n",
       "      <td>SIxxunklador@gatanda hhneuauf.bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sh: bash:: command not founds</td>\n",
       "      <td>h: bash:: command not foundsu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>udo ifdown eth9bash: xxunkgoalado</td>\n",
       "      <td>do ifdown eth9bash: xxunkgoalador</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>r@gatanda: command not found#</td>\n",
       "      <td>@gatanda: command not found#1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1517115588pwdwget pilotu.110m</td>\n",
       "      <td>517115588pwdwget pilotu.110mb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b.com/RaZvaNBv.tgz;tar xvf Ra</td>\n",
       "      <td>.com/RaZvaNBv.tgz;tar xvf RaZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ZvaNBv.tgz;rm -rf RaZvaNBv.tg</td>\n",
       "      <td>vaNBv.tgz;rm -rf RaZvaNBv.tgz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>z;cd .tmp;./start printubutic</td>\n",
       "      <td>;cd .tmp;./start printubutica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MyTokenizer(Transform):\n",
    "    def setups(self, items):\n",
    "        self.tok = SpacyTokenizer()\n",
    "        #self.tok.setup(items)\n",
    "        \n",
    "    def encodes(self, txts):\n",
    "        with open(txts, 'r') as file:\n",
    "            content = file.read()\n",
    "        flattened_list = [item for sublist in list(self.tok(content)) for item in sublist]\n",
    "        return flattened_list\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        decoded_values = TitledStr(''.join(encoded))\n",
    "        return  decoded_values\n",
    "            \n",
    "class MyNumerizer(Transform):\n",
    "    def setups(self, items):\n",
    "        self.num = Numericalize()\n",
    "        self.num.setup(items)\n",
    "        self.vocab = self.num.vocab\n",
    "        \n",
    "\n",
    "    def encodes(self, toks):\n",
    "        return self.num(toks)\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        return self.num.decode(encoded)  \n",
    "    \n",
    "limit = 100\n",
    "path_test = '/home/chris/University/gnn_project/'\n",
    "tfms = [[MyTokenizer(),MyNumerizer()]]\n",
    "files = get_text_files(path_test, folders = ['data'])\n",
    "dsets = Datasets(files[:limit], tfms)\n",
    "dls = dsets.dataloaders(dl_type=LMDataLoader, bs=64)\n",
    "\n",
    "dls.show_batch(max_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 80 ['xxunk', 'xxpad', 'xxbos', 'xxeos', 'xxfld', 'xxrep', 'xxwrep', 'xxup', 'xxmaj', ' ', 'a', 't', 'o', 's', 'e', 'i', 'n', 'r', 'l', 'c', '-', 'p', 'd', 'm', '.', 'g', '1', 'h', '2', 'f', 'u', '0', 'b', 'v', '/', '8', '_', '3', 'k', ':', 'y', '4', '7', '5', 'w', 'R', 'D', '9', '6', 'V', 'T', 'S', 'B', 'H', 'E', '#', '\"', 'A', 'I', 'x', 'N', 'z', '@', 'P', ';', 'O', 'q', '`', 'X', 'G', 'Z', '|', 'xxfake', 'xxfake', 'xxfake', 'xxfake', 'xxfake', 'xxfake', 'xxfake', 'xxfake']\n"
     ]
    }
   ],
   "source": [
    "print(\"Length:\",len(dls.vocab),dls.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/University/GNN/project/transformer_env/lib/python3.9/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m tfms \u001b[38;5;241m=\u001b[39m [[MyTokenizer(),MyNumerizer()]]\n\u001b[1;32m     32\u001b[0m files \u001b[38;5;241m=\u001b[39m get_text_files(path_test, folders \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m dsets \u001b[38;5;241m=\u001b[39m \u001b[43mDatasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m dls \u001b[38;5;241m=\u001b[39m dsets\u001b[38;5;241m.\u001b[39mdataloaders(dl_type\u001b[38;5;241m=\u001b[39mLMDataLoader, bs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m     36\u001b[0m dls\u001b[38;5;241m.\u001b[39mshow_batch(max_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastai/data/core.py:443\u001b[0m, in \u001b[0;36mDatasets.__init__\u001b[0;34m(self, items, tfms, tls, n_inp, dl_type, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    435\u001b[0m     items:\u001b[38;5;28mlist\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# List of items to create `Datasets`\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     tfms:MutableSequence\u001b[38;5;241m|\u001b[39mPipeline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# List of `Transform`(s) or `Pipeline` to apply\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    441\u001b[0m ):\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(dl_type\u001b[38;5;241m=\u001b[39mdl_type)\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtls \u001b[38;5;241m=\u001b[39m L(tls \u001b[38;5;28;01mif\u001b[39;00m tls \u001b[38;5;28;01melse\u001b[39;00m [TfmdLists(items, t, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m L(ifnone(tfms,[\u001b[38;5;28;01mNone\u001b[39;00m]))])\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_inp \u001b[38;5;241m=\u001b[39m ifnone(n_inp, \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtls)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastai/data/core.py:443\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    435\u001b[0m     items:\u001b[38;5;28mlist\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# List of items to create `Datasets`\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     tfms:MutableSequence\u001b[38;5;241m|\u001b[39mPipeline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# List of `Transform`(s) or `Pipeline` to apply\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    441\u001b[0m ):\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(dl_type\u001b[38;5;241m=\u001b[39mdl_type)\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtls \u001b[38;5;241m=\u001b[39m L(tls \u001b[38;5;28;01mif\u001b[39;00m tls \u001b[38;5;28;01melse\u001b[39;00m [\u001b[43mTfmdLists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m L(ifnone(tfms,[\u001b[38;5;28;01mNone\u001b[39;00m]))])\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_inp \u001b[38;5;241m=\u001b[39m ifnone(n_inp, \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtls)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastcore/foundation.py:98\u001b[0m, in \u001b[0;36m_L_Meta.__call__\u001b[0;34m(cls, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x,\u001b[38;5;28mcls\u001b[39m): \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastai/data/core.py:357\u001b[0m, in \u001b[0;36mTfmdLists.__init__\u001b[0;34m(self, items, tfms, use_list, do_setup, split_idx, train_setup, splits, types, verbose, dl_type)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_setup:\n\u001b[1;32m    356\u001b[0m     pv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSetting up \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtfms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, verbose)\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_setup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_setup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastai/data/core.py:378\u001b[0m, in \u001b[0;36mTfmdLists.setup\u001b[0;34m(self, train_setup)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    376\u001b[0m     train_setup:\u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m# Apply `Transform`(s) only on training `DataLoader`\u001b[39;00m\n\u001b[1;32m    377\u001b[0m ):\n\u001b[0;32m--> 378\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtfms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_setup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    380\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplits[\u001b[38;5;241m0\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastcore/transform.py:200\u001b[0m, in \u001b[0;36mPipeline.setup\u001b[0;34m(self, items, train_setup)\u001b[0m\n\u001b[1;32m    198\u001b[0m tfms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs[:]\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tfms: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_setup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastcore/transform.py:204\u001b[0m, in \u001b[0;36mPipeline.add\u001b[0;34m(self, ts, items, train_setup)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd\u001b[39m(\u001b[38;5;28mself\u001b[39m,ts, items\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, train_setup\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_listy(ts): ts\u001b[38;5;241m=\u001b[39m[ts]\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m ts: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_setup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mts\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39msorted(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastcore/transform.py:87\u001b[0m, in \u001b[0;36mTransform.setup\u001b[0;34m(self, items, train_setup)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup\u001b[39m(\u001b[38;5;28mself\u001b[39m, items\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, train_setup\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     86\u001b[0m     train_setup \u001b[38;5;241m=\u001b[39m train_setup \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_setup \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_setup\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetups\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_setup\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastcore/dispatch.py:120\u001b[0m, in \u001b[0;36mTypeDispatch.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minst \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: f \u001b[38;5;241m=\u001b[39m MethodType(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minst)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mowner \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: f \u001b[38;5;241m=\u001b[39m MethodType(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mowner)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 19\u001b[0m, in \u001b[0;36mMyNumerizer.setups\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetups\u001b[39m(\u001b[38;5;28mself\u001b[39m, items):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum \u001b[38;5;241m=\u001b[39m Numericalize()\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum\u001b[38;5;241m.\u001b[39mvocab\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastcore/transform.py:87\u001b[0m, in \u001b[0;36mTransform.setup\u001b[0;34m(self, items, train_setup)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup\u001b[39m(\u001b[38;5;28mself\u001b[39m, items\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, train_setup\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     86\u001b[0m     train_setup \u001b[38;5;241m=\u001b[39m train_setup \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_setup \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_setup\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetups\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_setup\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastcore/dispatch.py:120\u001b[0m, in \u001b[0;36mTypeDispatch.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minst \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: f \u001b[38;5;241m=\u001b[39m MethodType(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minst)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mowner \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: f \u001b[38;5;241m=\u001b[39m MethodType(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mowner)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastai/text/data.py:44\u001b[0m, in \u001b[0;36mNumericalize.setups\u001b[0;34m(self, dsets)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dsets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m     count \u001b[38;5;241m=\u001b[39m dsets\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(dsets, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcounter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mCounter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdsets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_toks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(dsets, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecial_toks\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_toks \u001b[38;5;241m=\u001b[39m dsets\u001b[38;5;241m.\u001b[39mspecial_toks\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/collections/__init__.py:593\u001b[0m, in \u001b[0;36mCounter.__init__\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Create a new, empty Counter object.  And if given, count elements\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;124;03mfrom an input iterable.  Or, initialize the count from another mapping\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;124;03mof elements to their counts.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    590\u001b[0m \n\u001b[1;32m    591\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 593\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/collections/__init__.py:679\u001b[0m, in \u001b[0;36mCounter.update\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mupdate(iterable)\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 679\u001b[0m         \u001b[43m_count_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds:\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(kwds)\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastai/text/data.py:44\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dsets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m     count \u001b[38;5;241m=\u001b[39m dsets\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(dsets, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcounter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m Counter(p \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m dsets \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m o)\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_toks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(dsets, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecial_toks\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_toks \u001b[38;5;241m=\u001b[39m dsets\u001b[38;5;241m.\u001b[39mspecial_toks\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastai/data/core.py:368\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m--> 368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m): \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)))\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastai/data/core.py:406\u001b[0m, in \u001b[0;36mTfmdLists.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    404\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(idx)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_item \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m--> 406\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_after_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m is_indexer(idx) \u001b[38;5;28;01melse\u001b[39;00m res\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_item)\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastai/data/core.py:366\u001b[0m, in \u001b[0;36mTfmdLists._after_item\u001b[0;34m(self, o)\u001b[0m\n\u001b[0;32m--> 366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_after_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, o): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtfms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastcore/transform.py:208\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, o)\u001b[0m\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, o): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompose_tfms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_idx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastcore/transform.py:158\u001b[0m, in \u001b[0;36mcompose_tfms\u001b[0;34m(x, tfms, is_enc, reverse, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m tfms:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_enc: f \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mdecode\n\u001b[0;32m--> 158\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastcore/transform.py:81\u001b[0m, in \u001b[0;36mTransform.__call__\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mencodes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastcore/transform.py:91\u001b[0m, in \u001b[0;36mTransform._call\u001b[0;34m(self, fn, x, split_idx, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, x, split_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m split_idx\u001b[38;5;241m!=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_idx \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastcore/transform.py:97\u001b[0m, in \u001b[0;36mTransform._do_call\u001b[0;34m(self, f, x, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m     96\u001b[0m     ret \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreturns(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturns\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retain_type(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, x, ret)\n\u001b[1;32m     98\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(f, x_, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m x_ \u001b[38;5;129;01min\u001b[39;00m x)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retain_type(res, x)\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastcore/dispatch.py:120\u001b[0m, in \u001b[0;36mTypeDispatch.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minst \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: f \u001b[38;5;241m=\u001b[39m MethodType(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minst)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mowner \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: f \u001b[38;5;241m=\u001b[39m MethodType(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mowner)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m, in \u001b[0;36mMyTokenizer.encodes\u001b[0;34m(self, txts)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencodes\u001b[39m(\u001b[38;5;28mself\u001b[39m, txts):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(txts, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m----> 8\u001b[0m         content \u001b[38;5;241m=\u001b[39m \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     flattened_list \u001b[38;5;241m=\u001b[39m [item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtok(content)) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist]\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m flattened_list\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_buffer_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, errors, final):\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;66;03m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;66;03m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[0;32m--> 319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m    322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_decode(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors, final)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class MyTokenizer(Transform):\n",
    "    def setups(self, items):\n",
    "        self.tok = WordTokenizer()\n",
    "        #self.tok.setup(items)\n",
    "        \n",
    "    def encodes(self, txts):\n",
    "        with open(txts, 'r') as file:\n",
    "            content = file.read()\n",
    "        flattened_list = [item for sublist in list(self.tok(content)) for item in sublist]\n",
    "        return flattened_list\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        decoded_values = TitledStr(''.join(encoded))\n",
    "        return  decoded_values\n",
    "            \n",
    "class MyNumerizer(Transform):\n",
    "    def setups(self, items):\n",
    "        self.num = Numericalize()\n",
    "        self.num.setup(items)\n",
    "        self.vocab = self.num.vocab\n",
    "        \n",
    "\n",
    "    def encodes(self, toks):\n",
    "        return self.num(toks)\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        return self.num.decode(encoded)  \n",
    "\n",
    "limit = 10000\n",
    "path_test = '/home/chris/University/gnn_project/'\n",
    "tfms = [[MyTokenizer(),MyNumerizer()]]\n",
    "files = get_text_files(path_test, folders = ['data'])\n",
    "dsets = Datasets(files[:limit], tfms)\n",
    "dls = dsets.dataloaders(dl_type=LMDataLoader, bs=64)\n",
    "\n",
    "dls.show_batch(max_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/University/GNN/project/transformer_env/lib/python3.9/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos i can not stress how bad this movie is . xxmaj this director took every cheap little unintelligent shot at making these people look so \" xxunk \" . xxmaj why are their clothes so dirty ? xxmaj why on earth would you get the new clark kent to play a crack head ? xxmaj you should be banned from motion pictures for the rest of your life xxmaj buddy xxmaj</td>\n",
       "      <td>i can not stress how bad this movie is . xxmaj this director took every cheap little unintelligent shot at making these people look so \" xxunk \" . xxmaj why are their clothes so dirty ? xxmaj why on earth would you get the new clark kent to play a crack head ? xxmaj you should be banned from motion pictures for the rest of your life xxmaj buddy xxmaj xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>believable . sorry , to me , this movie has no entertainment value at all . xxbos xxmaj maybe here in xxmaj sydney we are all poop side down and as a result we get to lap up xxunk like this s - xxunk in xxunk theaters . xxmaj released here in 1980 this hilarious all - xxunk drama was xxunk with xxunk of delight at the session i xxunk . xxmaj</td>\n",
       "      <td>. sorry , to me , this movie has no entertainment value at all . xxbos xxmaj maybe here in xxmaj sydney we are all poop side down and as a result we get to lap up xxunk like this s - xxunk in xxunk theaters . xxmaj released here in 1980 this hilarious all - xxunk drama was xxunk with xxunk of delight at the session i xxunk . xxmaj in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>book is the xxmaj general is not a xxmaj gothic monster like the characters in xxmaj catherine 's books . xxmaj his xxunk is far more complicated in his xxunk of his children 's spirits and his treatment of xxmaj catherine based on money concerns alone . xxmaj he does not lock up his wife or kill her but he does send xxmaj miss xxmaj morland on a 70 mile trip alone</td>\n",
       "      <td>is the xxmaj general is not a xxmaj gothic monster like the characters in xxmaj catherine 's books . xxmaj his xxunk is far more complicated in his xxunk of his children 's spirits and his treatment of xxmaj catherine based on money concerns alone . xxmaj he does not lock up his wife or kill her but he does send xxmaj miss xxmaj morland on a 70 mile trip alone in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxmaj singer 's career also took a xxunk with this one . \\n\\n xxmaj special effects crew has some fun , and xxmaj jerry xxmaj xxunk provides a score superior to its subject matter . xxbos xxmaj when i think about xxup tv movies , i always think of this film , i have watched it a few times on xxmaj sky xxmaj movies , it was terrible . \\n\\n xxmaj its</td>\n",
       "      <td>singer 's career also took a xxunk with this one . \\n\\n xxmaj special effects crew has some fun , and xxmaj jerry xxmaj xxunk provides a score superior to its subject matter . xxbos xxmaj when i think about xxup tv movies , i always think of this film , i have watched it a few times on xxmaj sky xxmaj movies , it was terrible . \\n\\n xxmaj its been</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>. \\n\\n xxmaj secondly , how many plot holes are in this movie ? xxmaj why introduce the phone call from xxmaj xxunk 's long lost xxmaj dad and never address it again ? xxmaj what was the point of his xxmaj mom hanging up on him - why even have her call to say he is xxunk her too much money - what was the point of that ? xxmaj the</td>\n",
       "      <td>\\n\\n xxmaj secondly , how many plot holes are in this movie ? xxmaj why introduce the phone call from xxmaj xxunk 's long lost xxmaj dad and never address it again ? xxmaj what was the point of his xxmaj mom hanging up on him - why even have her call to say he is xxunk her too much money - what was the point of that ? xxmaj the guy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xxmaj madsen . xxmaj do n't get me wrong , i can handle remakes , even obscure ones . xxmaj but this badly written and poorly filmed xxunk made me feel sorry for both xxmaj madsen and co - star xxmaj richard xxmaj thomas . xxmaj unlike the original , the dialogue here is xxunk , making me wonder , \" why did they bother to re - write it ? \"</td>\n",
       "      <td>madsen . xxmaj do n't get me wrong , i can handle remakes , even obscure ones . xxmaj but this badly written and poorly filmed xxunk made me feel sorry for both xxmaj madsen and co - star xxmaj richard xxmaj thomas . xxmaj unlike the original , the dialogue here is xxunk , making me wonder , \" why did they bother to re - write it ? \" xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>was xxmaj xxunk that did it . \\n\\n xxmaj their is no mention of xxmaj mark or his turning back so the writers of the script are forced to have xxmaj paul and xxmaj xxunk argue over xxmaj paul 's desire to xxunk in xxmaj rome as the basis of their xxunk . \\n\\n xxmaj no xxmaj xxunk on xxmaj paul 's xxmaj second and xxmaj third xxmaj missions ; xxmaj no</td>\n",
       "      <td>xxmaj xxunk that did it . \\n\\n xxmaj their is no mention of xxmaj mark or his turning back so the writers of the script are forced to have xxmaj paul and xxmaj xxunk argue over xxmaj paul 's desire to xxunk in xxmaj rome as the basis of their xxunk . \\n\\n xxmaj no xxmaj xxunk on xxmaj paul 's xxmaj second and xxmaj third xxmaj missions ; xxmaj no xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xxbos xxmaj totally forgettable and almost unwatchable . xxmaj if you enjoy bad acting , thin plots and xxunk weak xxunk , pull up a chair . xxmaj of passing interest to see xxmaj bridget xxmaj fonda look - a - like xxmaj xxunk xxmaj xxunk . xxbos xxmaj this is truly , without exaggerating , one of the worst xxmaj slasher movies ever made . i know , it came out</td>\n",
       "      <td>xxmaj totally forgettable and almost unwatchable . xxmaj if you enjoy bad acting , thin plots and xxunk weak xxunk , pull up a chair . xxmaj of passing interest to see xxmaj bridget xxmaj fonda look - a - like xxmaj xxunk xxmaj xxunk . xxbos xxmaj this is truly , without exaggerating , one of the worst xxmaj slasher movies ever made . i know , it came out in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>at the approaching guy on the horse . xxmaj for some reason , she does n't fire but yells several times for someone else . xxmaj then as xxmaj skeletor xxunk , she jumps out from behind the tree so that xxmaj skeletor can stick her with his spear . xxmaj then everybody starts shooting . xxmaj the bullets cause sparks to fly from the trees . xxmaj apparently the folks who</td>\n",
       "      <td>the approaching guy on the horse . xxmaj for some reason , she does n't fire but yells several times for someone else . xxmaj then as xxmaj skeletor xxunk , she jumps out from behind the tree so that xxmaj skeletor can stick her with his spear . xxmaj then everybody starts shooting . xxmaj the bullets cause sparks to fly from the trees . xxmaj apparently the folks who made</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>find xxmaj peter o'toole entertaining . xxmaj but that is no reason to rent it . xxmaj if you are curious about xxmaj roman history , there are much better movies available . xxbos i normally would n't waste my time criticizing a useless movie such as this . xxmaj however , xxmaj i 'm off of work this week , so i have plenty of time to xxunk in meaningless xxunk</td>\n",
       "      <td>xxmaj peter o'toole entertaining . xxmaj but that is no reason to rent it . xxmaj if you are curious about xxmaj roman history , there are much better movies available . xxbos i normally would n't waste my time criticizing a useless movie such as this . xxmaj however , xxmaj i 'm off of work this week , so i have plenty of time to xxunk in meaningless xxunk .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MyTokenizer(Transform):\n",
    "    def setups(self, items):\n",
    "        path = untar_data(URLs.IMDB)\n",
    "        self.tok =  Tokenizer.from_folder(path)\n",
    "        self.tok.setup(items)\n",
    "        \n",
    "    def encodes(self, txts):\n",
    "        with open(txts, 'r') as file:\n",
    "            content = file.read()\n",
    "        return self.tok(content)\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        return self.tok.decode(encoded)\n",
    "            \n",
    "class MyNumerizer(Transform):\n",
    "    def setups(self, items):\n",
    "        self.num = Numericalize()\n",
    "        self.num.setup(items)\n",
    "        self.vocab = self.num.vocab\n",
    "\n",
    "    def encodes(self, toks):\n",
    "        return self.num(toks)\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        return self.num.decode(encoded)\n",
    "\n",
    "limit = 1000\n",
    "path = untar_data(URLs.IMDB)\n",
    "tfms = [[MyTokenizer(), MyNumerizer()]]\n",
    "files = get_text_files(path, folders = ['train', 'test'])\n",
    "#splits = GrandparentSplitter(valid_name='test')(files)\n",
    "dsets = Datasets(files[:limit], tfms)#, splits=splits)\n",
    "dls = dsets.dataloaders(dl_type=LMDataLoader)\n",
    "\n",
    "dls.show_batch(max_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer from txts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer(Transform):\n",
    "    def setups(self, items):\n",
    "        path = untar_data(URLs.IMDB)\n",
    "        self.tok = Tokenizer.from_folder(path)\n",
    "        self.tok.setup(items)\n",
    "        \n",
    "    def encodes(self, txts):\n",
    "        return self.tok(txts)\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        return self.tok.decode(encoded)\n",
    "            \n",
    "class MyNumerizer(Transform):\n",
    "    def setups(self, items):\n",
    "        self.num = Numericalize()\n",
    "        #print(items)\n",
    "        self.num.setup(items)\n",
    "        self.vocab = self.num.vocab\n",
    "\n",
    "    def encodes(self, toks):\n",
    "        return self.num(toks)\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        return self.num.decode(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer(Transform):\n",
    "    def setups(self, items):\n",
    "        self.tok = SubwordTokenizer(vocab_sz=1000)\n",
    "        self.tok.setup(items)\n",
    "        \n",
    "    def encodes(self, txts):\n",
    "        toks = self.tok(txts)\n",
    "        tokenized_sentences = [[tok for tok in gen] for gen in toks]\n",
    "        flattened_list = [item for sublist in tokenized_sentences for item in sublist if item]\n",
    "        return flattened_list\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        decoded_values = TitledStr(''.join(encoded))\n",
    "        return  decoded_values\n",
    "            \n",
    "class MyNumerizer(Transform):\n",
    "    def setups(self, items):\n",
    "        self.num = Numericalize()\n",
    "        self.num.setup(items)\n",
    "        self.vocab = self.num.vocab\n",
    "        \n",
    "\n",
    "    def encodes(self, toks):\n",
    "        return self.num(toks)\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        return self.num.decode(encoded)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB)\n",
    "\n",
    "# Get the paths of all text files in the dataset\n",
    "files = get_text_files(path)\n",
    "\n",
    "# Initialize an empty list to store strings\n",
    "txts = []\n",
    "\n",
    "# Read the content of each file and append it to the list\n",
    "for file in files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        txts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = L(txts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt200 = txts[:]\n",
    "tokn = MyTokenizer()\n",
    "tokn.setup(txt200)\n",
    "\n",
    "toks = txt200.map(tokn)\n",
    "\n",
    "num = MyNumerizer()\n",
    "num.setup(toks)\n",
    "\n",
    "#tokenized_sentences = [[tok for tok in gen] for gen in toks]\n",
    "#flattened_toks = [[item for sublist in sublist_list for item in sublist] for sublist_list in tokenized_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj this is the kind of movie that sucks the fun out of going to the cinema . xxmaj why sit through this kind of drivel when you can watch the original \" all xxmaj that xxmaj heaven xxmaj allows \" . xxmaj this movie has n't an original idea in it except to put everyone 's guard up for the xxmaj politically xxmaj correct times we live in . xxmaj</td>\n",
       "      <td>xxmaj this is the kind of movie that sucks the fun out of going to the cinema . xxmaj why sit through this kind of drivel when you can watch the original \" all xxmaj that xxmaj heaven xxmaj allows \" . xxmaj this movie has n't an original idea in it except to put everyone 's guard up for the xxmaj politically xxmaj correct times we live in . xxmaj complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>part of this 9 - minutes long film , logically would be to start with the entertaining / funny things now , but in ' old xxmaj glory ' there is no fun . xxmaj there is n't even an attempt made to be funny ! xxmaj as if the creators forgot that xxmaj looney xxmaj tunes cartoons are supposed to be witty or spoofing or something , but this particular one</td>\n",
       "      <td>of this 9 - minutes long film , logically would be to start with the entertaining / funny things now , but in ' old xxmaj glory ' there is no fun . xxmaj there is n't even an attempt made to be funny ! xxmaj as if the creators forgot that xxmaj looney xxmaj tunes cartoons are supposed to be witty or spoofing or something , but this particular one is</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "limit = 20000\n",
    "tfms = [[tokn, num]]\n",
    "dsets = Datasets(txts[:limit], tfms)   \n",
    "dls = dsets.dataloaders(dl_type=LMDataLoader)\n",
    "dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1920"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dls.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained('bert-base-uncased')\n",
    "\n",
    "config.vocab_size = len(dls.vocab)\n",
    "config.num_labels = len(dls.vocab)\n",
    "config.hidden_size = 395\n",
    "config.num_hidden_layers = 12\n",
    "config.num_attention_heads = 5\n",
    "config.max_position_embeddings = 512\n",
    "transformer = ShellTransformer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1298' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1298 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/gnn_project/.venv/lib/python3.10/site-packages/fastprogress/fastprogress.py:73: UserWarning: Your generator is empty.\n",
      "  warn(\"Your generator is empty.\")\n"
     ]
    }
   ],
   "source": [
    "model = transformer\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "dls.to(device)\n",
    "\n",
    "\n",
    "learn = Learner(\n",
    "    dls, \n",
    "    model, \n",
    "    loss_func=CrossEntropyLossFlat(), \n",
    "    metrics=[accuracy]\n",
    ")\n",
    "\n",
    "learn.fit_one_cycle(1, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2899/1461868629.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(token_ids).unsqueeze(0).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxbos i did n't like this movie , because i was n't expecting a movie to watch it . xxmaj it was a great movie . xxmaj it was\n"
     ]
    }
   ],
   "source": [
    "# Define a function for text generation\n",
    "def generate_text(model, token_ids, max_length=20):\n",
    "    token_ids = token_ids.to(device)  \n",
    "    input_ids = torch.tensor(token_ids).unsqueeze(0).to(device)  \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs[:, -1, :]  \n",
    "            next_token_id = torch.argmax(logits, dim=-1)\n",
    "            token_ids = torch.cat((token_ids, next_token_id),dim=0)\n",
    "            input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0).to(device)], dim=-1)\n",
    "    return token_ids\n",
    "\n",
    "def decode_tokens(numerized_tokens, vocab):\n",
    "    generated_test = [vocab[token] for token in numerized_tokens]\n",
    "    return ' '.join(generated_test)\n",
    "\n",
    "\n",
    "pipe = Pipeline([tokn,num])\n",
    "\n",
    "example_sentence = \"I didn't like this movie, because\"\n",
    "start_text_ids = pipe(example_sentence)\n",
    "generated_ids = generate_text(learn.model, start_text_ids)\n",
    "vocab = dls.vocab\n",
    "     \n",
    "print(decode_tokens(generated_ids,vocab))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
