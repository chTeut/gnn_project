{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pickle\n",
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = \"cuda:0\"\n",
    "else:\n",
    "  device = \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    dim_k = query.size(-1)\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / np.sqrt(dim_k)\n",
    "    if mask is not None:\n",
    "        if scores.shape[1] == mask.shape[1]:\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        else:\n",
    "            mask = torch.tril(torch.ones(scores.shape[1], scores.shape[1], dtype=torch.float)).unsqueeze(0).to(device)\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return weights.bmm(value)\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "        self.mask = torch.tril(torch.ones(embed_dim, head_dim, dtype=torch.float)).unsqueeze(0).to(device)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        attn_outputs = scaled_dot_product_attention(\n",
    "            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state), self.mask)\n",
    "        return attn_outputs\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(embed_dim, head_dim, config.vocab_size) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n",
    "        x = self.output_linear(x)\n",
    "        return x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply layer normalization and then copy input into query, key, value\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        # Apply attention with a skip connection\n",
    "        x = x + self.attention(hidden_state)\n",
    "        # Apply feed-forward layer with a skip connection\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        return x\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size, \n",
    "                                             config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n",
    "                                                config.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        if input_ids.dtype != torch.int:\n",
    "            input_ids = input_ids.int()\n",
    "        # Create position IDs for input sequence\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.int).unsqueeze(0).to(device)\n",
    "        # Create token and position embeddings\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        # Combine token and position embeddings\n",
    "        embeddings = token_embeddings #+ position_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(config) \n",
    "                                     for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class ShellTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerDecoder(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)#[:, 0, :] # select hidden state of [CLS] token\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        if x.dtype != torch.float:\n",
    "            x = x.float()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/chris/University/gnn_project/dataset', 'rb') as fp:\n",
    "    _ = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nmap\\n',\n",
       " 'nmap -v 10.1.26.4\\n',\n",
       " 'nmap -v 10.1.26.9\\n',\n",
       " 'ssh --help\\n',\n",
       " 'ssh 10.1.26.9\\n',\n",
       " 'ssh 10.1.26.9 admin/123456\\n',\n",
       " 'ssh --help\\n',\n",
       " 'ssh 10.1.26.9\\n',\n",
       " 'ssh -l admin 10.1.26.9\\n',\n",
       " 'ssh admin@admin 10.1.26.9\\n']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Given array of text elements\n",
    "texts = _\n",
    "\n",
    "# Create a folder to store the text files\n",
    "folder_path = '/home/chris/University/gnn_project/data/'\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# Write each non-empty text element to a separate file\n",
    "for i, text in enumerate(texts):\n",
    "    # Remove trailing newline character\n",
    "    text = text.rstrip('\\n')\n",
    "    \n",
    "    # Check if text is not empty after stripping newline\n",
    "    if text.strip():\n",
    "        file_path = os.path.join(folder_path, f'text_{i}.txt')\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = L(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203101"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Encode from Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos cmake build xxbos trabalho xxbos make xxbos watch -n 1 \" syscoind getinfo &amp; &amp; tail -n 20 debug.log \" xxbos pwd xxbos java xxmaj xxunk xxbos sudo systemctl stop network - manager ; sleep xxunk systemctl restart network - manager xxbos set xxup xxunk xxbos chmod + x xxunk xxbos grep xxunk xxunk xxunk | sed -i 's / xxup xxunk / xxup xxunk / xxup g ' xxbos msfconsole</td>\n",
       "      <td>cmake build xxbos trabalho xxbos make xxbos watch -n 1 \" syscoind getinfo &amp; &amp; tail -n 20 debug.log \" xxbos pwd xxbos java xxmaj xxunk xxbos sudo systemctl stop network - manager ; sleep xxunk systemctl restart network - manager xxbos set xxup xxunk xxbos chmod + x xxunk xxbos grep xxunk xxunk xxunk | sed -i 's / xxup xxunk / xxup xxunk / xxup g ' xxbos msfconsole xxbos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>read xxbos sudo reboot xxbos rvm -v xxbos ls xxbos ls xxbos python3 xxunk xxbos \" xxunk - xxunk \" : \" xxunk \" , xxbos gwsh xxunk xxbos vim xxunk xxbos bash : [ goalador@gatanda : command not found xxbos ls xxbos bg xxunk xxbos rm bin / info_pc1_cesar xxbos make xxbos wget xxbos [ goalador@gatanda hhneuauf.de]$ [ goalador@gatanda hhneuauf.de]$ [ goalador@gatanda hhneuauf.de]$ [ chg ] xxmaj controller xxup c8 :</td>\n",
       "      <td>xxbos sudo reboot xxbos rvm -v xxbos ls xxbos ls xxbos python3 xxunk xxbos \" xxunk - xxunk \" : \" xxunk \" , xxbos gwsh xxunk xxbos vim xxunk xxbos bash : [ goalador@gatanda : command not found xxbos ls xxbos bg xxunk xxbos rm bin / info_pc1_cesar xxbos make xxbos wget xxbos [ goalador@gatanda hhneuauf.de]$ [ goalador@gatanda hhneuauf.de]$ [ goalador@gatanda hhneuauf.de]$ [ chg ] xxmaj controller xxup c8 : f7:33:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxunk admin xxbos vim xxunk xxbos cat xxunk xxbos ls -a xxbos git clone ssh : / / xxunk / xxunk / git / xxunk xxbos sudo vi xxunk xxbos clear xxbos ls xxbos mysql -h alas -u ml12087 -p xxbos git xxunk xxbos [ goalador@gatanda hhneuauf.de]$ bash : bash : : command not found xxbos sudo xxbos git log xxbos test \" $ user \" = \" xxunk \" &amp; &amp;</td>\n",
       "      <td>admin xxbos vim xxunk xxbos cat xxunk xxbos ls -a xxbos git clone ssh : / / xxunk / xxunk / git / xxunk xxbos sudo vi xxunk xxbos clear xxbos ls xxbos mysql -h alas -u ml12087 -p xxbos git xxunk xxbos [ goalador@gatanda hhneuauf.de]$ bash : bash : : command not found xxbos sudo xxbos git log xxbos test \" $ user \" = \" xxunk \" &amp; &amp; echo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxrep 3 x xxrep 3 c xxrep 3 v xxunk xxrep 4 n xxrep 3 m xxrep 4 , * / / xxrep 6 1 xxrep 6 ' xxup xxunk ! xxbos make xxbos bash : bash : : command not found xxbos reboot xxbos ls -la xxbos httpd xxbos mkdir .ssh xxbos mplayer xxunk xxbos node app xxbos run xxbos vi send.py xxbos ls xxbos python da.py xxbos dunst xxbos clear</td>\n",
       "      <td>3 x xxrep 3 c xxrep 3 v xxunk xxrep 4 n xxrep 3 m xxrep 4 , * / / xxrep 6 1 xxrep 6 ' xxup xxunk ! xxbos make xxbos bash : bash : : command not found xxbos reboot xxbos ls -la xxbos httpd xxbos mkdir .ssh xxbos mplayer xxunk xxbos node app xxbos run xxbos vi send.py xxbos ls xxbos python da.py xxbos dunst xxbos clear xxbos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MyTokenizer(Transform):\n",
    "    def setups(self, items):\n",
    "        path = untar_data(URLs.IMDB)\n",
    "        self.tok =  Tokenizer.from_folder(path)\n",
    "        self.tok.setup(items)\n",
    "        \n",
    "    def encodes(self, txts):\n",
    "        with open(txts, 'r') as file:\n",
    "            content = file.read()\n",
    "        return self.tok(content)\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        return self.tok.decode(encoded)\n",
    "            \n",
    "class MyNumerizer(Transform):\n",
    "    def setups(self, items):\n",
    "        self.num = Numericalize()\n",
    "        self.num.setup(items)\n",
    "        self.vocab = self.num.vocab\n",
    "\n",
    "    def encodes(self, toks):\n",
    "        return self.num(toks)\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        return self.num.decode(encoded)\n",
    "    \n",
    "limit = 10000\n",
    "path_test = '/home/chris/University/gnn_project/'\n",
    "tfms = [[MyTokenizer(),MyNumerizer()]]\n",
    "files = get_text_files(path_test, folders = ['data'])\n",
    "dsets = Datasets(files[:limit], tfms)\n",
    "dls = dsets.dataloaders(dl_type=LMDataLoader, bs=4)\n",
    "\n",
    "dls.show_batch(max_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 1400 ['xxunk', 'xxpad', 'xxbos', 'xxeos', 'xxfld', 'xxrep', 'xxwrep', 'xxup', 'xxmaj', '/', 'ls', '-', ':', 'sudo', '\"', 'git', '.', 'vim', '[', 'bash', 'goalador@gatanda', \"'\", '\\\\', 'hhneuauf.de]$', '#', 'command', 'not', 'vi', 'found', 'cat', 'make', 'nano', 'rm', 'clear', 'python', 'install', '|', 'll', 'apt', '3', '*', '>', 'ssh', 'etc', '-a', '0', 'add', '$', 'exit', '1', ';', '-p', 'bin', 'cd', 'grep', 'home', '=', 'status', '-l', '..', '-i', 'echo', '~', '&', '4', 'get', '-f', 'docker', 'pwd', '-u', 'push', 'run', 'nmap', '-v', 'mv', ',', '-r', 'commit', '172.18.1.5', '-h', 'set', 'master', '-m', ')', '{', 'ps', 'usr', '-d', 'origin', 'find', '}', 'mkdir', 'config', 'app', '(', 'ifconfig', 'node', 'cp', 'su', 'chmod', '-ltr', '`', 'less', '-t', 'log', '-s', '2', '-o', 'server', '-la', ']', 'dev', 'man', 'lib', 'python3', 'scp', 'a', 'test', 'php', 'w', 'root', 'update', 'service', 'tar', 'x', 'systemctl', '-rf', 'sh', '.invoices2019.zip', 'remote', 'pull', 'gcc', 'exploit', 'restart', 'clean', '-c', '--help', '-n', 'fcrackzip', 'the', '-k', 'wget', 'var', '-name', '+', 'tests', '%', 'java', 'src', 'page', 'ping', 'ansible', 'args', 'mysql', '.bashrc', 'all', 'trabalho0304', 'tail', 'search', 'share', 'parallel', 'top', 'manage.py', 'curl', 'checkout', 'ruby', '--', '.ssh', 'id_rsa', '5', 'opt', 'eth0', 'pip', 'pi', 'primeiro', 'http', 'pintos', 'l', '7', '-q', 'nohup.out', 'start', 'list', 'pacman', '_', 'local', 'init', 'reboot', 'linux', '.config', 'single', 'teste', '--global', 'show', 'g', 'stop', 'tmp', 'v', 'source', 'iptables', 'new', 'd', 's', 'personal', 'la', '--filesys', 'size=2', 'vm', '|grep', 'dir', 'admin@10.1.26.9', '-e', 'proc', 'use', '-al', 'hacking', 'edx', 'edxapp', 'passwd', 'build', 'print', 'history', 'diff', 'bundle', 'admin', 'locate', 'child', 'linear', 'lhost', 'templates', 'for', 'file', '-lrt', 'user', '-sv', 'which', 'options', 'in', 'db', 'awk', 'f', 'xzvf', 'javac', '-y', 'rails', '172.18.1.5:1', 'touch', 'tg', 'data', '!', 'c', 'downloads', 'configure', 'john', 'clone', 'theme', 'perl', 'sed', '<', 'auv_map', 'localhost', 'mdk', 'scan.tgz', 'corpwebsite', 'makefile', 'to', 'main', 'ip', '-b', 'invoices2019.zip', '-j', 'playbook', 'exec', 'emacs', 'bzr', '10', 'sessions', 'network', 'gulp', 'cut', 'documents', 'lo', 'cve_2019_15107.py', 'uname', 'kill', 'printf', 'pkill', 'starterkit]$', 'help', 'branch', 'hosts', 'qemu', 'nox', 'storage', 'sort', 'spec', 'i', 'and', 'more', \"'s\", 'up', 'isitools', 'msfconsole', 'play.sh', 'b', 'npm', 'syscoind', 'key', 'gwsh', 'oc', '--version', 'catalog', 'rhosts', 'yum', '10.1.135.83', 'jenkins', 'main.java', 'df', 'name', 'type', 'f.tgz', '60', 'trabalho', 'mount', 'netstat', 'tasks', '-x', '24', 'ifup', 'ssh2john.py', '.xresources', 'cache', 'head', 'gavana.uv.ro', '--qemu', 'if', 'scripts', 'env', '-type', 'shutdown', '58.34.244.67', 'nc', 'host', 'view', 'sxhkd', 'path', '6', 'gunicorn_start', '-ss', 'user.name', 'nohup', 'i3', '127.0.0.1', '8', 'mongod', 'do', 'qasnap', 'center_src', 'xargs', 'git@github.com', 'hello', 'watch', 'migrate', '-9', 'g++', 'rockyou.txt', 'ifdown', 'cpuinfo', 'ppgsigc', 'zsh', 'apache2', '--swap', 'size=4', 'lms', 'wal', 'index.php', 'user.email', 'done', 'wordlist.txt', '--cpu', '22', 'userprog', 'stranger', 'as', 'out', 'interfaces', 'public', 'cm', 'images', 'wordlists', '-jar', 'mplayer', 'readme.md', 'webmin_backdoor', 'cli', 'concataxofastqfiles.sh', 'udp.pl', 'html', '-w', 'du', 'newbie', 'printinfo.c', 'centos-5', 'controller', 'brew', 'desktop', 'default', 'scan', 'rept.rb', 'rake', 'runserver', 'users', 'image', 'collectd', 'eve@10.1.17.4', 'wlan0', '--pkg', 'nslookup', '--all', 'fi', 'upgrade', 'fastqc', 'so', 'modification', 'copy', 'redis', 'openstack', 'shm.c', 'rport', 'telegram', 'chg', 'a.out', 'trizen', '--bochs', 'q', 'server_unit_test.py', 'compose', 'nginx', 'temp', 'script', 'addview.c', 'index.html', 'check', 'common', 'fg', 'application', 'ubuntu', 'tools', 'things', 'date', 'init.d', 'true', 'cv', 'media', 'artisan', 'composer', '-1', 'workspace', 'mnt', 'ffmpeg', 'rvm', 'getnotice.php', 'dnf', 'comando', 'reload', '53', 'rosrun', '10.1.26.9', 'rubocop', 'views.py', 'server.c', '-lpthread', 'tessel', 'rspec', 'genetic', 'learn', '-fr', '0057home', '--clean', 'htop', 'tf', 'wiload.sh', '…', 'manager', 'you', 'filter', '.vimrc', '20', 'setup', 'rxeflash.sh', 'study.py', 'arg', 'axomiketest', 'ss', 'create', '--name', 'flow', 'sys', 'stack', 'h', 'chown', '3490', 'seq', 'tcp', 'first', '-lh', 'machine', 'firefox', '23', 'terraform', 'valgrind', 'unicorn', 'l.', '||', '.git', 'on', 'trabalho_03_04_12.sh', 'map_builder', 'views', 'activate', 'tg-server.pub', 'authorized_keys', 'csapp.c', 'matching', 'ad_placement_spec.rb', '-g', 'unzip', 'pictures', 'post', 'export', '135', 'result', 'import', 'doc', 'sbin', '.bash_profile', 'run.sh', 'accept', 'project', 'web', 'aux', '--set', 'gdb', 'info', 'admin@10.1.26.9:~', 'crontab', 'polybar', 'be', 'module', 'template', 'megastore', 'trab1', 'directory', 'epadmin', 'null', 'release', 'mongodb', 'conf', 'rhost', 'open', 'themes', 'raccoongang_theme', 'marvel', 'ginkgo', 'r', 'ln', 'fetch', 'dist', 'multiple', 'read', 'terror', 'who', 'logout', '--save', 'gem', 'namp', 'repository', 'cdrom', 'from', 'thunar', '-ef', 'eclipse', 'merge', 'iwconfig', 'profile', 'scriptrabalho', 'adb', 'jar', 'genetic.jar', 'samba', 'of', 'sqainfra', 'lex.yy.c', 'y.tab.c', 'wc', 'proc.c', 'css', 'addview', 'getnoticetype1.php', 'clonestoread', 'stdout', 'yaourt', 'connect', 'babymarvel', 'trabalho.sh', 'chrome', 'no', 'bjohnston', 'uniq', 'pdommeti', 'gnome', 'xvf', 'm', 'alas', 'ml12087', 'killall', 'adding', 'net', 'build_package.sh', 'firstpython.py', 'nofile', 'telnet', 'remove', '-ul=11', 'n', 'dig', 'led.js', 'rev', 'down', 'marathon', 'cmd', 'ant', 'reset', 'sleep', 'arquivotrabalho', 'daemon', 'generate', 'in01', 'system', 'json', '139', '16', 'hoje', 'root@attacker', 'whoami', 'where', 'httpd', 'trab', 'recipes', 'cyg', '-lt', '-iname', 'services', 'archives', 'pianobar', 'apache', 'server-rhel.yml', 'image.jpg', 'error', 'nagios', 'gedit', 'pgrep', 'webmin', 'serve', '^', 'index.js', 'id_rsa.pub', 'whereis', 'dns', 'debian', 'some', 'now', 'readme', 'zxvf', 'sonarqube', 'main.yml', 'tag', 'matlab', 'with', 'idade', '--list', 'docker-compose.yml', 'iwlist', 'proj4.asm', 'error.log', 'dpkg', 'conky', '.vim', 'ryo.tar.gz', 'main.c', 'rmi', 'platform', 'pulsar', 'extension', '04', '--refresh', 'login', '11', 'vilker3_pipe02', 'vilker3_pipe02.c', '-am', 'makemigrations', '.sh', 'back', 'kernel', 'sc_main.py', 'packages', 'test1.yml', 'map', 'navigation', 'navigation.html', 'include', '09', 'api', '?', 'integer', 'client', 'asm.c', 'content', 'register', 'mosquitto_pub', '.bash_history', '-version', 'while', 'time', '755', 'tomcat', 'send.py', 'main.py', '100', 'file.txt', '@', 'da.py', 'shell', 'gtk', '-ii', 'auxiliary', 'scanner', 'rc.local', '-exec', 'file1.py', 'root@10.1.26.23', 'sqlmap', 'intel', 'compilers_and_libraries', 'stulogin.dcs.shef.ac.uk', 'psql', '-ls', 'google.com', 'examples', 'test1', 'deurbel2.py', 'puppet', 'agent', 'object', 'led', 'java.yml', 'rofi', '-show', 'overcloud', 'stable', 'variavel', 'mpirun', 'bg', 'line', '-std', 'c++11', 'bitbucket', 'sftp', 'target', '--wordlist', 'startx', 'urxvt', 'xft', 'mobile', 'go', 'yacc', 'parse.y', 'sysctl', 'refresh_hb', 'apachectl', 'az', '01', 'p', 'mpeg1video', 'johnpi', 'test.c', 'isiseqruns', 'ronaxotrinity', 'account', 'mpd', '-path', 'devel', 'hash', 'test.pl', 'password', 'palind.rb', 'libattr.so', 'nmbd', 'is', 'virtualenv', '100311_axo4_evita', 'tcpdump', 'backup', 'mode', 'ipacct.sh', 'restore', 'test.py', 'trinity', 'feature', 'convert', 'screen', 'work', 'getinfo', 'sql', 'keygen', 'postgresql', '0.0.0.0:20568', 'ipython', 'con2.java', 'dc', '12', '600', 'bash.c', 'sl', 'jenkinsfile', 'combined.py', 'toto.sh', '--env', '--add', 'mpd.conf', 'none', 'session', 'uberwriter', 'example', 'syntax', 'my', 'vendor', 'msf', 'test2', '--wordlist=', '19', '.yahoo.com', 'testing.sh', '.xinitrc', 'teste.sh', 'r-2.15.2', 'edx.log', 'userinfo.sh', 'fortune', 'mongod.conf', 'borrow_lend.rs', 'unix', '0057damon', '25', 'what-time-script.sh', 'scaffold', 'chef', '-z', 'default.rb', 'nid', 'tx', 'reference', 'psybnc', 'sources.list.d', 'server.php', 'index', 'chrysalis', 'trinity.timing', 'a.txt', 'yii', 'socket', '.cpp', 'username', 'slider', 'by', '.mythtv', '.w++', '-ef|grep', 'grading', 'code', 'ovs', 'xfce4', 'avahi', 'your', 'have', 'install.sh', 'check_mk', '-ul=25', 'lxml', 'inventory', 'echo_selectserv.c', 'mydata.xlsx', 'cfg', 'resolv.conf', 'bus', 'br', 'notify', 'mod', '3_axo2pe_eva', '10.1.26.0', 'rebase', 'openssl', '.i3', '0057sean', '0057', 'led.py', '-sn', 'gzip', 'eve', 'both.fa', 'unexpected', 'ssl', 'bybys.c', '-aux', 'addnode', 'c8', 'f7:33:33', 'e2', 'f7', 'uuids', '-8', '-00805f9b34fb', 'then', 'gdpr', 'es', 'extractor', 'httpd.conf', 'essid', 'route', 'rostopic', 'rep1.rb', 'secrets', 'metasploit', 'mainapp.py', 'maxspan', 'l1', '-np', 'gpaw', '-zxvf', 'models.py', 'repair', 'myfirstplaybook.yml', 'ppa', 'svg', '2018', 'pz.lua', 'sources.list', 'id', '-all', '320x240', 'video4linux2', 'video0', 'rmdir', '0.0.0.0', 'engine', '.local', '.ssh]$', '22.py', 'utils', 'urls.py', 'cmakelists.txt', '-rn', 'deurbel', '1234', 'broadcom', 'execute.py', 'https', 'netctl', 'port', '80', 'libinput-gestures.conf', '64', 'sdb1', 'thackbarth', 'umount', 'purge', 'rules.v4', 'out.txt', 'added', '8080', 'jobs', '--track', 'origins', 'yes', 'clnt', '-altr', '537.36', 'rw', 'fc', 'smbd', 'set.sh', '4096', '15107', 'eth1', 'networking', 'ranger', 'conky.conf', 'snap', '.dotfiles', 'gpu.job', 'webapp', 'ctf', 'ro', 'sshd_config', 'nr', '66.34', 'in02', 'so_listing_tabs', 'default_items.twig', 'models', 'terminal', 'packages.yml', 'baremetal', 'ws', 'heat', 'active', 'spotify', 'kafka', 'repo', 'syscall', 'xrandr', 'frame.c', 'rlg327', '-lah', 'oh', 'yay', 'alice@172.18.1.5', 'diretorio', 'xml', 'group', 'exist', 'exit(0', 'stat', 'test2.py', 'ubuntu@ip-172', '31', '102:~', 'myproject$', 'mvim', 'readinfo.py', 'aravind.rb', 'reseed', 'nevens.sh', 'hash.txt', 'check_mk_agent', 'develop', 'stopwatchui.py', 'runs', 'like', 'convertfastqtofastaright.sh', 'chong@10.1.17.4', 'y29yb25h', 'vagrant', 'clmb', 'cmk', '5phi1jf+wojvarkqlzffamry0rhu6tsqtydm2qycssze1fhldxgnkpathyz1suhe', 'j', '-crf', '512k', 'aac', '-strict', '-2', '.mp4', 'software', 'test.lua', 'string', 'filesearch.c', '123,\"type', 'saveuserroot', '-f2', 'tee', 'djynp3xqcknflivtbwvsshqnhsn6zcl8btefdrsf6kaj6qm1', 'cafl73otlgzyr7', 'grafana', 'akamake', '-quit', 'blue', 'hostname', 'sd', 'boot', 'ep', 'kwset.c', 'fn', 'unset', '-empty', '-printf', 'forstalkapp.com', 'webappcreations', 'attach', 'mpicc', 'gemset', 'rrdtool', 'geanclm.sh', 'dump', 'br_ovs', 'howodd.sh', 'wlan_normal.sh', '--trace', 'mysum.sh', 'alias', 'boost', 'deluge', 'modules', '-help', '-sch', 'seed', 'acpi', 'ibm', 'fan', 'jq', 'chong', '-uroot', 'chsh', 'ks', 'objdump', 'router', 'library', 'edquota', 'air1', 'cfg.d', 'swift', 'machineip', '--upgrade', '-syu', '--topic', 'readlink', 'color', 'structure', '120706_ssprimaryhepmicrorna', '92_evita', '--nummon', 'php5', 'tab', '--force', '--continue', 't', 'relacaonumeros', 'pg', 'test3', 'github.com', 'helloworld', 'wallpapers', 'class', 'clones.git', 'groups', 'ap', '.htaccess', 'root@sqainfra-docker01.sqaextranet.akamai.com', 'forceexploit', 'chapter', '--no', 'unicorn.log', 'youtube', 'sensorcheck.py', 'arquivotrabalho2', 'certs', 'devices', 'popel.py', 'smb.conf', 'substitution', 'mesos-migrate-dc.py', '--colddc', '--hotdc', 'hello.c', 'wireshark', '-testnet', 'tony', 'build_binary.sh', 'room602', 'liu567890', 'auwx', 'publico_ins', 'usrbin', 'initialadminpassword', 'expose', 'dunst', 'johnkey', 'lms.env.json', 'courseware', 'python2', 'x86_64', 'nbgrader', 'logs', 'named', '0.0.0.0:8', 'hadoop', 'fs', 'info_pc1_cesar', 'sample', '2015', 'latest', 'addon', 'does', \"n't\", 'auxww', 'post_handler.c', '--node', 'webappcreations.diessbacherbablde', '172.18.1.1', 'aluno', 'flag.txt', 'bzimage', 'lightsensor.py', 'can', 'loops', 'axoprojecttest', 'bower', '-st', '-p-', 'zerador.sh', 'saiyuuki@', '.31yuuki.com', 'mac_rsa', 'cool-tree.java', 'tree', 'freqtrade', '0.05', 'running', 'output', '320', '240', 'axo_left.fq', 'state', 'ffserver.conf', 'goalad[goalador@gatanda', 'h>[goalador@gatanda', 'makefile.unix', 'rbenv', 's_1_1.fq.gz', 'vault', 'maior', 'z', 'quit', '.google.com', 'full_assembly', 'tissue_types', 'alan.zip', 'vsctl', '-stop', '.m2', 'express', 'cloud', 'projects', 'voyager', 'main.tf', 'public_html', 'neigh', 'star', 'digital', 'videos', 'mcopy', 'qsub', 'qsub_phase1a.sh', 'resource', 'serv', 'goalador@gatandabash', '--forms', 'pass', 'join', 'i386', 'delete', 'pro', 'orobardet', 'lsof', '--output', 'upstream', 'sshd', 'waitpid', 'sort.rb', 'dl', 'e]710;%s', '007', 'pixelsize=12', 'php_playbook.yml', 'dungeon.c', 'bc', 'y2o0rvcjsin3d2h9f0fp0xvc3ehzd7ct2hvzev6x3kulfdwehkuf+7wuetjfsmvq', 'getnoticetype6.php', '13', '9', 'browser', 'collectd.conf', 'sdb', 'average', 'ledcontrol.py', '-update', 'enable', 'ds', 'google', 'package', 'main.rb', '.h', 'dns.py', '-p5353', '-ax', 'sda', 'pylast', 'redshift', 'auto', 'files', '05', 'admin@server', 'grub', 'index.html.slim', 'postgres', 'lab01.txt', 'l2', 'sqlite3', 'namd-tutorial-160715.tgz', 'language', 'windows', 'connection', 'keep', '--data', 'binary', 'mac', 'dmc', 'playbooks', 'playbook1.yml', '.c', 'debug.log', 'mongo', 'usertests.c', 'rsa', 'login.php', '-j2', '-c1', 'bavail', 'wlan_ap.sh', 'zerador', 'write.c', 'root@sqainfra-docker05.sqaextranet.akamai.com', 'when', 'cve:2019', 'key.txt', 'stash', 'smbcredentials', 'launch.sh', '775', 'window', 'container', 'st', 'stage', 'libssh_auth_bypass', 'patch', '--exe', '--np', '--fi', '--fo', 'test_query_string_9', 'version', 'ref01', 'wifi', 'if2', '.themes', 'run_tests.sh', '196', 'pgcc', '-acc', '-minfo', 'accel', 'laplace_data_manag_acc.c', 'sbatch', 'cc', '.cure.data', 'xrdb', '2017', 'confirmed', 'sendmany', 'saph8g6s7efresnqfumruxn6eap1pw2ztu', 'acer', '-pn', 'mvn', 'cve-2019', 'sda1', 'sqa2', '2019', 'systemd', 'seeds.rb', 'dependencies', 'pip3', '0xc00221c5', 'cmake', 'msfdb', 'arch', 'components', 'config.php', 'rj6xi5cibddo83vdqhlyauawtlv', 'wupqsrc9xsk', 'y2dljzqhad+qkwbm98jrcagx', 'gradlew', 'setupdecompworkspace', 'colors-rofi-dark.rasi', 'product', '9][0', 'pt', 'grow', 'element', 'xxfake']\n"
     ]
    }
   ],
   "source": [
    "print(\"Length:\",len(dls.vocab),dls.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>▁n▁o▁d▁e▁b▁r▁e▁w▁a▁l▁i▁a▁s▁d▁e▁f▁a▁u▁l▁t▁v▁0▁.▁8▁.▁1▁5▁l▁s▁s▁h▁o▁w▁o▁p▁t</td>\n",
       "      <td>n▁o▁d▁e▁b▁r▁e▁w▁a▁l▁i▁a▁s▁d▁e▁f▁a▁u▁l▁t▁v▁0▁.▁8▁.▁1▁5▁l▁s▁s▁h▁o▁w▁o▁p▁t▁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>▁3▁2▁5▁3▁0▁4▁2▁m▁a▁k▁e▁s▁u▁d▁o▁s▁e▁r▁v▁i▁c▁e▁d▁o▁c▁k▁e▁r▁h▁e▁l▁p▁R▁U▁N▁A</td>\n",
       "      <td>3▁2▁5▁3▁0▁4▁2▁m▁a▁k▁e▁s▁u▁d▁o▁s▁e▁r▁v▁i▁c▁e▁d▁o▁c▁k▁e▁r▁h▁e▁l▁p▁R▁U▁N▁A▁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>▁a▁r▁e▁m▁e▁t▁a▁l▁n▁o▁d▁e▁l▁i▁s▁t▁l▁e▁s▁s▁c▁u▁t▁o▁f▁f▁.▁p▁y▁m▁a▁n▁t▁p▁u▁t</td>\n",
       "      <td>a▁r▁e▁m▁e▁t▁a▁l▁n▁o▁d▁e▁l▁i▁s▁t▁l▁e▁s▁s▁c▁u▁t▁o▁f▁f▁.▁p▁y▁m▁a▁n▁t▁p▁u▁t▁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>▁e▁x▁i▁t▁m▁k▁d▁i▁r▁r▁u▁n▁l▁s▁t▁r▁i▁n▁i▁t▁y▁_▁t▁e▁s▁t▁/▁v▁i▁c▁o▁n▁c▁a▁t▁A</td>\n",
       "      <td>e▁x▁i▁t▁m▁k▁d▁i▁r▁r▁u▁n▁l▁s▁t▁r▁i▁n▁i▁t▁y▁_▁t▁e▁s▁t▁/▁v▁i▁c▁o▁n▁c▁a▁t▁A▁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>▁a▁l▁l▁r▁m▁-▁r▁f▁e▁x▁p▁.▁t▁g▁z▁g▁i▁t▁c▁o▁m▁m▁i▁t▁-▁m▁\"▁u▁p▁d▁a▁t▁e▁s▁\"▁e</td>\n",
       "      <td>a▁l▁l▁r▁m▁-▁r▁f▁e▁x▁p▁.▁t▁g▁z▁g▁i▁t▁c▁o▁m▁m▁i▁t▁-▁m▁\"▁u▁p▁d▁a▁t▁e▁s▁\"▁e▁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>▁i▁t▁]▁$▁&gt;▁&gt;▁[▁g▁o▁a▁l▁a▁d▁o▁r▁@▁g▁a▁t▁a▁n▁d▁a▁c▁o▁r▁p▁w▁e▁b▁s▁i▁t▁e▁-▁s</td>\n",
       "      <td>i▁t▁]▁$▁&gt;▁&gt;▁[▁g▁o▁a▁l▁a▁d▁o▁r▁@▁g▁a▁t▁a▁n▁d▁a▁c▁o▁r▁p▁w▁e▁b▁s▁i▁t▁e▁-▁s▁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>▁.▁/▁s▁c▁a▁n▁2▁1▁6▁.▁8▁9▁;▁l▁a▁d▁o▁r▁@▁g▁a▁t▁a▁n▁d▁a▁h▁h▁n▁e▁u▁a▁u▁f▁.▁b</td>\n",
       "      <td>.▁/▁s▁c▁a▁n▁2▁1▁6▁.▁8▁9▁;▁l▁a▁d▁o▁r▁@▁g▁a▁t▁a▁n▁d▁a▁h▁h▁n▁e▁u▁a▁u▁f▁.▁b▁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>▁m▁e▁s▁o▁s▁p▁h▁e▁r▁e▁/▁m▁a▁r▁a▁t▁h▁o▁n▁/▁a▁p▁i▁/▁S▁y▁s▁t▁e▁m▁R▁e▁s▁o▁u▁r</td>\n",
       "      <td>m▁e▁s▁o▁s▁p▁h▁e▁r▁e▁/▁m▁a▁r▁a▁t▁h▁o▁n▁/▁a▁p▁i▁/▁S▁y▁s▁t▁e▁m▁R▁e▁s▁o▁u▁r▁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>▁4▁-▁0▁6▁-▁4▁4▁_▁e▁d▁i▁t▁.▁m▁k▁v▁.▁m▁k▁v▁s▁u▁d▁o▁n▁e▁t▁s▁t▁a▁t▁-▁p▁l▁a▁n</td>\n",
       "      <td>4▁-▁0▁6▁-▁4▁4▁_▁e▁d▁i▁t▁.▁m▁k▁v▁.▁m▁k▁v▁s▁u▁d▁o▁n▁e▁t▁s▁t▁a▁t▁-▁p▁l▁a▁n▁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>▁r▁t▁i▁e▁s▁-▁c▁o▁m▁m▁o▁n▁:▁q▁a▁d▁d▁u▁s▁e▁r▁e▁c▁o▁l▁l▁e▁c▁t▁l▁s▁v▁i▁m▁/▁u</td>\n",
       "      <td>r▁t▁i▁e▁s▁-▁c▁o▁m▁m▁o▁n▁:▁q▁a▁d▁d▁u▁s▁e▁r▁e▁c▁o▁l▁l▁e▁c▁t▁l▁s▁v▁i▁m▁/▁u▁</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MyTokenizer(Transform):\n",
    "    def setups(self, items):\n",
    "        self.tok = SubwordTokenizer(vocab_sz=200)\n",
    "        self.tok.setup(items)\n",
    "        \n",
    "    def encodes(self, txts):\n",
    "        with open(txts, 'r') as file:\n",
    "            content = file.read()\n",
    "        flattened_list = [item for sublist in list(self.tok(content)) for item in sublist]\n",
    "        return flattened_list\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        decoded_values = TitledStr(''.join(encoded))\n",
    "        return  decoded_values\n",
    "            \n",
    "class MyNumerizer(Transform):\n",
    "    def setups(self, items):\n",
    "        self.num = Numericalize()\n",
    "        self.num.setup(items)\n",
    "        self.vocab = self.num.vocab\n",
    "        \n",
    "\n",
    "    def encodes(self, toks):\n",
    "        return self.num(toks)\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        return self.num.decode(encoded)  \n",
    "\n",
    "limit = 1000\n",
    "path_test = '/home/chris/University/gnn_project/'\n",
    "tfms = [[MyTokenizer(),MyNumerizer()]]\n",
    "files = get_text_files(path_test, folders = ['data'])\n",
    "dsets = Datasets(files[:limit], tfms)\n",
    "dls = dsets.dataloaders(dl_type=LMDataLoader, bs=64)\n",
    "\n",
    "dls.show_batch(max_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 80 ['xxunk', 'xxpad', 'xxbos', 'xxeos', 'xxfld', 'xxrep', 'xxwrep', 'xxup', 'xxmaj', '▁', 'a', 't', 'o', 's', 'e', 'i', 'n', 'r', 'l', 'c', '-', 'p', 'd', 'm', '.', 'g', '1', 'h', '2', 'f', 'u', '0', 'b', 'v', '/', '8', '_', '3', 'k', ':', 'y', '4', '7', '5', 'w', 'R', 'D', '9', '6', 'V', 'T', 'S', 'B', 'H', 'E', '#', '\"', 'A', 'I', 'x', 'N', 'z', '@', 'P', ';', 'O', 'q', '`', 'X', 'G', 'Z', '|', 'xxfake', 'xxfake', 'xxfake', 'xxfake', 'xxfake', 'xxfake', 'xxfake', 'xxfake']\n"
     ]
    }
   ],
   "source": [
    "print(\"Length:\",len(dls.vocab),dls.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gitconfig--global--add</td>\n",
       "      <td>itconfig--global--add</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>web.browser/opt/firefox/fire</td>\n",
       "      <td>web.browser/opt/firefox/firef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>foxclearwgetpilotu.110mb.com/</td>\n",
       "      <td>oxclearwgetpilotu.110mb.com/R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RaZvaNBv.tgz;tarxvfRaZvaNBv</td>\n",
       "      <td>aZvaNBv.tgz;tarxvfRaZvaNBv.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.tgz;rm-rfRaZvaNBv.tgz;cd</td>\n",
       "      <td>tgz;rm-rfRaZvaNBv.tgz;cd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>.tmp;./startprintubutignome-t</td>\n",
       "      <td>tmp;./startprintubutignome-te</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>erminal--role\"gnome\"sudop</td>\n",
       "      <td>rminal--role\"gnome\"sudopy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ythongitcommit-m\"firstc</td>\n",
       "      <td>thongitcommit-m\"firstco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ommit\"ifconfig#1357789494ls</td>\n",
       "      <td>mmit\"ifconfig#1357789494ls-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-lpython3xxunkombined.pycdmysql</td>\n",
       "      <td>lpython3xxunkombined.pycdmysql</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MyTokenizer(Transform):\n",
    "    def setups(self, items):\n",
    "        self.tok = BaseTokenizer()\n",
    "        #self.tok.setup(items)\n",
    "        \n",
    "    def encodes(self, txts):\n",
    "        with open(txts, 'r') as file:\n",
    "            content = file.read()\n",
    "        flattened_list = [item for sublist in list(self.tok(content)) for item in sublist]\n",
    "        return flattened_list\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        decoded_values = TitledStr(''.join(encoded))\n",
    "        return  decoded_values\n",
    "            \n",
    "class MyNumerizer(Transform):\n",
    "    def setups(self, items):\n",
    "        self.num = Numericalize()\n",
    "        self.num.setup(items)\n",
    "        self.vocab = self.num.vocab\n",
    "        \n",
    "\n",
    "    def encodes(self, toks):\n",
    "        return self.num(toks)\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        return self.num.decode(encoded)  \n",
    "\n",
    "limit = 100\n",
    "path_test = '/home/chris/University/gnn_project/'\n",
    "tfms = [[MyTokenizer(),MyNumerizer()]]\n",
    "files = get_text_files(path_test, folders = ['data'])\n",
    "dsets = Datasets(files[:limit], tfms)\n",
    "dls = dsets.dataloaders(dl_type=LMDataLoader, bs=64)\n",
    "\n",
    "dls.show_batch(max_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 80 ['xxunk', 'xxpad', 'xxbos', 'xxeos', 'xxfld', 'xxrep', 'xxwrep', 'xxup', 'xxmaj', '', 'a', 't', 'o', 's', 'e', 'i', 'n', 'r', 'l', 'c', '-', 'p', 'd', 'm', '.', 'g', '1', 'h', '2', 'f', 'u', '0', 'b', 'v', '/', '8', '_', '3', 'k', ':', 'y', '4', '7', '5', 'w', 'R', 'D', '9', '6', 'V', 'T', 'S', 'B', 'H', 'E', '#', '\"', 'A', 'I', 'x', 'N', 'z', '@', 'P', ';', 'O', 'q', '`', 'X', 'G', 'Z', '|', 'xxfake', 'xxfake', 'xxfake', 'xxfake', 'xxfake', 'xxfake', 'xxfake', 'xxfake']\n"
     ]
    }
   ],
   "source": [
    "print(\"Length:\",len(dls.vocab),dls.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sudo raspi-configfind ./ -nam</td>\n",
       "      <td>udo raspi-configfind ./ -name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e \"xxunk.fq.gz\" |zcat | grep xxunkHxxunkI</td>\n",
       "      <td>\"xxunk.fq.gz\" |zcat | grep xxunkHxxunkIxxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxunkSIxxunklador@gatanda hhneuauf.ba</td>\n",
       "      <td>SIxxunklador@gatanda hhneuauf.bas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sh: bash:: command not founds</td>\n",
       "      <td>h: bash:: command not foundsu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>udo ifdown eth9bash: xxunkgoalado</td>\n",
       "      <td>do ifdown eth9bash: xxunkgoalador</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>r@gatanda: command not found#</td>\n",
       "      <td>@gatanda: command not found#1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1517115588pwdwget pilotu.110m</td>\n",
       "      <td>517115588pwdwget pilotu.110mb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b.com/RaZvaNBv.tgz;tar xvf Ra</td>\n",
       "      <td>.com/RaZvaNBv.tgz;tar xvf RaZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ZvaNBv.tgz;rm -rf RaZvaNBv.tg</td>\n",
       "      <td>vaNBv.tgz;rm -rf RaZvaNBv.tgz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>z;cd .tmp;./start printubutic</td>\n",
       "      <td>;cd .tmp;./start printubutica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MyTokenizer(Transform):\n",
    "    def setups(self, items):\n",
    "        self.tok = SpacyTokenizer()\n",
    "        #self.tok.setup(items)\n",
    "        \n",
    "    def encodes(self, txts):\n",
    "        with open(txts, 'r') as file:\n",
    "            content = file.read()\n",
    "        flattened_list = [item for sublist in list(self.tok(content)) for item in sublist]\n",
    "        return flattened_list\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        decoded_values = TitledStr(''.join(encoded))\n",
    "        return  decoded_values\n",
    "            \n",
    "class MyNumerizer(Transform):\n",
    "    def setups(self, items):\n",
    "        self.num = Numericalize()\n",
    "        self.num.setup(items)\n",
    "        self.vocab = self.num.vocab\n",
    "        \n",
    "\n",
    "    def encodes(self, toks):\n",
    "        return self.num(toks)\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        return self.num.decode(encoded)  \n",
    "    \n",
    "limit = 100\n",
    "path_test = '/home/chris/University/gnn_project/'\n",
    "tfms = [[MyTokenizer(),MyNumerizer()]]\n",
    "files = get_text_files(path_test, folders = ['data'])\n",
    "dsets = Datasets(files[:limit], tfms)\n",
    "dls = dsets.dataloaders(dl_type=LMDataLoader, bs=64)\n",
    "\n",
    "dls.show_batch(max_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 80 ['xxunk', 'xxpad', 'xxbos', 'xxeos', 'xxfld', 'xxrep', 'xxwrep', 'xxup', 'xxmaj', ' ', 'a', 't', 'o', 's', 'e', 'i', 'n', 'r', 'l', 'c', '-', 'p', 'd', 'm', '.', 'g', '1', 'h', '2', 'f', 'u', '0', 'b', 'v', '/', '8', '_', '3', 'k', ':', 'y', '4', '7', '5', 'w', 'R', 'D', '9', '6', 'V', 'T', 'S', 'B', 'H', 'E', '#', '\"', 'A', 'I', 'x', 'N', 'z', '@', 'P', ';', 'O', 'q', '`', 'X', 'G', 'Z', '|', 'xxfake', 'xxfake', 'xxfake', 'xxfake', 'xxfake', 'xxfake', 'xxfake', 'xxfake']\n"
     ]
    }
   ],
   "source": [
    "print(\"Length:\",len(dls.vocab),dls.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>php server.php#1473555325vim mysum.shhistoryservice redis-server stopfor</td>\n",
       "      <td>hp server.php#1473555325vim mysum.shhistoryservice redis-server stopfor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>udo pacman -R docker docker-composevim /usr/share/dbus-1/services/ vim t</td>\n",
       "      <td>do pacman -R docker docker-composevim /usr/share/dbus-1/services/ vim tu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sername@10.1.26.255lsiptables -Lmakesudo yum install jenkinsvim pintosap</td>\n",
       "      <td>ername@10.1.26.255lsiptables -Lmakesudo yum install jenkinsvim pintosapr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cp profile2vim src/genetic/Main.java lscomandogit add *[goalador@gatanda</td>\n",
       "      <td>p profile2vim src/genetic/Main.java lscomandogit add *[goalador@gatanda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>squitto_pub -h 127.0.0.1 -t \"application/1/node/0000000000000000/tx\" -m</td>\n",
       "      <td>quitto_pub -h 127.0.0.1 -t \"application/1/node/0000000000000000/tx\" -m \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-lhlslsexitexport LC_ALL=en_US.utf8exploitsudo reflector --verbose --lat</td>\n",
       "      <td>lhlslsexitexport LC_ALL=en_US.utf8exploitsudo reflector --verbose --late</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>it pull --rebasevim zerador.shsu ./server -p 3490lsuname -r | cut -c 1mk</td>\n",
       "      <td>t pull --rebasevim zerador.shsu ./server -p 3490lsuname -r | cut -c 1mkd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sigc/EventLevel.pm lllssort testeps auxtrizen -Ss digikamls -ltr /isiseq</td>\n",
       "      <td>igc/EventLevel.pm lllssort testeps auxtrizen -Ss digikamls -ltr /isiseqr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>d ..lssu -composer install --no-devps -fpython popel.py lsl temp/#150946</td>\n",
       "      <td>..lssu -composer install --no-devps -fpython popel.py lsl temp/#1509464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>.akamai.com:8443/api/v1/se/deployments?environment=qa&amp;isDeploy=true | jq</td>\n",
       "      <td>akamai.com:8443/api/v1/se/deployments?environment=qa&amp;isDeploy=true | jq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MyTokenizer(Transform):\n",
    "    def setups(self, items):\n",
    "        self.tok = WordTokenizer()\n",
    "        #self.tok.setup(items)\n",
    "        \n",
    "    def encodes(self, txts):\n",
    "        with open(txts, 'r') as file:\n",
    "            content = file.read()\n",
    "        flattened_list = [item for sublist in list(self.tok(content)) for item in sublist]\n",
    "        return flattened_list\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        decoded_values = TitledStr(''.join(encoded))\n",
    "        return  decoded_values\n",
    "            \n",
    "class MyNumerizer(Transform):\n",
    "    def setups(self, items):\n",
    "        self.num = Numericalize()\n",
    "        self.num.setup(items)\n",
    "        self.vocab = self.num.vocab\n",
    "        \n",
    "\n",
    "    def encodes(self, toks):\n",
    "        return self.num(toks)\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        return self.num.decode(encoded)  \n",
    "\n",
    "limit = 10000\n",
    "path_test = '/home/chris/University/gnn_project/'\n",
    "tfms = [[MyTokenizer(),MyNumerizer()]]\n",
    "files = get_text_files(path_test, folders = ['data'])\n",
    "dsets = Datasets(files[:limit], tfms)\n",
    "dls = dsets.dataloaders(dl_type=LMDataLoader, bs=64)\n",
    "\n",
    "dls.show_batch(max_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/University/GNN/project/transformer_env/lib/python3.9/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos i can not stress how bad this movie is . xxmaj this director took every cheap little unintelligent shot at making these people look so \" xxunk \" . xxmaj why are their clothes so dirty ? xxmaj why on earth would you get the new clark kent to play a crack head ? xxmaj you should be banned from motion pictures for the rest of your life xxmaj buddy xxmaj</td>\n",
       "      <td>i can not stress how bad this movie is . xxmaj this director took every cheap little unintelligent shot at making these people look so \" xxunk \" . xxmaj why are their clothes so dirty ? xxmaj why on earth would you get the new clark kent to play a crack head ? xxmaj you should be banned from motion pictures for the rest of your life xxmaj buddy xxmaj xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>believable . sorry , to me , this movie has no entertainment value at all . xxbos xxmaj maybe here in xxmaj sydney we are all poop side down and as a result we get to lap up xxunk like this s - xxunk in xxunk theaters . xxmaj released here in 1980 this hilarious all - xxunk drama was xxunk with xxunk of delight at the session i xxunk . xxmaj</td>\n",
       "      <td>. sorry , to me , this movie has no entertainment value at all . xxbos xxmaj maybe here in xxmaj sydney we are all poop side down and as a result we get to lap up xxunk like this s - xxunk in xxunk theaters . xxmaj released here in 1980 this hilarious all - xxunk drama was xxunk with xxunk of delight at the session i xxunk . xxmaj in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>book is the xxmaj general is not a xxmaj gothic monster like the characters in xxmaj catherine 's books . xxmaj his xxunk is far more complicated in his xxunk of his children 's spirits and his treatment of xxmaj catherine based on money concerns alone . xxmaj he does not lock up his wife or kill her but he does send xxmaj miss xxmaj morland on a 70 mile trip alone</td>\n",
       "      <td>is the xxmaj general is not a xxmaj gothic monster like the characters in xxmaj catherine 's books . xxmaj his xxunk is far more complicated in his xxunk of his children 's spirits and his treatment of xxmaj catherine based on money concerns alone . xxmaj he does not lock up his wife or kill her but he does send xxmaj miss xxmaj morland on a 70 mile trip alone in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxmaj singer 's career also took a xxunk with this one . \\n\\n xxmaj special effects crew has some fun , and xxmaj jerry xxmaj xxunk provides a score superior to its subject matter . xxbos xxmaj when i think about xxup tv movies , i always think of this film , i have watched it a few times on xxmaj sky xxmaj movies , it was terrible . \\n\\n xxmaj its</td>\n",
       "      <td>singer 's career also took a xxunk with this one . \\n\\n xxmaj special effects crew has some fun , and xxmaj jerry xxmaj xxunk provides a score superior to its subject matter . xxbos xxmaj when i think about xxup tv movies , i always think of this film , i have watched it a few times on xxmaj sky xxmaj movies , it was terrible . \\n\\n xxmaj its been</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>. \\n\\n xxmaj secondly , how many plot holes are in this movie ? xxmaj why introduce the phone call from xxmaj xxunk 's long lost xxmaj dad and never address it again ? xxmaj what was the point of his xxmaj mom hanging up on him - why even have her call to say he is xxunk her too much money - what was the point of that ? xxmaj the</td>\n",
       "      <td>\\n\\n xxmaj secondly , how many plot holes are in this movie ? xxmaj why introduce the phone call from xxmaj xxunk 's long lost xxmaj dad and never address it again ? xxmaj what was the point of his xxmaj mom hanging up on him - why even have her call to say he is xxunk her too much money - what was the point of that ? xxmaj the guy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xxmaj madsen . xxmaj do n't get me wrong , i can handle remakes , even obscure ones . xxmaj but this badly written and poorly filmed xxunk made me feel sorry for both xxmaj madsen and co - star xxmaj richard xxmaj thomas . xxmaj unlike the original , the dialogue here is xxunk , making me wonder , \" why did they bother to re - write it ? \"</td>\n",
       "      <td>madsen . xxmaj do n't get me wrong , i can handle remakes , even obscure ones . xxmaj but this badly written and poorly filmed xxunk made me feel sorry for both xxmaj madsen and co - star xxmaj richard xxmaj thomas . xxmaj unlike the original , the dialogue here is xxunk , making me wonder , \" why did they bother to re - write it ? \" xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>was xxmaj xxunk that did it . \\n\\n xxmaj their is no mention of xxmaj mark or his turning back so the writers of the script are forced to have xxmaj paul and xxmaj xxunk argue over xxmaj paul 's desire to xxunk in xxmaj rome as the basis of their xxunk . \\n\\n xxmaj no xxmaj xxunk on xxmaj paul 's xxmaj second and xxmaj third xxmaj missions ; xxmaj no</td>\n",
       "      <td>xxmaj xxunk that did it . \\n\\n xxmaj their is no mention of xxmaj mark or his turning back so the writers of the script are forced to have xxmaj paul and xxmaj xxunk argue over xxmaj paul 's desire to xxunk in xxmaj rome as the basis of their xxunk . \\n\\n xxmaj no xxmaj xxunk on xxmaj paul 's xxmaj second and xxmaj third xxmaj missions ; xxmaj no xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xxbos xxmaj totally forgettable and almost unwatchable . xxmaj if you enjoy bad acting , thin plots and xxunk weak xxunk , pull up a chair . xxmaj of passing interest to see xxmaj bridget xxmaj fonda look - a - like xxmaj xxunk xxmaj xxunk . xxbos xxmaj this is truly , without exaggerating , one of the worst xxmaj slasher movies ever made . i know , it came out</td>\n",
       "      <td>xxmaj totally forgettable and almost unwatchable . xxmaj if you enjoy bad acting , thin plots and xxunk weak xxunk , pull up a chair . xxmaj of passing interest to see xxmaj bridget xxmaj fonda look - a - like xxmaj xxunk xxmaj xxunk . xxbos xxmaj this is truly , without exaggerating , one of the worst xxmaj slasher movies ever made . i know , it came out in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>at the approaching guy on the horse . xxmaj for some reason , she does n't fire but yells several times for someone else . xxmaj then as xxmaj skeletor xxunk , she jumps out from behind the tree so that xxmaj skeletor can stick her with his spear . xxmaj then everybody starts shooting . xxmaj the bullets cause sparks to fly from the trees . xxmaj apparently the folks who</td>\n",
       "      <td>the approaching guy on the horse . xxmaj for some reason , she does n't fire but yells several times for someone else . xxmaj then as xxmaj skeletor xxunk , she jumps out from behind the tree so that xxmaj skeletor can stick her with his spear . xxmaj then everybody starts shooting . xxmaj the bullets cause sparks to fly from the trees . xxmaj apparently the folks who made</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>find xxmaj peter o'toole entertaining . xxmaj but that is no reason to rent it . xxmaj if you are curious about xxmaj roman history , there are much better movies available . xxbos i normally would n't waste my time criticizing a useless movie such as this . xxmaj however , xxmaj i 'm off of work this week , so i have plenty of time to xxunk in meaningless xxunk</td>\n",
       "      <td>xxmaj peter o'toole entertaining . xxmaj but that is no reason to rent it . xxmaj if you are curious about xxmaj roman history , there are much better movies available . xxbos i normally would n't waste my time criticizing a useless movie such as this . xxmaj however , xxmaj i 'm off of work this week , so i have plenty of time to xxunk in meaningless xxunk .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MyTokenizer(Transform):\n",
    "    def setups(self, items):\n",
    "        path = untar_data(URLs.IMDB)\n",
    "        self.tok =  Tokenizer.from_folder(path)\n",
    "        self.tok.setup(items)\n",
    "        \n",
    "    def encodes(self, txts):\n",
    "        with open(txts, 'r') as file:\n",
    "            content = file.read()\n",
    "        return self.tok(content)\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        return self.tok.decode(encoded)\n",
    "            \n",
    "class MyNumerizer(Transform):\n",
    "    def setups(self, items):\n",
    "        self.num = Numericalize()\n",
    "        self.num.setup(items)\n",
    "        self.vocab = self.num.vocab\n",
    "\n",
    "    def encodes(self, toks):\n",
    "        return self.num(toks)\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        return self.num.decode(encoded)\n",
    "\n",
    "limit = 1000\n",
    "path = untar_data(URLs.IMDB)\n",
    "tfms = [[MyTokenizer(), MyNumerizer()]]\n",
    "files = get_text_files(path, folders = ['train', 'test'])\n",
    "#splits = GrandparentSplitter(valid_name='test')(files)\n",
    "dsets = Datasets(files[:limit], tfms)#, splits=splits)\n",
    "dls = dsets.dataloaders(dl_type=LMDataLoader)\n",
    "\n",
    "dls.show_batch(max_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer from txts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer(Transform):\n",
    "    def setups(self, items):\n",
    "        path = untar_data(URLs.IMDB)\n",
    "        self.tok =  Tokenizer.from_folder(path)\n",
    "        self.tok.setup(items)\n",
    "        \n",
    "    def encodes(self, txts):\n",
    "        return self.tok(txts)\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        return self.tok.decode(encoded)\n",
    "            \n",
    "class MyNumerizer(Transform):\n",
    "    def setups(self, items):\n",
    "        self.num = Numericalize()\n",
    "        #print(items)\n",
    "        self.num.setup(items)\n",
    "        self.vocab = self.num.vocab\n",
    "\n",
    "    def encodes(self, toks):\n",
    "        return self.num(toks)\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        return self.num.decode(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer(Transform):\n",
    "    def setups(self, items):\n",
    "        self.tok = SubwordTokenizer(vocab_sz=1000)\n",
    "        self.tok.setup(items)\n",
    "        \n",
    "    def encodes(self, txts):\n",
    "        toks = self.tok(txts)\n",
    "        tokenized_sentences = [[tok for tok in gen] for gen in toks]\n",
    "        flattened_list = [item for sublist in tokenized_sentences for item in sublist if item]\n",
    "        return flattened_list\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        decoded_values = TitledStr(''.join(encoded))\n",
    "        return  decoded_values\n",
    "            \n",
    "class MyNumerizer(Transform):\n",
    "    def setups(self, items):\n",
    "        self.num = Numericalize()\n",
    "        self.num.setup(items)\n",
    "        self.vocab = self.num.vocab\n",
    "        \n",
    "\n",
    "    def encodes(self, toks):\n",
    "        return self.num(toks)\n",
    "    \n",
    "    def decodes(self, encoded):\n",
    "        return self.num.decode(encoded)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "txt200 = txts[:1000]\n",
    "tokn = MyTokenizer()\n",
    "tokn.setup(txt200)\n",
    "\n",
    "toks = txt200.map(tokn)\n",
    "\n",
    "num = MyNumerizer()\n",
    "num.setup(toks)\n",
    "\n",
    "#tokenized_sentences = [[tok for tok in gen] for gen in toks]\n",
    "#flattened_toks = [[item for sublist in sublist_list for item in sublist] for sublist_list in tokenized_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>▁l▁s▁l▁s▁l▁s▁r▁o▁o▁t▁/▁D▁o▁w▁n▁l▁o▁a▁d▁s▁f▁c▁r▁a▁c▁k▁z▁i▁p▁-▁u▁-▁D▁-▁p▁/▁r▁o▁o▁t▁/▁h▁</td>\n",
       "      <td>l▁s▁l▁s▁l▁s▁r▁o▁o▁t▁/▁D▁o▁w▁n▁l▁o▁a▁d▁s▁f▁c▁r▁a▁c▁k▁z▁i▁p▁-▁u▁-▁D▁-▁p▁/▁r▁o▁o▁t▁/▁h▁o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>▁m▁i▁n▁@▁1▁0▁.▁1▁.▁2▁6▁.▁9▁:▁T▁r▁e▁a▁s▁u▁r▁e▁e▁-▁I▁s▁l▁a▁n▁d▁.▁p▁d▁f▁/▁r▁o▁o▁t▁/▁D▁o▁</td>\n",
       "      <td>▁i▁n▁@▁1▁0▁.▁1▁.▁2▁6▁.▁9▁:▁T▁r▁e▁a▁s▁u▁r▁e▁e▁-▁I▁s▁l▁a▁n▁d▁.▁p▁d▁f▁/▁r▁o▁o▁t▁/▁D▁o▁w</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modus = 'Manpage' #IMBD\n",
    "limit = 1000\n",
    "tfms = [[tokn, num]]\n",
    "\n",
    "if modus == 'Manpage':\n",
    "    # Manpage\n",
    "    dsets = Datasets(txts[:limit], tfms)\n",
    "else:\n",
    "    # IMBD\n",
    "    path = untar_data(URLs.IMDB)\n",
    "    files = get_text_files(path, folders = ['train', 'test'])\n",
    "    dsets = Datasets(files[:limit], tfms)\n",
    "    \n",
    "dls = dsets.dataloaders(dl_type=LMDataLoader)\n",
    "\n",
    "dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained('bert-base-uncased')\n",
    "\n",
    "config.vocab_size = len(dls.vocab)\n",
    "config.num_labels = len(dls.vocab)\n",
    "config.hidden_size = 395\n",
    "config.num_hidden_layers = 12\n",
    "config.num_attention_heads = 5\n",
    "config.max_position_embeddings = 512\n",
    "transformer = ShellTransformer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/7 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastprogress/fastprogress.py:73: UserWarning: Your generator is empty.\n",
      "  warn(\"Your generator is empty.\")\n"
     ]
    }
   ],
   "source": [
    "model = transformer\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "dls.to(device)\n",
    "\n",
    "\n",
    "learn = Learner(\n",
    "    dls, \n",
    "    model, \n",
    "    loss_func=CrossEntropyLossFlat(), \n",
    "    metrics=[accuracy]\n",
    ")\n",
    "\n",
    "learn.fit_one_cycle(1, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMBD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#1) [(TensorText([2, 8, 0, 0, 0, 0, 0]),)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5733/1300371960.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(token_ids).unsqueeze(0).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie was good becausexxbos xxmaj xxunk xxunk xxunk xxunk xxunk \" / etc / etc / etc / etc / etc / etc / etc / etc / etc /\n"
     ]
    }
   ],
   "source": [
    "# Define a function for text generation\n",
    "def generate_text(model, token_ids, max_length=20):\n",
    "    token_ids = token_ids[0][0]\n",
    "    input_ids = torch.tensor(token_ids).unsqueeze(0).to(device)  \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs[:, -1, :]  \n",
    "            next_token_id = torch.argmax(logits, dim=-1)\n",
    "            token_ids = torch.cat((token_ids, next_token_id),dim=0)\n",
    "            input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0).to(device)], dim=-1)\n",
    "    return token_ids\n",
    "\n",
    "def decode_tokens(numerized_tokens, vocab):\n",
    "    generated_test = [vocab[token] for token in numerized_tokens]\n",
    "    return ' '.join(generated_test)\n",
    "\n",
    "\n",
    "# Generate text\n",
    "files = get_text_files('', folders = ['test'])\n",
    "#vocab = dls.vocab\n",
    "start_text_ids = Datasets(files, tfms)\n",
    "\n",
    "print(start_text_ids)\n",
    "\n",
    "generated_ids = generate_text(learn.model, start_text_ids)\n",
    "\n",
    "path= '/home/chris/Git_Repos/gnn_project/FastAI/test/text_generation.txt'\n",
    "\n",
    "\n",
    "with open(path, 'r') as file:\n",
    "     content = file.read()\n",
    "     \n",
    "print(content+decode_tokens(generated_ids,vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_95637/2104236549.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(token_ids).unsqueeze(0).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "▁ s ▁ s ▁h ▁ a ▁d ▁m ▁i ▁n ▁ 0 ▁1 ▁ 0 ▁1 ▁ 0 ▁1 ▁ 0 ▁1 ▁ 0 ▁1 ▁ 0 ▁1 ▁ 0 ▁1 ▁ 0 ▁1 ▁ 0 ▁1 ▁ 0 ▁1\n"
     ]
    }
   ],
   "source": [
    "# Define a function for text generation\n",
    "def generate_text(model, token_ids, max_length=30):\n",
    "    token_ids = token_ids\n",
    "    input_ids = torch.tensor(token_ids).unsqueeze(0).to(device)  \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs[:, -1, :]  \n",
    "            next_token_id = torch.argmax(logits, dim=-1)\n",
    "            token_ids = torch.cat((token_ids, next_token_id),dim=0)\n",
    "            input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0).to(device)], dim=-1)\n",
    "    return token_ids\n",
    "\n",
    "def decode_tokens(numerized_tokens, vocab):\n",
    "    generated_test = [vocab[token] for token in numerized_tokens]\n",
    "    return ' '.join(generated_test)\n",
    "\n",
    "# Generate text\n",
    "test_sentence = 'ssh admin'\n",
    "#test_sentence = get_text_files('', folders = ['test'])[0]\n",
    "pipe = Pipeline([tokn,num])\n",
    "\n",
    "start_text_ids = pipe(test_sentence)\n",
    "\n",
    "generated_ids = generate_text(learn.model, start_text_ids)\n",
    "\n",
    "vocab = dls.vocab\n",
    "     \n",
    "print(decode_tokens(generated_ids,vocab))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
