{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = \"cuda:0\"\n",
    "else:\n",
    "  device = \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.fast.ai/text.core.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_text_files(path, folders = ['train', 'test', 'unsup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's about jealousy, it's about racism, it's about manipulation, but the un\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = files[0].open().read()\n",
    "txt[:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#200) ['It\\'s about jealousy, it\\'s about racism, it\\'s about manipulation, but the underlying message is love. Geoffrey Sax tried to pull off Shakespeare\\'s Othello, by bringing it to modern day context. However, the actors were not convincing enough to pull this off. There were extra bodies to help put everything in to perspective, however, John Othello, played by Eamonn Walker, over reacted a lot in this film, causing for the down fall of Keeley Hawes, Dessie Brabant, eventually ending in Dessie\\'s death. <br /><br />Ben Jago, played by Christopher Eccleston, was seen as the main character in the film. He didn\\'t give enough evidence for Dessie to be cheating on Othello, with Michael Cass, played by Richard Coyle. Instead he just played a friend to all and gave one reason as to why she \"was\" cheating. In the play, it took a lot more convincing from Iago to make Othello even suspect anything. This change made the movie more about rage for the wrong reasons, than what the book was based off of. However, the movie did have a few good points. It turned the army scenes into more a racist group toward blacks, where Othello is the main chief of the police squad. These scenes are made believable by the raging crowds, and burning fires. You are able to sense the amount of racism in the movie, more so than you can in the book. This book plays up the modern day scenes by making it much easier to understand, than the Shakespearian times it was written in. In the play Iago (Jago) gets tortured at the end, but in the film he gets his satisfaction, and gets Othello\\'s position. He never gets what he deserves and is never caught for telling the lie to Othello until it is too late. I saw this as a downfall in the movie, because I feel that the villain is granted his treasure of the promotion out of lying, and in the book, he is found out by Rodrigo. Overall, the movie could have done a better job based on the play than what it did. I feel that the director of the movie left out some of the most important parts of the play that were mentioned or there to make the play flow, or make it more of a tragedy. I would say that you should read the book first, in order to understand all of the events that happened in the movie, otherwise you may find yourself lost, and confused.','on this quagmire of mediocrity? You are SO much better than this.<br /><br />Simply put, Frostbite is worthless. Bad acting (and I use that term loosely), minimalist \"plot,\" sophomoric humor, and lackluster snowboarding. There\\'s not even a sufficient display of feminine pulchritude to spark the prurient interest of socially inept, but red-blooded, males.<br /><br />Top Gun had spectacular flight sequences to goggle at. Days of Thunder had heart-pounding racing action. Even Point Break had skydiving scenes to its credit. Frostbite has none of these. It\\'s not worth your time, my time, Traci Lords\\' time, Carmen Nicole\\'s time, nor the time of anyone involved with this destruction of celluloid that would have been perfectly usable on something worthwhile had it not been wasted on this fodder for the recycling center.<br /><br />The world will be a better place when we forget that Frostbite ever existed.','I watched this movie about six years ago and I recently did so again. If I remember correctly I did not like it at all the first time and I appreciated it slightly more this second time.<br /><br />This movie is obviously on a big budget. The effects are mostly top notch (except for one or two \"impacts\") and the cast is impressive. However, there are some elements that destroy the overall impression of the show.<br /><br />Firstly, whoever decided that Peter Stormare should act as a crazy Russian astronaut should be fired. Being a Swede and a fan of Peter, I\\'m pretty sure he can play a Russian character well. But his performance in this case is plain stupid, both with respect the lines uttered and the acting. So... something must be wrong with the script. I\\'d like to see Peter as a professional Russian astronaut instead.<br /><br />Secondly, the action scenes that take place on the surface are so intense that it is nearly unbearable to watch. It is a total chaos that lasts over thirty minutes with too few moments to catch one\\'s breath. In addition to this, the events that unfold are simply not credible. I\\'d like to see a much more sensible and stripped down version of this part of the movie.<br /><br />Finally, the scenes that involve flying space shuttles are too action-biased. The shuttles are maneuvering like if they were a couple of MIGs, at zero safety distance, while bouncing off car-sized ice blocks like ping-pong balls. The director should watch Apollo 13 to learn the limitations of spacecraft like these.<br /><br />I like the music score because it is dramatic to a degree making it very touching. The overall performance of the actors is great. Apart from the things mentioned above the story is interesting and quite easy to follow.<br /><br />With some minor changes this would have been a 8/10 movie. I\\'m sorry it isn\\'t!',\"This was one of the worst movies that I have ever watched. The story was about a woman prisoner sent into space to try and save mankind but what it actually turned out to be was that the prisoner was actually put on board a space ship with a nymphomaniac lesbian commander. All the story was about was having sex over and over again. There was no need for all of this footage - only to create a soft porn film. Of course we had to see the heroin of the movie having sex for about 2 minutes. I don't know what all this had to do with the actual plot of the movie -except I assume to get men to watch it. I gave it 1 out of 10 only because there was no other rating lower. Don't waste your money or time- it has nothing to do with science fiction but rather a movie for young adolescent boys to watch to see t&a. The creature was shown only more than half way through the movie and looked like the alien in the movie with Signorney Weaver. Poor excuse for entertainment.\",'In this horrible attempt at a Blair Witch mockumentary, a bunch of people go to Africa to investigate a creature called the Half-Caste. It\\'s pretty obvious that there was no script to speak of, and that everything was improvised. That can work if you have good actors, which this film didn\\'t. <br /><br />This movie tries to gain points for originality by exploring a more obscure myth and an exotic culture. As a result, there are a lot of scenes out in the bush where characters do \"quirky African stuff\" like eating elephant dung. There is also some pretty good footage of lions eating (from a National Geographic perspective) but there\\'s not a single scare in the whole movie. <br /><br />If you\\'ve seen Cannibal Holocaust or the Blair Witch Project, this movie will hold no surprises for you, and you can probably watch better lion footage on the Discovery Channel.<br /><br />Definitely a Half- Aste effort. <br /><br />A note to the filmmakers: guys, do us all a favor and next time save the \"How I spent my African Vacation\" home movie for your family and close friends. Nobody else wants to see it.','The spoof genre, which has lacked creativity and humor for some time already, gets spat upon yet again by hacks with no talent. No point, no fun, no originality; just a few cheap bucks for the film makers. <br /><br />It takes more than just referencing some recent movies and giving characters double-meaning names to be satire; to make people laugh. Any clod can pick up a cam-corder, and have some bad-acting buddies in cheap costumes imitate somebody. Since the genre being targeted this time is inspirational sports movies, there are a few lame references thrown out to movies of that type: the jokes are so weak the characters actually have to emphasize the references in various ways, to get you to laugh hysterically. It doesn\\'t work.<br /><br />That\\'s not comedy. However, the same old worn out sophomoric \"jokes\" ripped off from a middle school washroom (done even more blandly than usual) are all here. If that\\'s not enough, there\\'s a running \"gag\" of a bus running somebody over. So funny, right? Also, one pathetically poor scene does more product placement than Michael Bay; again with the same unfunny results. A musical bit flops miserably. Pity Carl Weathers, once Apollo Creed in the Rocky series, now stuck with roles in swill like this.<br /><br />Lousy beyond words. Watching a snail run the marathon would be less tedious than watching this film is. Probably a lot funnier, too.','I was a hippie age 22 in 1965, have seen the play 5 times, have 2 versions of the music, and have read the script many times. Maybe taken as an separate thing, the movie is OK, but as an adoption of the play, it\\'s terrible.<br /><br />First the good. The songs are sung well, and the production values are pretty good. The homosexual implications in Black Boys, White Boys is cute. The Claude/Berger switch was interesting. Hair is a fluid production, and constant changes in it are inherent provided the basic spirit of it is retained.<br /><br />But the basic spirit was very much distorted. It seemed like all the producer wanted was to sing certain songs, and fit the story-line to them regardless of what that did to the original intent of the play.<br /><br />Claude, Sheila, and Hud were presented as a selfish brats who could care less about anyone outside their tribe. Claude destroyed Sheila\\'s parents party (it\\'s hard to believe that she smiled while her parents were being hurt), and repeatedly stole cars. Sheila stole the soldier\\'s clothes and car. Then she left him in the middle of the desert where he would certainly sunburn badly, probably be busted in rank, and could likely die on that lonely Nevada road. Good fun. And Hud found it fine to break his son\\'s heart while yelling at his ex. This is not what hippies were like, not what the play described, and is directly opposite to a love generation.<br /><br />I\\'ve never seen a hippie beg for money. The point was to be self-sufficient outside the capitalist system, not beg from it.<br /><br />After Berger is thrown in jail for ruining Sheila\\'s party and Claude offers to bail some out, Berger insists on being bailed instead, with no better plan for bailing the rest that hitting on Sheila\\'s parents, and then hitting on his own mommy.<br /><br />\"Be In\" has always been a very spiritual point, but here it is wasted on silly brides floating around. Even if this is part of his trip, it ruins a beautiful song. I know they\\'re contrasting \"Floating In Space\" with the Army. But again, it totally ruins a beautiful song.<br /><br />While Sheila and Claude are skinny dipping, Berger steals their clothes, laughing \"it was fun man\" even after he could see both were very annoyed. Then a little later, Berger does one thing that no hippie ever does. He hits Claude. Terrible.<br /><br />Then they have the perfect opportunity to sing \"Starshine\" at night in some beautiful SF spot with the stars twinkling down, maybe working into an inventive duet. Instead, the whole tribe sings it in the blazing sun speeding down the road in a convertible (stolen of course).<br /><br />I was vastly disappointed with the movie in 1979, and I like it even less now. I think it would have been possible to do a reasonably close adoption of the show, but this ain\\'t it.','Only a very small child could overlook the absurdities in this bomb; the first difficulty faced by the submarine \"Seaview\" is what appear to be chunks of--rock? falling down through the water and crashing into its hull. But it\\'s not rock, they\\'re under the North Pole--it is ICE! Everybody, except possibly hitherto mentioned small children (and even some of them) know that ICE FLOATS.<br /><br />Then, disaster strikes--that darn VAN ALLEN RADIATION BELT around the Earth catches fire! No one knows how this happened, we are told, which is understandable, because it is utterly impossible for radiation to \"catch fire\", and even if it could, there is NO AIR IN SPACE for it to burn.<br /><br />There is literally no good reason to overlook science concepts basic to 2nd grade school textbooks when making a film; however, Irwin Allen manages to do it again and again; perhaps we are meant to focus on the \"people\" instead, which is pretty easy, as they are CARDBOARD.<br /><br />The cast tries very hard not to look embarrassed in this ridiculous sub-kiddie romp, much like later episodes of his \"Lost in Space\" TV series, the concept of which was swiped outright from writer Ib Melchior and then rushed into production.<br /><br />The sub looks pretty good, though, which is why this one gets a \"2\".',\"What an awful movie. Full of cliches, perplexing scenes, very bad acting, and an atrotious script. It is hard to believe the same guys that wrote The People vs. Larry Flint and Man on the Moon wrote this garbage. Man, this makes my list of Top 10 Worst Movies of All-Time. Didn't this guy, this director, if you can call him that, realize that the first Problem Child was bad enough? Let alone make a sequel for it!!?? Amazing that piece of trash films like this can be shown to children let alone be released! 1 out of 10 *'s\",\"Naach would have won an Razzie for the Worst Film in 2004 (may be overall too) if it were global. When it comes acting badly (aka showing attitude/yawning/over (not) acting) Halle Berry is no match for Antra Malli. While the catwoman had storeline, supposedly hot actress in microscopic costume, and some action sequences, Naach had nothing at all.<br /><br />One of those movies which makes me wonder why IMDb does allow one to rate a movie as 0/10. Yet again, I think that movie does not even deserve a 0. It has to be something negative or minus infinity.<br /><br />OK what about the plot outline? It is a funda-giving, arrogant, full of attitude choreographer meet an useless, skill-less, loafer who aspires to be an actor tale. The story is so short that if just another sentence, IMDb might ban me for writing a spoiler. About the story-telling? Its like a bunch (sorry 2 people for the most part) of people moving in super slow motion. Don't try this movie if you have bought new DVD-player. You would end up believing that either DVD is in bad shape or DVD-player is struck. Not its fault at all.<br /><br />At the end of it all, you end up giving some credits to the director. At least he realized that both Antra Malli and Abhishek Bachan (at least at that time) can't speak dialogues convincingly, so there are not too many dialogues in this movie. So, you can at least sleep your way through the movie, with some annoying noises from those Antra-malli song sequences.<br /><br />Do watch this movie if you are new to Bollywood Cinema. Once you have tolerated this movie, you would be able to see any Bollywood movie and enjoy it.<br /><br />There can't be worse 3 hour torture than this!\"...]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txts = L(o.open().read() for o in files[:200])\n",
    "txts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subword Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subword(sz):\n",
    "    sp = SubwordTokenizer(vocab_sz=sz)\n",
    "    sp.setup(txts)\n",
    "    return ' '.join(first(sp([txt]))[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"▁It ' s ▁about ▁jea lousy , ▁it ' s ▁about ▁racism , ▁it ' s ▁about ▁ manipulati on , ▁but ▁the ▁under ly ing ▁message ▁is ▁love . ▁Ge off re y ▁S ax ▁tri ed ▁to ▁pull\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'▁It \\' s ▁about ▁ j e al ous y , ▁it \\' s ▁about ▁ra c is m , ▁it \\' s ▁about ▁man i p ul ation , ▁but ▁the ▁under ly ing ▁mess age ▁is ▁love . ▁G e off re y ▁S a x ▁tri ed ▁to ▁p u ll ▁off ▁S h ake s p ear e \\' s ▁Othello , ▁by ▁ br ing ing ▁it ▁to ▁modern ▁day ▁con t e x t . ▁How ever , ▁the ▁actors ▁were ▁not ▁con v in c ing ▁enough ▁to ▁p u ll ▁this ▁off . ▁There ▁were ▁ex t ra ▁bo d ies ▁to ▁help ▁put ▁everything ▁in ▁to ▁p ers p ective , ▁how ever , ▁John ▁Othello , ▁play ed ▁by ▁E a m on n ▁W al k er , ▁over ▁re act ed ▁a ▁lot ▁in ▁this ▁film , ▁c a us ing ▁for ▁the ▁down ▁fall ▁of ▁K e e le y ▁H a w es , ▁De s s ie ▁B ra b ant , ▁eventually ▁end ing ▁in ▁De s s ie \\' s ▁de a th . ▁ < br ▁/> < br ▁/> B en ▁Ja g o , ▁play ed ▁by ▁Christ op h er ▁E c c le st on , ▁was ▁seen ▁as ▁the ▁main ▁character ▁in ▁the ▁film . ▁He ▁did n \\' t ▁give ▁enough ▁ e v id en ce ▁for ▁De s s ie ▁to ▁be ▁ch e ating ▁on ▁Othello , ▁with ▁M ich a el ▁C as s , ▁play ed ▁by ▁R ich ard ▁C o y le . ▁Instead ▁he ▁just ▁play ed ▁a ▁friend ▁to ▁all ▁and ▁ga ve ▁one ▁reason ▁as ▁to ▁why ▁she ▁\" w as \" ▁ch e ating . ▁In ▁the ▁play , ▁it ▁to ok ▁a ▁lot ▁more ▁con v in c ing ▁fro m ▁I ago ▁to ▁make ▁Othello ▁even ▁ s us p e c t ▁anything . ▁Thi s ▁ch an ge ▁made ▁the ▁movie ▁more ▁about ▁ra ge ▁for ▁the ▁wrong ▁reason s , ▁than ▁what ▁the ▁book ▁was ▁base d ▁off ▁of . ▁How ever , ▁the ▁movie ▁did ▁have ▁a ▁f ew ▁good ▁point s . ▁It ▁turn ed ▁the ▁ ar m y ▁scenes ▁in to ▁more ▁a ▁ra c ist ▁group ▁to ward ▁b la ck s , ▁where ▁Othello ▁is ▁the ▁main ▁ch ie f ▁of ▁the ▁poli ce ▁ s qu ad . ▁The s e ▁scenes ▁are ▁made ▁believable ▁by ▁the ▁ra g ing ▁cr ow d s , ▁and ▁b ur n ing ▁f ir es . ▁You ▁are ▁ able ▁to ▁ s ense ▁the ▁amount ▁of ▁ra c is m ▁in ▁the ▁movie , ▁more ▁so ▁than ▁you ▁can ▁in ▁the ▁book . ▁Thi s ▁book ▁play s ▁up ▁the ▁modern ▁day ▁scenes ▁by ▁mak ing ▁it ▁much ▁easi er ▁to ▁understand , ▁than ▁the ▁S h ake s p ear ian ▁time s ▁it ▁was ▁written ▁in . ▁In ▁the ▁play ▁I ago ▁( J ago ) ▁get s ▁to r t ur ed ▁at ▁the ▁end , ▁but ▁in ▁the ▁film ▁he ▁get s ▁his ▁ s at is f act ion , ▁and ▁get s ▁Othello \\' s ▁ po s it ion . ▁He ▁n ever ▁get s ▁what ▁he ▁deserve s ▁and ▁is ▁n ever ▁caught ▁for ▁tell ing ▁the ▁li e ▁to ▁Othello ▁unti l ▁it ▁is ▁too ▁late . ▁I ▁saw ▁this ▁as ▁a ▁down f all ▁in ▁the ▁movie , ▁because ▁I ▁feel ▁that ▁the ▁villain ▁is ▁gra n t ed ▁his ▁tr e as ure ▁of ▁the ▁pro m o t ion ▁out ▁of ▁ ly ing , ▁and ▁in ▁the ▁book , ▁he ▁is ▁f ound ▁out ▁by ▁R o d ri g o . ▁O v er all , ▁the ▁movie ▁could ▁have ▁done ▁a ▁better ▁job ▁base d ▁on ▁the ▁play ▁than ▁what ▁it ▁did . ▁I ▁feel ▁that ▁the ▁director ▁of ▁the ▁movie ▁left ▁out ▁some ▁of ▁the ▁most ▁important ▁part s ▁of ▁the ▁play ▁that ▁were ▁mention ed ▁or ▁there ▁to ▁make ▁the ▁play ▁f l ow , ▁or ▁make ▁it ▁more ▁of ▁a ▁tr age d y . ▁I ▁would ▁say ▁that ▁you ▁should ▁read ▁the ▁book ▁first , ▁in ▁or d er ▁to ▁understand ▁all ▁of ▁the ▁even t s ▁that ▁happen ed ▁in ▁the ▁movie , ▁other w ise ▁you ▁may ▁find ▁yourself ▁ lo st , ▁and ▁con f us ed .'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/University/GNN/project/transformer_env/lib/python3.9/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "spacy = WordTokenizer()\n",
    "tkn = Tokenizer(spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#530) ['xxbos','xxmaj','it',\"'s\",'about','jealousy',',','it',\"'s\",'about'...]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks200 = txts[:200].map(tkn)\n",
    "toks200[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#1920) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the','.',',','and','a','to','of','is','i','it','in'...]\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = Numericalize()\n",
    "num.setup(toks200)\n",
    "coll_repr(num.vocab,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = tkn(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorText([   2,    8,   18,   25,   66,    0,   11,   18,   25,   66, 1416,\n",
       "              11,   18,   25,   66,    0,   11,   31,    9,    0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = num(toks)[:20]\n",
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"xxbos xxmaj it 's about xxunk , it 's about racism , it 's about xxunk , but the xxunk\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#20) ['xxbos','xxmaj','it',\"'s\",'about','jealousy',',','it',\"'s\",'about'...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 72]), torch.Size([64, 72]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums200 = toks200.map(num)\n",
    "\n",
    "dl = LMDataLoader(nums200)\n",
    "\n",
    "x,y = first(dl)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"xxbos xxmaj it 's about xxunk , it 's about racism , it 's about xxunk , but the xxunk\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in x[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"xxmaj it 's about xxunk , it 's about racism , it 's about xxunk , but the xxunk message\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in y[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastAI Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_text_files(path, folders = ['train','test'])\n",
    "txts = L(o.open().read() for o in files[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#530) ['xxbos','xxmaj','it',\"'s\",'about','jealousy',',','it',\"'s\",'about'...]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = Tokenizer.from_folder(path)\n",
    "tok.setup(txts)\n",
    "toks = txts.map(tok)\n",
    "toks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorText([   2,    8,   18,   23,   62, 6709,   11,   18,   23,   62])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = Numericalize()\n",
    "num.setup(toks)\n",
    "nums = toks.map(num)\n",
    "nums[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#10) ['xxbos','xxmaj','it',\"'s\",'about','jealousy',',','it',\"'s\",'about']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums_dec = num.decode(nums[0][:10])\n",
    "nums_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"xxbos xxmaj it 's about jealousy , it 's about\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.decode(nums_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#530) ['xxbos','xxmaj','it',\"'s\",'about','jealousy',',','it',\"'s\",'about'...],\n",
       " (#2) ['xxbos','t'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok((txts[0], txt[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2.0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x:int): return x+1\n",
    "tfm = Transform(f)\n",
    "tfm(2),tfm(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2.0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@Transform\n",
    "def f(x:int): return x+1\n",
    "f(2),f(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeMean(Transform):\n",
    "    def setups(self, items): self.mean = sum(items)/len(items)\n",
    "    def encodes(self, x): return x-self.mean\n",
    "    def decodes(self, x): return x+self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.0, -1.0, 2.0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfm = NormalizeMean()\n",
    "tfm.setup([1,2,3,4,5])\n",
    "start = 2\n",
    "y = tfm(start)\n",
    "z = tfm.decode(y)\n",
    "tfm.mean,y,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform(self, enc=None, dec=None, split_idx=None, order=None)\n",
      "Delegates (`__call__`,`decode`,`setup`) to (<code>encodes</code>,<code>decodes</code>,<code>setups</code>) if `split_idx` matches\n",
      "\n",
      "To get a prettier result with hyperlinks to source code and documentation, install nbdev: pip install nbdev\n"
     ]
    }
   ],
   "source": [
    "doc(Transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorText([   2,    8,   18,   23,   62, 6709,   11,   18,   23,   62, 4564,\n",
       "              11,   18,   23,   62, 7975,   11,   31,    9, 5107])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfms = Pipeline([tok, num])\n",
    "t = tfms(txts[0])\n",
    "t[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"xxbos xxmaj it 's about jealousy , it 's about racism , it 's about manipulation , but the underlyin\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfms.decode(t)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tls = TfmdLists(files, [Tokenizer.from_folder(path), Numericalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorText([   2,    8,   17,   23,   60, 5851,   11,   17,   23,   60, 3192,\n",
       "              11,   17,   23,   60, 7436,   11,   31,    9, 4473])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tls[0]\n",
    "t[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"xxbos xxmaj it 's about jealousy , it 's about racism , it 's about manipulation , but the underlyin\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tls.decode(t)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxbos xxmaj it 's about jealousy , it 's about racism , it 's about manipulation , but the underlying message is love . xxmaj geoffrey xxmaj sax tried to pull off xxmaj shakespeare 's xxmaj othello , by bringing it to modern day context . xxmaj however , the actors were not convincing enough to pull this off . xxmaj there were extra bodies to help put everything in to perspective , however , xxmaj john xxmaj othello , played by xxmaj eamonn xxmaj walker , over reacted a lot in this film , causing for the down fall of xxmaj keeley xxmaj hawes , xxmaj xxunk xxmaj xxunk , eventually ending in xxmaj xxunk 's death . \n",
      "\n",
      " xxmaj ben xxmaj jago , played by xxmaj christopher xxmaj eccleston , was seen as the main character in the film . xxmaj he did n't give enough evidence for xxmaj xxunk to be cheating on xxmaj othello , with xxmaj michael xxmaj cass , played by xxmaj richard xxmaj coyle . xxmaj instead he just played a friend to all and gave one reason as to why she \" was \" cheating . xxmaj in the play , it took a lot more convincing from xxmaj iago to make xxmaj othello even suspect anything . xxmaj this change made the movie more about rage for the wrong reasons , than what the book was based off of . xxmaj however , the movie did have a few good points . xxmaj it turned the army scenes into more a racist group toward blacks , where xxmaj othello is the main chief of the police squad . xxmaj these scenes are made believable by the raging crowds , and burning fires . xxmaj you are able to sense the amount of racism in the movie , more so than you can in the book . xxmaj this book plays up the modern day scenes by making it much easier to understand , than the xxmaj shakespearian times it was written in . xxmaj in the play xxmaj iago ( jago ) gets tortured at the end , but in the film he gets his satisfaction , and gets xxmaj othello 's position . xxmaj he never gets what he deserves and is never caught for telling the lie to xxmaj othello until it is too late . i saw this as a downfall in the movie , because i feel that the villain is granted his treasure of the promotion out of lying , and in the book , he is found out by xxmaj rodrigo . xxmaj overall , the movie could have done a better job based on the play than what it did . i feel that the director of the movie left out some of the most important parts of the play that were mentioned or there to make the play flow , or make it more of a tragedy . i would say that you should read the book first , in order to understand all of the events that happened in the movie , otherwise you may find yourself lost , and confused .\n"
     ]
    }
   ],
   "source": [
    "tls.show(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = int(len(files)*0.8)\n",
    "splits = [list(range(cut)), list(range(cut,len(files)))]\n",
    "tls = TfmdLists(files, [Tokenizer.from_folder(path), Numericalize], splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorText([  2,   8,  65,  80,  19, 722,  21,   9, 917, 807,  42,  36, 493,\n",
       "            326,  10,   8,  20,  16,  13, 103])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tls.valid[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [[Tokenizer.from_folder(path), Numericalize]]\n",
    "files = get_text_files(path, folders = ['train', 'test'])\n",
    "splits = GrandparentSplitter(valid_name='test')(files)\n",
    "dsets = Datasets(files, tfms, splits=splits)\n",
    "dls = dsets.dataloaders(dl_type=LMDataLoader, before_batch=pad_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj from the q &amp; a before and after , this is what i could gather : xxmaj some xxmaj irish guy wants to make a movie . xxmaj nothing in particular , just any movie . xxmaj so , one night at a party , he hears some ex - roadie tell him a classic bit of rock n ' roll lore ; the one about how xxmaj gram xxmaj</td>\n",
       "      <td>xxmaj from the q &amp; a before and after , this is what i could gather : xxmaj some xxmaj irish guy wants to make a movie . xxmaj nothing in particular , just any movie . xxmaj so , one night at a party , he hears some ex - roadie tell him a classic bit of rock n ' roll lore ; the one about how xxmaj gram xxmaj parsons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>style . xxmaj but this movie , however , was probably the worst movie i have seen in 2008 . i can honestly believe that this movie is unknown , and i think it should stay like this , for movies like these are making the thriller genre a joke . \\n\\n i advise anyone that is a fan of thriller movies , or even simply movies to stay far away from</td>\n",
       "      <td>. xxmaj but this movie , however , was probably the worst movie i have seen in 2008 . i can honestly believe that this movie is unknown , and i think it should stay like this , for movies like these are making the thriller genre a joke . \\n\\n i advise anyone that is a fan of thriller movies , or even simply movies to stay far away from this</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj gives my have mouth him acts whore this as is . xxmaj i is and though of t'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a batch of data from the DataLoaders\n",
    "x, y = dls.one_batch()\n",
    "\n",
    "' '.join(num.vocab[o] for o in x[0][:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#200) ['It\\'s about jealousy, it\\'s about racism, it\\'s about manipulation, but the underlying message is love. Geoffrey Sax tried to pull off Shakespeare\\'s Othello, by bringing it to modern day context. However, the actors were not convincing enough to pull this off. There were extra bodies to help put everything in to perspective, however, John Othello, played by Eamonn Walker, over reacted a lot in this film, causing for the down fall of Keeley Hawes, Dessie Brabant, eventually ending in Dessie\\'s death. <br /><br />Ben Jago, played by Christopher Eccleston, was seen as the main character in the film. He didn\\'t give enough evidence for Dessie to be cheating on Othello, with Michael Cass, played by Richard Coyle. Instead he just played a friend to all and gave one reason as to why she \"was\" cheating. In the play, it took a lot more convincing from Iago to make Othello even suspect anything. This change made the movie more about rage for the wrong reasons, than what the book was based off of. However, the movie did have a few good points. It turned the army scenes into more a racist group toward blacks, where Othello is the main chief of the police squad. These scenes are made believable by the raging crowds, and burning fires. You are able to sense the amount of racism in the movie, more so than you can in the book. This book plays up the modern day scenes by making it much easier to understand, than the Shakespearian times it was written in. In the play Iago (Jago) gets tortured at the end, but in the film he gets his satisfaction, and gets Othello\\'s position. He never gets what he deserves and is never caught for telling the lie to Othello until it is too late. I saw this as a downfall in the movie, because I feel that the villain is granted his treasure of the promotion out of lying, and in the book, he is found out by Rodrigo. Overall, the movie could have done a better job based on the play than what it did. I feel that the director of the movie left out some of the most important parts of the play that were mentioned or there to make the play flow, or make it more of a tragedy. I would say that you should read the book first, in order to understand all of the events that happened in the movie, otherwise you may find yourself lost, and confused.','on this quagmire of mediocrity? You are SO much better than this.<br /><br />Simply put, Frostbite is worthless. Bad acting (and I use that term loosely), minimalist \"plot,\" sophomoric humor, and lackluster snowboarding. There\\'s not even a sufficient display of feminine pulchritude to spark the prurient interest of socially inept, but red-blooded, males.<br /><br />Top Gun had spectacular flight sequences to goggle at. Days of Thunder had heart-pounding racing action. Even Point Break had skydiving scenes to its credit. Frostbite has none of these. It\\'s not worth your time, my time, Traci Lords\\' time, Carmen Nicole\\'s time, nor the time of anyone involved with this destruction of celluloid that would have been perfectly usable on something worthwhile had it not been wasted on this fodder for the recycling center.<br /><br />The world will be a better place when we forget that Frostbite ever existed.','I watched this movie about six years ago and I recently did so again. If I remember correctly I did not like it at all the first time and I appreciated it slightly more this second time.<br /><br />This movie is obviously on a big budget. The effects are mostly top notch (except for one or two \"impacts\") and the cast is impressive. However, there are some elements that destroy the overall impression of the show.<br /><br />Firstly, whoever decided that Peter Stormare should act as a crazy Russian astronaut should be fired. Being a Swede and a fan of Peter, I\\'m pretty sure he can play a Russian character well. But his performance in this case is plain stupid, both with respect the lines uttered and the acting. So... something must be wrong with the script. I\\'d like to see Peter as a professional Russian astronaut instead.<br /><br />Secondly, the action scenes that take place on the surface are so intense that it is nearly unbearable to watch. It is a total chaos that lasts over thirty minutes with too few moments to catch one\\'s breath. In addition to this, the events that unfold are simply not credible. I\\'d like to see a much more sensible and stripped down version of this part of the movie.<br /><br />Finally, the scenes that involve flying space shuttles are too action-biased. The shuttles are maneuvering like if they were a couple of MIGs, at zero safety distance, while bouncing off car-sized ice blocks like ping-pong balls. The director should watch Apollo 13 to learn the limitations of spacecraft like these.<br /><br />I like the music score because it is dramatic to a degree making it very touching. The overall performance of the actors is great. Apart from the things mentioned above the story is interesting and quite easy to follow.<br /><br />With some minor changes this would have been a 8/10 movie. I\\'m sorry it isn\\'t!',\"This was one of the worst movies that I have ever watched. The story was about a woman prisoner sent into space to try and save mankind but what it actually turned out to be was that the prisoner was actually put on board a space ship with a nymphomaniac lesbian commander. All the story was about was having sex over and over again. There was no need for all of this footage - only to create a soft porn film. Of course we had to see the heroin of the movie having sex for about 2 minutes. I don't know what all this had to do with the actual plot of the movie -except I assume to get men to watch it. I gave it 1 out of 10 only because there was no other rating lower. Don't waste your money or time- it has nothing to do with science fiction but rather a movie for young adolescent boys to watch to see t&a. The creature was shown only more than half way through the movie and looked like the alien in the movie with Signorney Weaver. Poor excuse for entertainment.\",'In this horrible attempt at a Blair Witch mockumentary, a bunch of people go to Africa to investigate a creature called the Half-Caste. It\\'s pretty obvious that there was no script to speak of, and that everything was improvised. That can work if you have good actors, which this film didn\\'t. <br /><br />This movie tries to gain points for originality by exploring a more obscure myth and an exotic culture. As a result, there are a lot of scenes out in the bush where characters do \"quirky African stuff\" like eating elephant dung. There is also some pretty good footage of lions eating (from a National Geographic perspective) but there\\'s not a single scare in the whole movie. <br /><br />If you\\'ve seen Cannibal Holocaust or the Blair Witch Project, this movie will hold no surprises for you, and you can probably watch better lion footage on the Discovery Channel.<br /><br />Definitely a Half- Aste effort. <br /><br />A note to the filmmakers: guys, do us all a favor and next time save the \"How I spent my African Vacation\" home movie for your family and close friends. Nobody else wants to see it.','The spoof genre, which has lacked creativity and humor for some time already, gets spat upon yet again by hacks with no talent. No point, no fun, no originality; just a few cheap bucks for the film makers. <br /><br />It takes more than just referencing some recent movies and giving characters double-meaning names to be satire; to make people laugh. Any clod can pick up a cam-corder, and have some bad-acting buddies in cheap costumes imitate somebody. Since the genre being targeted this time is inspirational sports movies, there are a few lame references thrown out to movies of that type: the jokes are so weak the characters actually have to emphasize the references in various ways, to get you to laugh hysterically. It doesn\\'t work.<br /><br />That\\'s not comedy. However, the same old worn out sophomoric \"jokes\" ripped off from a middle school washroom (done even more blandly than usual) are all here. If that\\'s not enough, there\\'s a running \"gag\" of a bus running somebody over. So funny, right? Also, one pathetically poor scene does more product placement than Michael Bay; again with the same unfunny results. A musical bit flops miserably. Pity Carl Weathers, once Apollo Creed in the Rocky series, now stuck with roles in swill like this.<br /><br />Lousy beyond words. Watching a snail run the marathon would be less tedious than watching this film is. Probably a lot funnier, too.','I was a hippie age 22 in 1965, have seen the play 5 times, have 2 versions of the music, and have read the script many times. Maybe taken as an separate thing, the movie is OK, but as an adoption of the play, it\\'s terrible.<br /><br />First the good. The songs are sung well, and the production values are pretty good. The homosexual implications in Black Boys, White Boys is cute. The Claude/Berger switch was interesting. Hair is a fluid production, and constant changes in it are inherent provided the basic spirit of it is retained.<br /><br />But the basic spirit was very much distorted. It seemed like all the producer wanted was to sing certain songs, and fit the story-line to them regardless of what that did to the original intent of the play.<br /><br />Claude, Sheila, and Hud were presented as a selfish brats who could care less about anyone outside their tribe. Claude destroyed Sheila\\'s parents party (it\\'s hard to believe that she smiled while her parents were being hurt), and repeatedly stole cars. Sheila stole the soldier\\'s clothes and car. Then she left him in the middle of the desert where he would certainly sunburn badly, probably be busted in rank, and could likely die on that lonely Nevada road. Good fun. And Hud found it fine to break his son\\'s heart while yelling at his ex. This is not what hippies were like, not what the play described, and is directly opposite to a love generation.<br /><br />I\\'ve never seen a hippie beg for money. The point was to be self-sufficient outside the capitalist system, not beg from it.<br /><br />After Berger is thrown in jail for ruining Sheila\\'s party and Claude offers to bail some out, Berger insists on being bailed instead, with no better plan for bailing the rest that hitting on Sheila\\'s parents, and then hitting on his own mommy.<br /><br />\"Be In\" has always been a very spiritual point, but here it is wasted on silly brides floating around. Even if this is part of his trip, it ruins a beautiful song. I know they\\'re contrasting \"Floating In Space\" with the Army. But again, it totally ruins a beautiful song.<br /><br />While Sheila and Claude are skinny dipping, Berger steals their clothes, laughing \"it was fun man\" even after he could see both were very annoyed. Then a little later, Berger does one thing that no hippie ever does. He hits Claude. Terrible.<br /><br />Then they have the perfect opportunity to sing \"Starshine\" at night in some beautiful SF spot with the stars twinkling down, maybe working into an inventive duet. Instead, the whole tribe sings it in the blazing sun speeding down the road in a convertible (stolen of course).<br /><br />I was vastly disappointed with the movie in 1979, and I like it even less now. I think it would have been possible to do a reasonably close adoption of the show, but this ain\\'t it.','Only a very small child could overlook the absurdities in this bomb; the first difficulty faced by the submarine \"Seaview\" is what appear to be chunks of--rock? falling down through the water and crashing into its hull. But it\\'s not rock, they\\'re under the North Pole--it is ICE! Everybody, except possibly hitherto mentioned small children (and even some of them) know that ICE FLOATS.<br /><br />Then, disaster strikes--that darn VAN ALLEN RADIATION BELT around the Earth catches fire! No one knows how this happened, we are told, which is understandable, because it is utterly impossible for radiation to \"catch fire\", and even if it could, there is NO AIR IN SPACE for it to burn.<br /><br />There is literally no good reason to overlook science concepts basic to 2nd grade school textbooks when making a film; however, Irwin Allen manages to do it again and again; perhaps we are meant to focus on the \"people\" instead, which is pretty easy, as they are CARDBOARD.<br /><br />The cast tries very hard not to look embarrassed in this ridiculous sub-kiddie romp, much like later episodes of his \"Lost in Space\" TV series, the concept of which was swiped outright from writer Ib Melchior and then rushed into production.<br /><br />The sub looks pretty good, though, which is why this one gets a \"2\".',\"What an awful movie. Full of cliches, perplexing scenes, very bad acting, and an atrotious script. It is hard to believe the same guys that wrote The People vs. Larry Flint and Man on the Moon wrote this garbage. Man, this makes my list of Top 10 Worst Movies of All-Time. Didn't this guy, this director, if you can call him that, realize that the first Problem Child was bad enough? Let alone make a sequel for it!!?? Amazing that piece of trash films like this can be shown to children let alone be released! 1 out of 10 *'s\",\"Naach would have won an Razzie for the Worst Film in 2004 (may be overall too) if it were global. When it comes acting badly (aka showing attitude/yawning/over (not) acting) Halle Berry is no match for Antra Malli. While the catwoman had storeline, supposedly hot actress in microscopic costume, and some action sequences, Naach had nothing at all.<br /><br />One of those movies which makes me wonder why IMDb does allow one to rate a movie as 0/10. Yet again, I think that movie does not even deserve a 0. It has to be something negative or minus infinity.<br /><br />OK what about the plot outline? It is a funda-giving, arrogant, full of attitude choreographer meet an useless, skill-less, loafer who aspires to be an actor tale. The story is so short that if just another sentence, IMDb might ban me for writing a spoiler. About the story-telling? Its like a bunch (sorry 2 people for the most part) of people moving in super slow motion. Don't try this movie if you have bought new DVD-player. You would end up believing that either DVD is in bad shape or DVD-player is struck. Not its fault at all.<br /><br />At the end of it all, you end up giving some credits to the director. At least he realized that both Antra Malli and Abhishek Bachan (at least at that time) can't speak dialogues convincingly, so there are not too many dialogues in this movie. So, you can at least sleep your way through the movie, with some annoying noises from those Antra-malli song sequences.<br /><br />Do watch this movie if you are new to Bollywood Cinema. Once you have tolerated this movie, you would be able to see any Bollywood movie and enjoy it.<br /><br />There can't be worse 3 hour torture than this!\"...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = get_text_files(path, folders = ['train', 'test', 'unsup'])\n",
    "txts = L(o.open().read() for o in files[:200])\n",
    "txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer.from_folder(path)\n",
    "tok.setup(txts)\n",
    "toks = txts.map(tok)\n",
    "\n",
    "num = Numericalize()\n",
    "num.setup(toks)\n",
    "nums = toks.map(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfms = [[tok, num]]\n",
    "files = get_text_files(path, folders = ['train', 'test'])\n",
    "splits = GrandparentSplitter(valid_name='test')(files)\n",
    "dsets = Datasets(files, tfms, splits=splits)\n",
    "dls = dsets.dataloaders(dl_type=LMDataLoader, before_batch=pad_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj this film is described as a action / comedy . xxmaj the first xxunk minutes and xxmaj xxunk xxmaj xxunk 's presence point to it being a xxunk xxunk film . \\n\\n xxmaj however , the xxunk of the ridiculous female lead begins a xxunk of xxunk plot twists which do little to xxunk the contrived ending . xxmaj the film xxunk between xxunk used loosely ) and xxunk drama</td>\n",
       "      <td>xxmaj this film is described as a action / comedy . xxmaj the first xxunk minutes and xxmaj xxunk xxmaj xxunk 's presence point to it being a xxunk xxunk film . \\n\\n xxmaj however , the xxunk of the ridiculous female lead begins a xxunk of xxunk plot twists which do little to xxunk the contrived ending . xxmaj the film xxunk between xxunk used loosely ) and xxunk drama with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>have not read xxmaj xxunk xxmaj xxunk , i am xxunk that i have missed very little in the way of dialogue and plot or of xxunk and xxunk . i only xxunk to xxunk the novel due to the xxunk xxunk and xxunk that this xxunk has xxunk within me . \\n\\n i need not xxunk anything in the way of plot here . xxmaj let me just say this :</td>\n",
       "      <td>not read xxmaj xxunk xxmaj xxunk , i am xxunk that i have missed very little in the way of dialogue and plot or of xxunk and xxunk . i only xxunk to xxunk the novel due to the xxunk xxunk and xxunk that this xxunk has xxunk within me . \\n\\n i need not xxunk anything in the way of plot here . xxmaj let me just say this : if</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cool about this , \" as she gives him an xxunk xxunk xxunk . \\n\\n xxmaj the xxunk - year - old father wants to be a father . xxmaj he wants to be a xxunk . \\n\\n xxmaj why not ? xxmaj we see the \" new \" xxunk a few minutes after xxunk -- it xxunk to be a xxunk , happy 4 - xxunk - old xxunk . xxmaj</td>\n",
       "      <td>about this , \" as she gives him an xxunk xxunk xxunk . \\n\\n xxmaj the xxunk - year - old father wants to be a father . xxmaj he wants to be a xxunk . \\n\\n xxmaj why not ? xxmaj we see the \" new \" xxunk a few minutes after xxunk -- it xxunk to be a xxunk , happy 4 - xxunk - old xxunk . xxmaj just</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>that this xxunk high xxunk . i say it xxunk the xxunk of the cover artist who did n't even bother to xxmaj xxunk xxmaj xxunk 's xxunk on xxunk . xxbos xxmaj this was the only time i ever xxunk out on a movie . xxmaj years later , i saw it in the cable xxunk and thought , \" maybe i should give it another try . \" xxmaj xxunk</td>\n",
       "      <td>this xxunk high xxunk . i say it xxunk the xxunk of the cover artist who did n't even bother to xxmaj xxunk xxmaj xxunk 's xxunk on xxunk . xxbos xxmaj this was the only time i ever xxunk out on a movie . xxmaj years later , i saw it in the cable xxunk and thought , \" maybe i should give it another try . \" xxmaj xxunk to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>film . ) of this film . xxmaj for a xxunk xxunk with a movie you do n't have to be ashamed of if your kids or anyone xxunk in on your viewing , get this , or catch it on one of the classic movie xxunk . xxbos a family with dad xxmaj xxunk ( xxunk xxmaj xxunk ) , mom xxmaj xxunk ( xxunk xxmaj xxunk ) , 10 year</td>\n",
       "      <td>. ) of this film . xxmaj for a xxunk xxunk with a movie you do n't have to be ashamed of if your kids or anyone xxunk in on your viewing , get this , or catch it on one of the classic movie xxunk . xxbos a family with dad xxmaj xxunk ( xxunk xxmaj xxunk ) , mom xxmaj xxunk ( xxunk xxmaj xxunk ) , 10 year old</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>on the xxunk of my xxunk . xxmaj five : xxmaj the songs in this are lovely . xxmaj my xxunk song starts with \" i need xxmaj xxunk , the xxunk , the xxunk \" … \\n\\n xxmaj so , watch this lovely film when you can , you wo n't be disappointed ! xxunk xxunk xxunk xxbos a xxunk of xxunk have mentioned the age xxunk between xxmaj xxunk and</td>\n",
       "      <td>the xxunk of my xxunk . xxmaj five : xxmaj the songs in this are lovely . xxmaj my xxunk song starts with \" i need xxmaj xxunk , the xxunk , the xxunk \" … \\n\\n xxmaj so , watch this lovely film when you can , you wo n't be disappointed ! xxunk xxunk xxunk xxbos a xxunk of xxunk have mentioned the age xxunk between xxmaj xxunk and xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>, xxunk \" she 's back \" . xxmaj and she wo n't let him rest until he does so . xxmaj she xxunk him xxunk … and the viewers , too ! xxmaj this movie is truly one of the worst movies xxmaj i 've ever seen . xxmaj hey , i like bad movies , though ( my xxunk movie is xxmaj xxunk ) . i was really xxunk my</td>\n",
       "      <td>xxunk \" she 's back \" . xxmaj and she wo n't let him rest until he does so . xxmaj she xxunk him xxunk … and the viewers , too ! xxmaj this movie is truly one of the worst movies xxmaj i 've ever seen . xxmaj hey , i like bad movies , though ( my xxunk movie is xxmaj xxunk ) . i was really xxunk my head</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n\\n 10 / 10 xxbos xxmaj this movie is xxmaj great ! xxmaj it xxunk my xxunk xxunk heart . i could n't relate about the xxunk xxunk that xxmaj xxunk has experienced because xxmaj i 'm living in my own country . i guess it is really hard to be xxunk . \\n\\n xxmaj we watched this film in our xxunk class in xxmaj new xxmaj era xxmaj xxunk , and</td>\n",
       "      <td>10 / 10 xxbos xxmaj this movie is xxmaj great ! xxmaj it xxunk my xxunk xxunk heart . i could n't relate about the xxunk xxunk that xxmaj xxunk has experienced because xxmaj i 'm living in my own country . i guess it is really hard to be xxunk . \\n\\n xxmaj we watched this film in our xxunk class in xxmaj new xxmaj era xxmaj xxunk , and i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dialog is not funny . xxmaj see this movie at your xxunk . \\n\\n xxmaj david xxmaj xxunk from xxmaj xxunk xxmaj xxunk should be after the movie for the little or no xxunk this movie will xxunk . xxmaj the banal plot xxunk to xxunk with other xxunk - xxunk out there , and just xxunk into the xxunk . xxmaj it was like watching xxunk dry . \\n\\n xxmaj in</td>\n",
       "      <td>is not funny . xxmaj see this movie at your xxunk . \\n\\n xxmaj david xxmaj xxunk from xxmaj xxunk xxmaj xxunk should be after the movie for the little or no xxunk this movie will xxunk . xxmaj the banal plot xxunk to xxunk with other xxunk - xxunk out there , and just xxunk into the xxunk . xxmaj it was like watching xxunk dry . \\n\\n xxmaj in short</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    dim_k = query.size(-1)\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / np.sqrt(dim_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return weights.bmm(value)\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "        self.mask = torch.tril(torch.ones(vocab_size, vocab_size)).unsqueeze(0)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        attn_outputs = scaled_dot_product_attention(\n",
    "            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state), self.mask)\n",
    "        return attn_outputs\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(embed_dim, head_dim, 72) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n",
    "        x = self.output_linear(x)\n",
    "        return x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply layer normalization and then copy input into query, key, value\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        # Apply attention with a skip connection\n",
    "        x = x + self.attention(hidden_state)\n",
    "        # Apply feed-forward layer with a skip connection\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        return x\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size, \n",
    "                                             config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n",
    "                                                config.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # Create position IDs for input sequence\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)\n",
    "        # Create token and position embeddings\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        \n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        # Combine token and position embeddings\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(config) \n",
    "                                     for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class TransformerForSequenceClassification(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x)\n",
    "        x = self.encoder(x)#[:, 0, :] # select hidden state of [CLS] token\n",
    "        #print(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        #print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/University/GNN/project/transformer_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.vocab_size = len(dls.vocab)\n",
    "config.num_labels = len(dls.vocab)\n",
    "config.hidden_size = 132\n",
    "encoder_classifier = TransformerForSequenceClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='30' class='' max='1631' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      1.84% [30/1631 01:56&lt;1:43:58 6.5158]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m\n\u001b[1;32m      5\u001b[0m dls\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m learn \u001b[38;5;241m=\u001b[39m Learner(\n\u001b[1;32m      8\u001b[0m     dls, \n\u001b[1;32m      9\u001b[0m     model, \n\u001b[1;32m     10\u001b[0m     loss_func\u001b[38;5;241m=\u001b[39mCrossEntropyLossFlat(), \n\u001b[1;32m     11\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[accuracy]\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_one_cycle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m learn\u001b[38;5;241m.\u001b[39mexport(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmymodel.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastai/callback/schedule.py:119\u001b[0m, in \u001b[0;36mfit_one_cycle\u001b[0;34m(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    116\u001b[0m lr_max \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([h[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mhypers])\n\u001b[1;32m    117\u001b[0m scheds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: combined_cos(pct_start, lr_max\u001b[38;5;241m/\u001b[39mdiv, lr_max, lr_max\u001b[38;5;241m/\u001b[39mdiv_final),\n\u001b[1;32m    118\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmom\u001b[39m\u001b[38;5;124m'\u001b[39m: combined_cos(pct_start, \u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoms \u001b[38;5;28;01mif\u001b[39;00m moms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m moms))}\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mParamScheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheds\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcbs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_opt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_epoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastai/learner.py:264\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mset_hypers(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lr)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch \u001b[38;5;241m=\u001b[39m n_epoch\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelFitException\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_end_cleanup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastai/learner.py:253\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch):\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m=\u001b[39mepoch\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelEpochException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastai/learner.py:247\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch_validate()\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastai/learner.py:239\u001b[0m, in \u001b[0;36mLearner._do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch_train\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls\u001b[38;5;241m.\u001b[39mtrain\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelTrainException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastai/learner.py:205\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_batches\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl)\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastai/learner.py:235\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    233\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_device(b)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(b)\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_one_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelBatchException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastai/learner.py:223\u001b[0m, in \u001b[0;36mLearner._do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myb): \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_grad_opt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastai/learner.py:211\u001b[0m, in \u001b[0;36mLearner._do_grad_opt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_grad_opt\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbackward\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelBackwardException\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_with_events(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m, CancelStepException)\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastai/learner.py:207\u001b[0m, in \u001b[0;36mLearner._backward\u001b[0;34m(self)\u001b[0m\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_backward\u001b[39m(\u001b[38;5;28mself\u001b[39m): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/torch/_tensor.py:513\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \n\u001b[1;32m    468\u001b[0m \u001b[38;5;124;03mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03m        used to compute the attr::tensors.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/torch/overrides.py:1621\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1615\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefining your `__torch_function__ as a plain method is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1616\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be an error in future, please define it as a classmethod.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1617\u001b[0m                   \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;66;03m# Use `public_api` instead of `implementation` so __torch_function__\u001b[39;00m\n\u001b[1;32m   1620\u001b[0m \u001b[38;5;66;03m# implementations can do equality/identity comparisons.\u001b[39;00m\n\u001b[0;32m-> 1621\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_func_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/fastai/torch_core.py:382\u001b[0m, in \u001b[0;36mTensorBase.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__str__\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m): \u001b[38;5;28mprint\u001b[39m(func, types, args, kwargs)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _torch_handled(args, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_opt, func): types \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mTensor,)\n\u001b[0;32m--> 382\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mifnone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m dict_objs \u001b[38;5;241m=\u001b[39m _find_args(args) \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;28;01melse\u001b[39;00m _find_args(\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mtype\u001b[39m(res),TensorBase) \u001b[38;5;129;01mand\u001b[39;00m dict_objs: res\u001b[38;5;241m.\u001b[39mset_meta(dict_objs[\u001b[38;5;241m0\u001b[39m],as_copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/torch/_tensor.py:1418\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m-> 1418\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1420\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/GNN/project/transformer_env/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = encoder_classifier\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "dls.to(device)\n",
    "\n",
    "learn = Learner(\n",
    "    dls, \n",
    "    model, \n",
    "    loss_func=CrossEntropyLossFlat(), \n",
    "    metrics=[accuracy]\n",
    ")\n",
    "\n",
    "learn.fit_one_cycle(1, 1e-3)\n",
    "\n",
    "learn.export('mymodel.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
